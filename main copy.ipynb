{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea063ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "# Define a dictionary to map sentiment values to category names\n",
    "sentiment_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "\n",
    "\n",
    "# PROCESSING\n",
    "MIN_FREQ = 2\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7a0eb01",
   "metadata": {},
   "source": [
    "Goal of project: \n",
    "\n",
    "This notebook includes: (steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5416c0b",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "First, we load and explore the dataset and apply some initial processing such as setting the '*Id*' column as index and removing any empty rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1152d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing(data):\n",
    "    # Remove any rows with missing values and reset the index\n",
    "    data.replace('', np.nan, inplace=True)\n",
    "    data = data.dropna()\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b6f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/input/\"\n",
    "data_filename = \"reviews_data.csv\"\n",
    "data_file_path = os.path.join(input_folder_path, data_filename)\n",
    "\n",
    "df_raw = pd.read_csv(data_file_path)\n",
    "df_raw = df_raw[:5000]\n",
    "\n",
    "# Set ID as index\n",
    "df_raw.set_index('Id', inplace=True, drop=True)\n",
    "\n",
    "# Remove NaN rows, before cleaning text\n",
    "df_raw = drop_missing(df_raw)\n",
    "\n",
    "# Create a copy of the original DataFrame to preserve the original data\n",
    "df = df_raw.copy()\n",
    "\n",
    "print(df_raw.info())\n",
    "print(f'\\nDataset shape: {df_raw.shape}\\n')\n",
    "df_raw.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "622a91fc",
   "metadata": {},
   "source": [
    "### Analysing Data (TODO)\n",
    "We then analyse the dataset by observing the distribution of review per sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb16161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reviews per sentiment\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "\n",
    "# Print the counts for each category\n",
    "for sentiment_value, count in sentiment_counts.items():\n",
    "    sentiment_name = sentiment_labels[sentiment_value]\n",
    "    print(f\"{sentiment_value} ({sentiment_name}): {count} reviews\")\n",
    "\n",
    "# Define labels and colors for the pie chart\n",
    "labels = ['Positive', 'Neutral', 'Negative']\n",
    "colors = ['limegreen', 'dodgerblue', 'red']\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.pie(sentiment_counts, colors=colors, autopct='%1.1f%%',  pctdistance=0.8, textprops={'fontsize': 10, 'color': 'black'}, startangle=90)\n",
    "plt.axis('equal')  # pie as a circle\n",
    "plt.legend(labels=labels, loc='lower left')\n",
    "plt.title('Distribution of Reviews per Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc1bd547",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "675d3d0f",
   "metadata": {},
   "source": [
    "Next, we clean the data applying the following techniques (TODO: add info):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import EMOTICON_RE\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('words')\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from spellchecker import SpellChecker #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "#spell_checker = SpellChecker()\n",
    "#english_words = set(nltk.corpus.words.words())\n",
    "emojis = [\n",
    "        #HAPPY\n",
    "        \":-)\",\n",
    "        \":)\",\n",
    "        \";)\",\n",
    "        \":o)\",\n",
    "        \":]\",\n",
    "        \":3\",\n",
    "        \":c)\",\n",
    "        \":>\",\n",
    "        \"=]\",\n",
    "        \"8)\",\n",
    "        \"=)\",\n",
    "        \":}\",\n",
    "        \":^)\",\n",
    "        \":-D\",\n",
    "        \":D\",\n",
    "        \"8-D\",\n",
    "        \"8D\",\n",
    "        \"x-D\",\n",
    "        \"xD\",\n",
    "        \"X-D\",\n",
    "        \"XD\",\n",
    "        \"=-D\",\n",
    "        \"=D\",\n",
    "        \"=-3\",\n",
    "        \"=3\",\n",
    "        \":-))\",\n",
    "        \":'-)\",\n",
    "        \":')\",\n",
    "        \":*\",\n",
    "        \":^*\",\n",
    "        \">:P\",\n",
    "        \":-P\",\n",
    "        \":P\",\n",
    "        \"X-P\",\n",
    "        \"x-p\",\n",
    "        \"xp\",\n",
    "        \"XP\",\n",
    "        \":-p\",\n",
    "        \":p\",\n",
    "        \"=p\",\n",
    "        \":-b\",\n",
    "        \":b\",\n",
    "        \">:)\",\n",
    "        \">;)\",\n",
    "        \">:-)\",\n",
    "        \"<3\",\n",
    "        # SAD\n",
    "        \":L\",\n",
    "        \":-/\",\n",
    "        \">:/\",\n",
    "        \":S\",\n",
    "        \">:[\",\n",
    "        \":@\",\n",
    "        \":-(\",\n",
    "        \":[\",\n",
    "        \":-||\",\n",
    "        \"=L\",\n",
    "        \":<\",\n",
    "        \":-[\",\n",
    "        \":-<\",\n",
    "        \"=\\\\\",\n",
    "        \"=/\",\n",
    "        \">:(\",\n",
    "        \":(\",\n",
    "        \">.<\",\n",
    "        \":'-(\",\n",
    "        \":'(\",\n",
    "        \":\\\\\",\n",
    "        \":-c\",\n",
    "        \":c\",\n",
    "        \":{\",\n",
    "        \">:\\\\\",\n",
    "        \";(\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Lowercase\n",
    "df['Review'] = df['Review'].str.lower()\n",
    "#pd.set_option('display.max_rows', df.shape[0]+1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Replace contractions with their standard full forms\n",
    "contraction_mapping = {\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        }\n",
    "\n",
    "for contraction, standard in contraction_mapping.items():\n",
    "    df['Review'] = df['Review'].str.replace(contraction, standard)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Remove punctuation in between words e.g. \"course.sometimes\" \n",
    "# and replace with space\n",
    "pattern = r'(?<=\\w)[^\\w\\s]+(?=\\w)'\n",
    "df['Review'] = df['Review'].str.replace(pattern, ' ')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Tokenize text into individual words (removes all extra spaces \\s)\n",
    "tokenizer = TweetTokenizer()\n",
    "df['Review'] = df['Review'].apply(tokenizer.tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05647ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: three dots i.e. ... not removed\n",
    "# 5) Remove punctuation first in between words (typo),\n",
    "# and then all punctuation and numerals except for tokenized emojis\n",
    "pattern = r\"[^\\w\\s\" + \"\".join(re.escape(e) for e in emojis + list(emoji.EMOJI_DATA.keys())) + \"]|[\\d]+\" # match non-emoji special characters\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [token for token in tokens if not re.match(pattern, token)])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca98a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Remove single characters\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [word for word in tokens if len(word) > 1])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: package not loading\n",
    "'''\n",
    "# 7) Correct Spelling\n",
    "corrected_tokens = []\n",
    "for token in filtered_tokens:\n",
    "    if token in emojis or token in emoji.EMOJI_DATA.keys():\n",
    "        corrected_tokens.append(token)  # If token is an emoji, add it to the corrected tokens\n",
    "    else:\n",
    "        corrected_token = spell_checker.correction(token)\n",
    "        if corrected_token is not None:\n",
    "            corrected_tokens.append(corrected_token)\n",
    "#print('spell-check: '+str(corrected_tokens))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db373fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: note to self (to be added to word-doc): If you check token by token, it also removes english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Perform negation tagging\n",
    "df['Review'] = df['Review'].apply(mark_negation)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fce956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Remove stopwords --> also removes words like 'not'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [token for token in tokens if token.split('_')[0] not in stop_words])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: also lemmatize word removing _NEG\n",
    "# 10) Lemmatize words using WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert preprocessed tokens back to string\n",
    "df['Review'] = df['Review'].apply(' '.join)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25434a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape before: {df_raw.shape}')\n",
    "print(f'Shape after preprocessing, before removing empty rows: {df.shape}')\n",
    "\n",
    "# Remove NaN rows, after cleaning text\n",
    "df = drop_missing(df) \n",
    "print(f'Shape after preprocessing, after removing empty rows: {df.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset_path = \"cleaned_input/cleaned_data.csv\"\n",
    "df.to_csv(cleaned_dataset_path, sep=',', index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create word clouds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f18328f",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pickle\n",
    "from numpy import asarray\n",
    "import gensim.downloader as api\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f65f114",
   "metadata": {},
   "source": [
    "### Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f09fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Review'], df['Sentiment'],\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "# Split the training dataset further into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data Distribution:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(len(x_train), len(x_val), len(x_test)))\n",
    "print(f\"x_train: {x_train.head()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceb8e8d3",
   "metadata": {},
   "source": [
    "### Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b30607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words to create vocabulary\n",
    "word_counter = Counter()\n",
    "for review in x_train:\n",
    "    word_counter.update(review.split())\n",
    "\n",
    "print(word_counter.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter vocabulary by removing words with frequency less than a set minimum frequency\n",
    "vocab = [word for word, count in word_counter.items() if count >= MIN_FREQ]\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size of {} reduced to {}.\\n\".format(len(word_counter), vocab_size))\n",
    "print(\"Vocabulary (first 50 tokens):\\n{}\".format(vocab[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/processed\"\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(processed_folder_path):\n",
    "    os.makedirs(processed_folder_path)\n",
    "\n",
    "vocab_filename = 'vocab.txt'\n",
    "file_path = os.path.join(processed_folder_path, vocab_filename)\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write('\\n'.join(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83fc4550",
   "metadata": {},
   "source": [
    "### Filter data with vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_filter_dataset(docs, filename, vocab):\n",
    "    filtered_dataset = []\n",
    "    for doc in docs:\n",
    "        filtered_text = ' '.join([word for word in doc.split() if word in vocab])\n",
    "        filtered_dataset.append(filtered_text)\n",
    "\n",
    "    # Save filtered dataset to a txt file\n",
    "    filtered_filename = f'filtered_{str(filename)}.txt'\n",
    "    file_path = os.path.join(processed_folder_path, filtered_filename)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(filtered_dataset))\n",
    "\n",
    "    # Convert the processed documents back to pandas.Series\n",
    "    filtered_dataset = pd.Series(filtered_dataset, index=docs.index)\n",
    "\n",
    "    # Convert empty rows to '<empty>'\n",
    "    placeholder = \"<empty>\"\n",
    "    filtered_dataset.replace('', placeholder, inplace=True)\n",
    "    \n",
    "    # Count the number of rows with '<empty>' #TODO: remove empty rows\n",
    "    num_empty_rows = filtered_dataset.str.count('<empty>').sum()\n",
    "    print(f'Number of rows with <empty> for {filename}: {num_empty_rows}')\n",
    "\n",
    "    # TODO: instead of saving, print?\n",
    "    # Save filled dataset to a txt file\n",
    "    filled_filename = f'filled_{str(filename)}.txt'\n",
    "    file_path = os.path.join(processed_folder_path, filled_filename)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(filtered_dataset))\n",
    "    \n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136714f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter dataset based on vocabulary\n",
    "x_train = freq_filter_dataset(x_train, \"x_train\", vocab)\n",
    "x_val = freq_filter_dataset(x_val, \"x_val\", vocab)\n",
    "x_test = freq_filter_dataset(x_test, \"x_test\", vocab)\n",
    "\n",
    "print(\"\\nData Distribution:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(len(x_train), len(x_val), len(x_test)))\n",
    "print(f\"x_train - updated: {x_train.head()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8c6d509",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS \n",
    "\n",
    "# TF-IDF\n",
    "MAX_FEATURES = 10000\n",
    "MAX_DF = 0.95\n",
    "MIN_DF = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccee997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer with the filtered vocabulary\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES, # maximum number of features to keep, check unique vocabs and determine based on that, high causes saprse metrics and low value causes loss in important words/vocab\n",
    "    vocabulary=vocab,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1, 1),  # range of n-grams, only unigrams now\n",
    "    max_df=MAX_DF,  # ignore terms that have a document frequency strictly higher than the threshold\n",
    "    min_df=MIN_DF,  # ignore terms that have a document frequency strictly lower than the threshold.\n",
    "    use_idf=True,  # enable IDF weighting\n",
    "    smooth_idf=True,  # smooth IDF weights --> provides stability, reduces run time errors\n",
    "    sublinear_tf=True  # apply sublinear scaling to term frequencies\n",
    ")\n",
    "\n",
    "# Fit and transform the training set\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Transform the validation and testing set\n",
    "x_val_tfidf = tfidf_vectorizer.transform(x_val)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afdbb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tfidf_data(data, data_name, feature_names):\n",
    "    # Save the matrix with feature names as a DataFrame\n",
    "    data = pd.DataFrame(data.toarray(), columns=feature_names)\n",
    "    tfidf_filename = f'{data_name}.csv'\n",
    "    file_path = os.path.join(processed_folder_path, tfidf_filename)\n",
    "    data.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d62af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Save vectorized data\n",
    "save_tfidf_data(x_train_tfidf, \"train_tfidf\", feature_names)\n",
    "save_tfidf_data(x_train_tfidf, \"val_tfidf\", feature_names)\n",
    "save_tfidf_data(x_test_tfidf, \"test_tfidf\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Given vocabulary-size : {},\".format(vocab_size))\n",
    "print(\"\\nData Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_tfidf.shape, x_val_tfidf.shape, x_test_tfidf.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nData Types:\\nx_train_tfidf - type: {type(x_train_tfidf)}\\nx_val_tfidf - type: {type(x_val_tfidf)}\\ny-train - type: {type(y_train)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ca1595c",
   "metadata": {},
   "source": [
    "# Classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c72e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # TODO: not anymore?? Handle the zero-division error when there are no predicted samples for a label\n",
    "    # only interested in labels that were predicted at least once\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    \n",
    "    # Print results\n",
    "    print(model_name)\n",
    "    print(f\"Accuracy: {(accuracy * 100):.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"f1-score: {f1:.2f}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_report(y_true, y_pred):\n",
    "    # Calculate classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeea9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(accuracy, precision, recall, f1_score, report, model_name):\n",
    "    save_dir = f'results/{model_name}_results.txt'\n",
    "    with open(save_dir, 'w') as file:\n",
    "        file.write(model_name)\n",
    "        file.write(f\"Accuracy: {(accuracy * 100):.2f}\")\n",
    "        file.write(f\"Precision: {precision:.2f}\")\n",
    "        file.write(f\"Recall: {recall:.2f}\")\n",
    "        file.write(f\"f1-score: {f1_score:.2f}\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30335352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, model_name):\n",
    "    save_dir = f'results/{model_name}_confusion_matrix.png'\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cnf_matrix, display_labels=labels)\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    plt.savefig(save)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def plot(history, save_dir, model_name):\n",
    "    accuracy_plot = f'{save_dir}/{model_name}_plot.png'\n",
    "    loss_plot = f'{save_dir}/{model_name}_loss_plot.png'\n",
    "    \n",
    "    accuracy = history.history['accuracy']\n",
    "    #val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(accuracy))\n",
    "    plt.plot(epochs, accuracy, 'r', label='Training acc')\n",
    "    #plt.plot(epochs, val_accuracy, 'b', label='Validation acc')\n",
    "\n",
    "    plt.title(f'{model_name} Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(save_dir)\n",
    "    plt.close()\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    #val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "    plt.plot(epochs, loss, 'r', label='Training acc')\n",
    "    #plt.plot(epochs, val_loss, 'b', label='Validation acc')\n",
    "\n",
    "    plt.title(f'{model_name} Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(save_dir)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefe79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, x, y_true, only_metrics):\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(y_true, y_pred, model_name)\n",
    "    \n",
    "    if not only_metrics:\n",
    "        # Calculate classification report\n",
    "        report = calculate_classification_report(y_true, y_pred)\n",
    "        save_results(accuracy, precision, recall, f1, report, model_name)\n",
    "\n",
    "        # Plot accuracy # TODO\n",
    "        #plot(history, model_name)\n",
    "\n",
    "        # Plot confusion matrix # TODO\n",
    "        senti_labels = ['negative', 'neutral', 'positive']\n",
    "        #plot_confusion_matrix(y_test, y_pred, senti_labels, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10848f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top3_models(top3_models):    \n",
    "    # Print the sorted list of mean test scores and standard deviation of test scores\n",
    "    print(\"\\nTop 3 parameter combinations ranked by performance (from best to worst):\")\n",
    "    for index, row in top3_models.iterrows():\n",
    "        mean_score = row['mean_test_score']\n",
    "        std_score = row['std_test_score']\n",
    "        params = row['params']\n",
    "        print(f\"Mean Test Score: {mean_score:.4f} (±{std_score:.4f}) for {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3dba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_val(x_val, model, model_name, params):\n",
    "    y_pred = model.predict(x_val)\n",
    "    \n",
    "    calculate_metrics(y_val, y_pred, model_name)\n",
    "    print(\"Params: {}\\n\".format(params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b616c18",
   "metadata": {},
   "source": [
    "## 1. Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6e69ba1",
   "metadata": {},
   "source": [
    "### Training & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Random Forest model\n",
    "rf_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be257e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    #'min_samples_split': [2, 5, 10],\n",
    "    #'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    #'max_features': ['auto', 'sqrt'],  # Number of features to consider when looking for the best split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=rf_param_grid, cv=5)\n",
    "grid_search.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "top3_models = sorted_results[:5] # TODO: update to 3\n",
    "print_top3_models(top3_models)\n",
    "\n",
    "top3_models = sorted_results[:3] # TODO: and delete this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "159c13c0",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26126294",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_params = top3_models['params'].values\n",
    "\n",
    "# Evaluate the top 3 models on the validation set\n",
    "rf_cand_0 = RandomForestClassifier(**top3_params[0])\n",
    "rf_cand_0.fit(x_train_tfidf, y_train)\n",
    "evaluate_model(rf_cand_0, \"RF-0\", x_val_tfidf, y_val, only_metrics=True)\n",
    "print(f\"Params: {top3_params[0]}\\n\")\n",
    "\n",
    "rf_cand_1 = RandomForestClassifier(**top3_params[1])\n",
    "rf_cand_1.fit(x_train_tfidf, y_train)\n",
    "evaluate_model(rf_cand_1, \"RF-1\", x_val_tfidf, y_val, only_metrics=True)\n",
    "print(f\"Params: {top3_params[1]}\\n\")\n",
    "\n",
    "rf_cand_2 = RandomForestClassifier(**top3_params[2])\n",
    "rf_cand_2.fit(x_train_tfidf, y_train)\n",
    "evaluate_model(rf_cand_2, \"RF-2\", x_val_tfidf, y_val, only_metrics=True)\n",
    "print(f\"Params: {top3_params[2]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdaaf668",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "rf_best = rf_cand_0\n",
    "y_pred = rf_best.predict(x_test_tfidf)\n",
    "evaluate_model(rf_best, \"RF-best\", x_test_tfidf, y_test, only_metrics=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1790120",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "214966ae",
   "metadata": {},
   "source": [
    "### Training & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Naive Bayes model & fit on training data\n",
    "nb_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with TF-IDF vectorizer and multinomial Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vectorizer),  # Replace tfidf_vectorizer with your existing TF-IDF vectorizer\n",
    "    ('nb_clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "nb_param_grid = {\n",
    "    'tfidf__max_features': [1000, 5000, 10000],  # Maximum number of features\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Range of n-grams\n",
    "    'nb_clf__alpha': [0.1, 0.5, 1.0],  # Smoothing parameter for MultinomialNB\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(\"type of x_train_tfidf: \", type(x_train_tfidf))\n",
    "print(\"type of x_train_tfidf: \", type(y_train))\n",
    "print(\"shape of x_train_tfidf: \", x_train_tfidf.shape)\n",
    "print(\"shape of x_train_tfidf: \", y_train.shape)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid=nb_param_grid, cv=5, error_score='raise')\n",
    "grid_search.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Get the best parameters and best score from grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "top3_models = sorted_results[:5] # TODO: update 10 to 3\n",
    "print_top3_models(top3_models)\n",
    "top3_models = sorted_results[:3] # TODO: and delete this\n",
    "top3_params = top3_models['params'].values\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fae5d8b7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dd270a0",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d223208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d7f0cd9",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bad5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3287aee",
   "metadata": {},
   "source": [
    "## 3. SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f34d76f",
   "metadata": {},
   "source": [
    "### Training &  Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the SVM model\n",
    "svm_model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': [0.1, 1, 'scale']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(svm_model, param_grid=svm_param_grid, cv=5)\n",
    "grid_search.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "top3_models = sorted_results[:5] # TODO: update to 3\n",
    "print_top3_models(top3_models)\n",
    "\n",
    "top3_models = sorted_results[:3] # TODO: and delete this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceb654f8",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "525c68dd",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4da28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top3_params = top3_models['params'].values\n",
    "\n",
    "# Evaluate the top 3 models on the validation set\n",
    "svm_cand_0 = SVC(**top3_params[0])\n",
    "svm_cand_0.fit(x_train_tfidf, y_train)\n",
    "evaluate_model(svm_cand_0, \"SVM-0\", x_val_tfidf, y_val, only_metrics=True)\n",
    "print(f\"Params: {top3_params[0]}\\n\")\n",
    "\n",
    "svm_cand_1 = SVC(**top3_params[1])\n",
    "svm_cand_1.fit(x_train_tfidf, y_train)\n",
    "evaluate_model(svm_cand_1, \"SVM-0\", x_val_tfidf, y_val, only_metrics=True)\n",
    "print(f\"Params: {top3_params[0]}\\n\")\n",
    "\n",
    "svm_cand_2 = SVC(**top3_params[2])\n",
    "svm_cand_2.fit(x_train_tfidf, y_train)\n",
    "evaluate_model(svm_cand_2, \"SVM-0\", x_val_tfidf, y_val, only_metrics=True)\n",
    "print(f\"Params: {top3_params[0]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0071c488",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "svm_best = svm_cand_0\n",
    "y_pred = svm_best.predict(x_test_tfidf)\n",
    "evaluate_model(svm_best, \"SVM-best\", x_test_tfidf, y_test, only_metrics=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9982ae27",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d99acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find maximum sequence length\n",
    "max_seq_length = max([len(doc.split()) for doc in x_train])\n",
    "print(f'\\nMaximum review length: {max_seq_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tokenizer (on training data)\n",
    "tokenizer = Tokenizer()\n",
    "# Remove default filters, including punctuation\n",
    "tokenizer.filters = \"\"  \n",
    "# Disable lowercase conversion\n",
    "tokenizer.lower = False  \n",
    "tokenizer.fit_on_texts(x_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb76b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(lines, tokenizer, max_length):\n",
    "    # Integer encode\n",
    "    encoded_seq = tokenizer.texts_to_sequences(lines)\n",
    "    # Pad the encoded sequences\n",
    "    padded = pad_sequences(encoded_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data\n",
    "x_train_encoded = encode_text(x_train, tokenizer, max_seq_length)\n",
    "x_val_encoded = encode_text(x_val, tokenizer, max_seq_length)\n",
    "x_test_encoded = encode_text(x_test, tokenizer, max_seq_length)\n",
    "\n",
    "print(\"Encoded-data shapes:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_encoded.shape, x_val_encoded.shape, x_test_encoded.shape))\n",
    "print(f\"x_train_encoded[:3]:\\n{x_val_encoded[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure labels\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "y_test = y_test.values\n",
    "print(\"target-data shapes:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train.shape, y_val.shape, y_test.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "527d5290",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total vocabulary size plus 0 for unknown words\n",
    "embedding_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"embedding_vocab_size: \", embedding_vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a3248ef",
   "metadata": {},
   "source": [
    "Checking why embedding vocab_size is 2 greater than original vocab size due to <empty> \n",
    "    #TODO: remove rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e71425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tokenizer word index into a set\n",
    "tokenizer_words = set(tokenizer.word_index.keys())\n",
    "\n",
    "# Convert the manual vocabulary into a set\n",
    "vocab_set = set(vocab)\n",
    "\n",
    "# Find the words in tokenizer but not in vocab\n",
    "tokenizer_only_words = tokenizer_words.difference(vocab_set)\n",
    "\n",
    "print(\"Words in tokenizer but not in vocab:\")\n",
    "print(tokenizer_only_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b891123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: gigaword or twitter?\n",
    "def load_embedding():\n",
    "    # Check if the pre-trained Word2Vec model is already downloaded\n",
    "    #w2v_pretrained_model = \"glove-twitter-100\"\n",
    "    w2v_pretrained_model = \"glove-wiki-gigaword-100\"\n",
    "    w2v_pretrained_model_filename = str(w2v_pretrained_model) + \"-word2vec.txt\"\n",
    "    if not os.path.exists(w2v_pretrained_model_filename):\n",
    "        print(\"\\nw2v model doesn't exist\")\n",
    "        # If the model does not exist, download it\n",
    "        model = api.load(\"glove-twitter-100\")\n",
    "        # Save the word2vec embeddings in the appropriate format\n",
    "        model.save_word2vec_format(w2v_pretrained_model_filename, binary=False)\n",
    "\n",
    "    # load embedding into memory, skip first line\n",
    "    print(\"Loading w2v model...\")\n",
    "    file = open(w2v_pretrained_model_filename, 'r', encoding='utf8')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2010e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_embedding = load_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8aec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matrix(embedding, tokenizer):\n",
    "    # create a weight matrix for the Embedding layer from a loaded embedding\n",
    "\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((embedding_vocab_size, EMBEDDING_DIM))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    count_all = 0\n",
    "    count_na = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # TODO: important note, pretrained word2vec model removes all neg_ and emojis (also other words) that are\n",
    "        #  not defined in the model it These values should prob? also be removed from the vocab (and update vocab size) to avoid mismatch in the embedding layer\n",
    "        if word in embedding.keys():\n",
    "            # print(embedding.get(word)[:3])\n",
    "            weight_matrix[i] = embedding.get(word)\n",
    "        else:\n",
    "            #print(word)\n",
    "            count_na += 1\n",
    "        count_all += 1\n",
    "    print(f'count_na/count_all: {str(count_na)}/{count_all}')\n",
    "    print(f\"embedding matrix shape: {weight_matrix.shape}\")\n",
    "\n",
    "    # save model in ASCII (word2vec) format\n",
    "    w2v_filename =  'processed/weight_matrix_word2vec.txt'\n",
    "    file_path = os.path.join(processed_folder_path, w2v_filename)\n",
    "    with open(w2v_filename, 'w') as file:\n",
    "        file.write('\\n'.join(' '.join(str(x) for x in row) for row in weight_matrix))\n",
    "    \n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embedding_vectors = get_weight_matrix(raw_embedding, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fa517b2",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate, LSTM, Dense, Conv1D, Dropout, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment labels to one-hot encoding\n",
    "num_classes = 3  # Number of sentiment classes [pos, neg, neut]\n",
    "y_train_encoded = np.zeros((len(y_train), num_classes))\n",
    "for i, label in enumerate(y_train):\n",
    "    y_train_encoded[i, label - 1] = 1\n",
    "\n",
    "y_val_encoded = np.zeros((len(y_val), num_classes))\n",
    "for i, label in enumerate(y_val):\n",
    "    y_val_encoded[i, label - 1] = 1\n",
    "\n",
    "y_test_encoded = np.zeros((len(y_test), num_classes))\n",
    "for i, label in enumerate(y_test):\n",
    "    y_test_encoded[i, label - 1] = 1\n",
    "\n",
    "    \n",
    "print(\"Check one-hot encoding:\\n\", y_train_encoded[:3])    \n",
    "print(\"\\ny-encoded Data Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape))\n",
    "print(\"\\nx_train_encoded - type:\", type(x_train_encoded))\n",
    "print(\"y_train_encoded - type:\", type(y_train_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0b1edcd",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4958c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reorganize?\n",
    "# TODO: early stopping?\n",
    "# TODO: evaluation from the general functions\n",
    "# TODO: model architecture aspects (dropout) etc.\n",
    "# TODO: add train accuracy where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69975c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeefe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(score, model_name):\n",
    "    acc =  (score[1] * 100)\n",
    "    loss = score[0]\n",
    "    print(\"{} Accuracy: {}\".format(model_name, acc))\n",
    "    print(\"{} Loss: {}\".format(model_name, loss))\n",
    "    \n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(accuracy, loss, report, model_name):\n",
    "        # Save results\n",
    "    save_dir = f'results/{model_name}_results.txt'\n",
    "    with open(save_dir, 'w') as file:\n",
    "        file.write(f'{model_name} Accuracy: {accuracy}\\n')\n",
    "        file.write(f'{model_name} Loss: {loss}\\n')\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, x_test_encoded, y_test_encoded, y_test_true, params):\n",
    "    print(f'{model_name} Testing complete!\\n')\n",
    "    \n",
    "    score = model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "    # Calculate and save metrics\n",
    "    loss, accuracy = calculate_metrics(score, model_name)\n",
    "    \n",
    "    # Predict labels for the validation set\n",
    "    y_pred = model.predict(x_test_encoded)\n",
    "    # Convert one-hot encoded labels back to original format\n",
    "    y_pred = np.argmax(y_pred, axis=1)     \n",
    "    # Calculate classification report\n",
    "    report = calculate_classification_report(y_test_true, y_pred)\n",
    "    \n",
    "    # Save results\n",
    "    save_results(accuracy, loss, report, model_name)\n",
    "\n",
    "    # Plot accuracy # TODO\n",
    "    #plot(history, model_name)\n",
    "\n",
    "    # Plot confusion matrix # TODO\n",
    "    senti_labels = ['negative', 'neutral', 'positive']\n",
    "    #plot_confusion_matrix(y_test, y_pred, senti_labels, model_name)\n",
    "    \n",
    "    print(\"Params: {}\\n\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(x_val_encoded, y_val_encoded, model, model_name, params):\n",
    "    score = model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss, accuracy = calculate_metrics(score, model_name)\n",
    "    print(\"Params: {}\\n\".format(params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8fdb95a",
   "metadata": {},
   "source": [
    "### Single - Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the LSTM model\n",
    "def define_lstm_model(units, dropout_rate): #optimizer, learning_rate\n",
    "    single_lstm_model = Sequential()\n",
    "    single_lstm_model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length))\n",
    "    single_lstm_model.add(Dropout(dropout_rate))\n",
    "    single_lstm_model.add(LSTM(units=units))\n",
    "    single_lstm_model.add(Dense(3, activation='softmax'))\n",
    "    single_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return single_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set epochs and batch size\n",
    "single_lstm_model = KerasClassifier(build_fn=define_lstm_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46363d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "lstm_param_grid = {\n",
    "    'units': [32, 64, 128],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    #'optimizer': [Adam, SGD, RMSprop, Adagrad],\n",
    "    #'learning_rate': [0.001, 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce49c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=single_lstm_model, param_grid=lstm_param_grid, cv=3)\n",
    "grid_search.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "# Get the best parameters and best score from grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "top3_models = sorted_results[:5] # TODO: update 10 to 3\n",
    "print_top3_models(top3_models)\n",
    "top3_models = sorted_results[:3] # TODO: and delete this\n",
    "top3_params = top3_models['params'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d084baf",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c5e9303",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the top 3 models on the validation set\n",
    "# TODO: remove f1_score  \n",
    "# TODO: early stopping & batch_size?\n",
    "single_lstm_candidate_1 = define_lstm_model(units=top3_params[0]['units'], dropout_rate=top3_params[0]['dropout_rate'])\n",
    "single_lstm_candidate_1.fit(x_train_encoded, y_train_encoded, epochs=3)\n",
    "\n",
    "single_lstm_candidate_2 = define_lstm_model(units=top3_params[1]['units'], dropout_rate=top3_params[1]['dropout_rate'])\n",
    "single_lstm_candidate_2.fit(x_train_encoded, y_train_encoded, epochs=3)\n",
    "\n",
    "single_lstm_candidate_3 = define_lstm_model(units=top3_params[2]['units'], dropout_rate=top3_params[2]['dropout_rate'])\n",
    "single_lstm_candidate_3.fit(x_train_encoded, y_train_encoded, epochs=3)\n",
    "\n",
    "\n",
    "# TODO: after fixing plotting set boolean to include or exclude plotting etc.\n",
    "predict_data(x_val_encoded, y_val_encoded, single_lstm_candidate_1,  \"LSTM-single-1\", top3_params[0])\n",
    "predict_data(x_val_encoded, y_val_encoded, single_lstm_candidate_2, \"LSTM-single-2\", top3_params[1])\n",
    "predict_data(x_val_encoded, y_val_encoded, single_lstm_candidate_3, \"LSTM-single-3\", top3_params[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc6833b2",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "single_lstm_best = single_lstm_candidate_1\n",
    "evaluate_model(single_lstm_best,  \"LSTM-single-best\", x_test_encoded, y_test_encoded, y_test, top3_params[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7ab0df6",
   "metadata": {},
   "source": [
    "### Multi - Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cfd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: copy-paste above to multi\n",
    "def define_multi_channel_lstm_model(units1, units2, dense_units):\n",
    "    # Vocabulary-based embedding layer\n",
    "    inputs1 = Input(shape=(max_seq_length,))\n",
    "    embedding1 = Embedding(embedding_vocab_size, EMBEDDING_DIM,\n",
    "                           input_length=max_seq_length)(inputs1)\n",
    "    lstm1 = LSTM(units=units1)(embedding1)\n",
    "\n",
    "    # Word2Vec embedding layer\n",
    "    inputs2 = Input(shape=(max_seq_length,))\n",
    "    embedding2 = Embedding(embedding_vocab_size, EMBEDDING_DIM,\n",
    "                           input_length=max_seq_length,\n",
    "                           weights=[w2v_embedding_vectors], trainable=False)(inputs2)\n",
    "    lstm2 = LSTM(units=units2)(embedding2)\n",
    "\n",
    "    # Concatenate the two inputs\n",
    "    merged = concatenate([lstm1, lstm2])\n",
    "\n",
    "    # Dense layer for the merged inputs & Output Layer\n",
    "    merged_dense = Dense(units=dense_units, activation='relu')(merged)\n",
    "    outputs = Dense(3, activation='softmax')(merged_dense)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lstm_model = KerasClassifier(build_fn=define_multi_channel_lstm_model, verbose=0)\n",
    "# multi_lstm_model = define_multi_channel_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "multi_lstm_param_grid = {\n",
    "    'units1': [64, 128],\n",
    "    'units2': [64, 128],\n",
    "    'dense_units': [32, 64],\n",
    "    #'batch_size': [16, 32],\n",
    "    #'epochs': [10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_combinations = product(*multi_lstm_param_grid.values())\n",
    "models = []\n",
    "\n",
    "for params in param_combinations:\n",
    "    print(params)\n",
    "    units1, units2, dense_units = params\n",
    "    \n",
    "    multi_lstm_model = define_multi_channel_lstm_model(\n",
    "        units1=units1,\n",
    "        units2=units2,\n",
    "        dense_units=dense_units\n",
    "    )\n",
    "    \n",
    "    x_train = [x_train_encoded, x_train_encoded]\n",
    "    y_train = asarray(y_train_encoded)\n",
    "    \n",
    "    multi_lstm_model.fit(x_train, y_train)\n",
    "    loss, accuracy = multi_lstm_model.evaluate(x_train, y_train)\n",
    "    \n",
    "    accuracy = accuracy * 100\n",
    "    models.append({\n",
    "            'units1': units1,\n",
    "            'units2': units2,\n",
    "            'dense_units': dense_units,\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top3_models(top3_models):\n",
    "    print(\"\\nTop 3 parameter combinations ranked by performance (from best to worst):\")\n",
    "    for index, row in top3_models.iterrows():\n",
    "        units1 = row['units1']\n",
    "        units2 = row['units2']\n",
    "        dense_units = row['dense_units']\n",
    "        loss = row['loss']\n",
    "        accuracy = row['accuracy']\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}, Loss: {loss:.4f} for units1: {units1}, units2: {units2}, dense_units: {dense_units}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f03c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of models to a pandas DataFrame\n",
    "top3_models = pd.DataFrame(models)\n",
    "\n",
    "# Sort models based on accuracy in descending order and loss in ascending order\n",
    "top3_models = top3_models.sort_values(by=['accuracy', 'loss'], ascending=[False, False])\n",
    "\n",
    "top3_models = models[:5] # TODO: change 5 to 3\n",
    "top3_models = pd.DataFrame(top3_models)\n",
    "\n",
    "print_top3_models(top3_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14999c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in top3_models.iterrows():\n",
    "    units1 = int(row['units1'])\n",
    "    units2 = int(row['units2'])\n",
    "    dense_units = int(row['dense_units'])\n",
    "\n",
    "    multi_lstm_candidate_model = define_multi_channel_lstm_model(\n",
    "        units1=units1,\n",
    "        units2=units2,\n",
    "        dense_units=dense_units\n",
    "    )\n",
    "\n",
    "    x_train = [x_train_encoded, x_train_encoded]\n",
    "    y_train = asarray(y_train_encoded)\n",
    "\n",
    "    multi_lstm_candidate_model.fit(x_train, y_train)\n",
    "    # save the model\n",
    "    multi_lstm_model.save(f'multi-lstm-model-{index}.h5')\n",
    "\n",
    "    x_val = [x_val_encoded, x_val_encoded]\n",
    "    predict_data(x_val, y_val_encoded, multi_lstm_candidate_model,  f\"LSTM-multi-{index}\", top3_params[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4410f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# set index and load the model\n",
    "index = 0\n",
    "multi_lstm_best = load_model(f'multi-lstm-model-{index}.h5')\n",
    "x_test = [x_test_encoded, x_test_encoded]\n",
    "evaluate_model(multi_lstm_best,  \"LSTM-multi-best\", x_test, y_test_encoded, y_test, top3_params[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d38e720",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "658bc4eb",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bae840e2",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
