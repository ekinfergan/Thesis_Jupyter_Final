{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from numpy import asarray\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, RocCurveDisplay, confusion_matrix, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate, GRU, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.legacy import Adam, SGD, RMSprop, Adagrad\n",
    "\n",
    "import skopt\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer  \n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "# DATASET\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "# Define a dictionary to map sentiment values to category names\n",
    "senti_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_categories = list(senti_labels.values())\n",
    "NUM_of_CLASSES = 3\n",
    "\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/input/\"\n",
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.22519    -0.34231001  0.049165   ...  0.38266    -0.14099\n",
      "  -0.14488   ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.38699001  0.37981001  0.12822001 ...  0.44394001  0.27914\n",
      "  -0.27467999]\n",
      " [ 0.82581002 -0.17398    -0.36208999 ...  0.01873    -0.34252\n",
      "  -0.49366999]\n",
      " [ 0.064471    0.83850998 -0.22317    ...  0.028637    0.63722003\n",
      "  -0.78961998]]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(input_folder_path, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(input_folder_path, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(input_folder_path, \"test.csv\"))\n",
    "\n",
    "x_train = train['x']\n",
    "y_train = train['y']\n",
    "x_val = val['x']\n",
    "y_val = val['y']\n",
    "x_test = test['x']\n",
    "y_test = test['y']\n",
    "\n",
    "x_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_x.npy\"))\n",
    "y_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_y.npy\"))\n",
    "x_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_x.npy\"))\n",
    "y_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_y.npy\"))\n",
    "x_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_x.npy\"))\n",
    "y_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_y.npy\"))\n",
    "\n",
    "w2v_embedding_vectors = np.load(os.path.join(processed_folder_path, \"embedding_w2v_matrix.npy\"))\n",
    "print(w2v_embedding_vectors)\n",
    "\n",
    "%store -r embedding_vocab_size\n",
    "%store -r EMBEDDING_DIM\n",
    "%store -r max_seq_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(score):\n",
    "    acc =  score[1]\n",
    "    loss = score[0]\n",
    "\n",
    "    print(f\"Accuracy: {acc:.2%}\")\n",
    "    print(f\"Loss: {loss:.2f}\")\n",
    "    \n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_report(y, y_pred, labels):\n",
    "    report = classification_report(y, y_pred, labels=labels)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
    "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=labels)\n",
    "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, x_encoded, y_encoded, y=None, only_metrics=True):    \n",
    "    y_pred_prob = model.predict(x_encoded)\n",
    "\n",
    "    print(f\"*{model_name}\")\n",
    "    \n",
    "    score = model.evaluate(x_encoded, y_encoded, verbose=0)\n",
    "    calculate_metrics(score)\n",
    "    \n",
    "    senti_labels = ['negative', 'neutral', 'positive'] #TODO: to constants\n",
    "    \n",
    "    if not only_metrics:\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1) + 1\n",
    "        calculate_classification_report(y, y_pred, labels=senti_labels)\n",
    "        plot_confusion_matrix(y, y_pred, labels=senti_labels)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(prob_test_vec, y_test, labels):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels\n",
    "    colors = cycle(['limegreen', 'dodgerblue', 'red'])\n",
    "    for senti, color in zip(range(NUM_of_CLASSES), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_test[:, senti],\n",
    "            prob_test_vec[:, senti],\n",
    "            name=f\"ROC curve for {labels[senti]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_OvR_roc_auc_score(model, x, y, x_test, y_test, labels): #average??\n",
    "    #y = one_hot_encode(y)\n",
    "    #y_test = one_hot_encode(y_test)\n",
    "\n",
    "    ovr_model = OneVsRestClassifier(model).fit(x, y)\n",
    "    prob_test_vec = ovr_model.predict_proba(x_test)\n",
    "    \n",
    "    fpr, tpr, thresholds, auc_score = [], [], [], []\n",
    "    for _ in range(NUM_of_CLASSES):\n",
    "        fpr.append(0)\n",
    "        tpr.append(0)\n",
    "        thresholds.append(0)\n",
    "        auc_score.append(0)\n",
    "    \n",
    "    for i in range(NUM_of_CLASSES):\n",
    "        fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], prob_test_vec[:, i])\n",
    "        auc_score[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    print(f\"AUC score: {auc_score}\")\n",
    "    averaged_auc_score = (sum(auc_score) / NUM_of_CLASSES)\n",
    "    print(f\"Averaged AUC score: {averaged_auc_score:.2f}\")\n",
    "    \n",
    "    plot_roc_curve(prob_test_vec, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_development(history):\n",
    "    acc =  history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "    \n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a basic neural net to see the baseline for accuracy with minimum tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_classes = 3\n",
    "batch_size= 32\n",
    "epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 15:57:19.176721: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/snappy/1.1.9-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/nsync/1.25.0-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/LMDB/0.9.29-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/JsonCpp/1.9.5-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/ICU/71.1-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/giflib/5.2.1-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/double-conversion/3.2.0-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/cURL/7.83.0-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/HDF5/1.12.2-gompi-2022a/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/Szip/2.1.1-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/cuDNN/8.4.1.50-CUDA-11.7.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/CUDA/11.7.0/nvvm/lib64:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/CUDA/11.7.0/extras/CUPTI/lib64:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/CUDA/11.7.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.7.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/magma/2.6.2-foss-2022a-CUDA-11.7.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/GDRCopy/2.3-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/LibTIFF/4.3.0-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libdeflate/1.10-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/zstd/1.5.2-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/lz4/1.9.3-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/jbigkit/2.1-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libjpeg-turbo/2.1.3-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/FFmpeg/4.4.2-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/FriBidi/1.0.12-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/X11/20220504-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/fontconfig/2.14.0-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/util-linux/2.38-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/freetype/2.12.1-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/Brotli/1.0.9-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libpng/1.6.37-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/expat/2.4.8-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/x265/3.5-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/LAME/3.100-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/x264/20220620-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/MPFR/4.1.0-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libyaml/0.2.5-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/numpy/core/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/protobuf/3.19.4-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/Python/3.10.4-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libffi/3.4.2-GCCcore-11.3.0/lib64:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/GMP/6.2.1-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/SQLite/3.38.3-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/Tcl/8.6.12-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libreadline/8.1.2-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/ncurses/6.3-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/bzip2/1.0.8-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/ScaLAPACK/2.2.0-gompi-2022a-fb/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/FFTW.MPI/3.3.10-gompi-2022a/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/FFTW/3.3.10-GCC-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/FlexiBLAS/3.2.0-GCC-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/OpenBLAS/0.3.20-GCC-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/OpenMPI/4.1.4-GCC-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/UCC/1.0.0-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/PMIx/4.1.2-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libfabric/1.15.1-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/UCX/1.12.1-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libevent/2.1.12-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/hwloc/2.7.1-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libpciaccess/0.16-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/libxml2/2.9.13-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/XZ/5.2.5-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/numactl/2.0.14-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/binutils/2.38-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/zlib/1.2.12-GCCcore-11.3.0/lib:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/GCCcore/11.3.0/lib64:/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/OpenSSL/1.1/lib\n",
      "2023-06-09 15:57:19.176781: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_layer (Embedding)  (None, 13, 100)          1100      \n",
      "                                                                 \n",
      " hidden_layer (GRU)          (None, 64)                31872     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,167\n",
      "Trainable params: 33,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "110/110 [==============================] - 3s 13ms/step - loss: 1.0699 - accuracy: 0.4389 - val_loss: 0.8050 - val_accuracy: 0.6657\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9172 - accuracy: 0.5397 - val_loss: 0.7277 - val_accuracy: 0.6717\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8981 - accuracy: 0.5380 - val_loss: 0.7355 - val_accuracy: 0.6521\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8918 - accuracy: 0.5357 - val_loss: 0.7218 - val_accuracy: 0.6627\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8889 - accuracy: 0.5423 - val_loss: 0.7223 - val_accuracy: 0.6958\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8826 - accuracy: 0.5357 - val_loss: 0.9288 - val_accuracy: 0.5949\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8755 - accuracy: 0.5411 - val_loss: 0.7045 - val_accuracy: 0.6747\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8627 - accuracy: 0.5520 - val_loss: 0.8279 - val_accuracy: 0.5904\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8572 - accuracy: 0.5591 - val_loss: 0.6488 - val_accuracy: 0.8584\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8350 - accuracy: 0.5671 - val_loss: 0.6747 - val_accuracy: 0.6521\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.6543 - accuracy: 0.6517\n",
      "Naive model Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# input layer is sequence of integers (words)\n",
    "model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, name=\"embedding_layer\")) # part of input layer as it transforms integers into dense vectors, input shape = (None, max_seq_length)\n",
    "model.add(GRU(64, name='hidden_layer')) # hidden layer\n",
    "model.add(Dense(num_output_classes, activation='softmax', name=\"output_layer\"))\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "basic_history = model.fit(x_train_encoded, y_train_encoded, validation_data=(x_val_encoded, y_val_encoded), batch_size=batch_size, epochs=10)\n",
    "\n",
    "accuracy = model.evaluate(x_test_encoded, y_test_encoded)[1]\n",
    "print(f\"Naive model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "del model\n",
    "\n",
    "backend.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our naive model, we get an accuracy of x%. # TODO: x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gru_layers = Integer(low=1, high=5, name='num_gru_layers')\n",
    "num_gru_units = Integer(low=32, high=256, name='num_gru_units') # TODO: step Keras Tuner\n",
    "learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "adam_decay = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n",
    "#batch_size = Integer(low=1, high=128, name='batch_size')\n",
    "\n",
    "search_space = [\n",
    "            num_gru_layers,\n",
    "            num_gru_units,\n",
    "            learning_rate,\n",
    "            adam_decay\n",
    "            ]\n",
    "\n",
    "# Specify one or more initial points for the search of optimal parameter\n",
    "default_params = [1,\n",
    "                  32,\n",
    "                  1e-3, \n",
    "                  1e-3,\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gru_model(num_gru_layers, num_gru_units, learning_rate, adam_decay):\n",
    "    # Start the model making process and create our first layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length))\n",
    "\n",
    "    # Create a loop making a new GRU layer for the amount passed to this model.\n",
    "    # Naming the layers helps avoid tensorflow error deep in the stack trace.\n",
    "    for i in range(num_gru_layers):\n",
    "        name = 'layer_gru_{0}'.format(i+1)\n",
    "        if i < num_gru_layers-1:\n",
    "            model.add(GRU(num_gru_units, return_sequences=True, name=name))\n",
    "        else:\n",
    "            model.add(GRU(num_gru_units, return_sequences=False, name=name))\n",
    "\n",
    "    # Add our classification layer.\n",
    "    model.add(Dense(num_output_classes, activation='softmax'))\n",
    "\n",
    "    # Setup our optimizer and compile\n",
    "    adam = Adam(learning_rate=learning_rate, decay=adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=search_space)\n",
    "def objective_function(num_gru_layers, num_gru_units, learning_rate, adam_decay):\n",
    "\n",
    "    model = define_gru_model(num_gru_layers=num_gru_layers,\n",
    "                         num_gru_units=num_gru_units,\n",
    "                         learning_rate=learning_rate,\n",
    "                         adam_decay=adam_decay\n",
    "                         )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    loss = history.history['val_loss'][-1]\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Loss: {loss:.2}\\n\")\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13, 100)           1100      \n",
      "                                                                 \n",
      " layer_gru_1 (GRU)           (None, 32)                12864     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,063\n",
      "Trainable params: 14,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 3s 10ms/step - loss: 1.0808 - accuracy: 0.4283 - val_loss: 0.8440 - val_accuracy: 0.9051\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9840 - accuracy: 0.5054 - val_loss: 0.6644 - val_accuracy: 0.8870\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.8977 - accuracy: 0.5466 - val_loss: 0.7669 - val_accuracy: 0.6657\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.8854 - accuracy: 0.5409 - val_loss: 0.7313 - val_accuracy: 0.6642\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.8751 - accuracy: 0.5503 - val_loss: 0.7964 - val_accuracy: 0.5181\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.8598 - accuracy: 0.5617 - val_loss: 0.7278 - val_accuracy: 0.5783\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8509 - accuracy: 0.5626 - val_loss: 0.6692 - val_accuracy: 0.8012\n",
      "Accuracy: 80.12%\n",
      "Loss: 0.67\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13, 100)           1100      \n",
      "                                                                 \n",
      " layer_gru_1 (GRU)           (None, 13, 206)           190344    \n",
      "                                                                 \n",
      " layer_gru_2 (GRU)           (None, 206)               255852    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 621       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 447,917\n",
      "Trainable params: 447,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 8s 42ms/step - loss: 0.9843 - accuracy: 0.5080 - val_loss: 0.7658 - val_accuracy: 0.7033\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 3s 30ms/step - loss: 0.9049 - accuracy: 0.5380 - val_loss: 0.8600 - val_accuracy: 0.5557\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 3s 31ms/step - loss: 0.8685 - accuracy: 0.5514 - val_loss: 0.7598 - val_accuracy: 0.6611\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 3s 30ms/step - loss: 0.8474 - accuracy: 0.5749 - val_loss: 0.7039 - val_accuracy: 0.6822\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 3s 29ms/step - loss: 0.8253 - accuracy: 0.5920 - val_loss: 0.7099 - val_accuracy: 0.6807\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 3s 31ms/step - loss: 0.8088 - accuracy: 0.5943 - val_loss: 0.7301 - val_accuracy: 0.6175\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.7817 - accuracy: 0.5949 - val_loss: 0.7526 - val_accuracy: 0.6084\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 3s 30ms/step - loss: 0.7631 - accuracy: 0.6003 - val_loss: 0.7412 - val_accuracy: 0.6160\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.7546 - accuracy: 0.6103 - val_loss: 0.7096 - val_accuracy: 0.6190\n",
      "Accuracy: 61.90%\n",
      "Loss: 0.71\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13, 100)           1100      \n",
      "                                                                 \n",
      " layer_gru_1 (GRU)           (None, 13, 235)           237585    \n",
      "                                                                 \n",
      " layer_gru_2 (GRU)           (None, 13, 235)           332760    \n",
      "                                                                 \n",
      " layer_gru_3 (GRU)           (None, 13, 235)           332760    \n",
      "                                                                 \n",
      " layer_gru_4 (GRU)           (None, 235)               332760    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 708       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,237,673\n",
      "Trainable params: 1,237,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 17s 100ms/step - loss: 1.0219 - accuracy: 0.4789 - val_loss: 0.6541 - val_accuracy: 0.8780\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 8s 74ms/step - loss: 0.9074 - accuracy: 0.5420 - val_loss: 0.8260 - val_accuracy: 0.6627\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 8s 71ms/step - loss: 0.8953 - accuracy: 0.5380 - val_loss: 0.7488 - val_accuracy: 0.6687\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 8s 69ms/step - loss: 0.8907 - accuracy: 0.5414 - val_loss: 0.7261 - val_accuracy: 0.6852\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 8s 71ms/step - loss: 0.8854 - accuracy: 0.5294 - val_loss: 0.8636 - val_accuracy: 0.6145\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 8s 77ms/step - loss: 0.8838 - accuracy: 0.5323 - val_loss: 0.7323 - val_accuracy: 0.6898\n",
      "Accuracy: 68.98%\n",
      "Loss: 0.73\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13, 100)           1100      \n",
      "                                                                 \n",
      " layer_gru_1 (GRU)           (None, 13, 90)            51840     \n",
      "                                                                 \n",
      " layer_gru_2 (GRU)           (None, 90)                49140     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 273       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,353\n",
      "Trainable params: 102,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 6s 27ms/step - loss: 1.0764 - accuracy: 0.4289 - val_loss: 0.8004 - val_accuracy: 0.9006\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 2s 16ms/step - loss: 0.9272 - accuracy: 0.5317 - val_loss: 0.7154 - val_accuracy: 0.8464\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.8945 - accuracy: 0.5460 - val_loss: 0.7757 - val_accuracy: 0.6566\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.8922 - accuracy: 0.5331 - val_loss: 0.7779 - val_accuracy: 0.6566\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.8887 - accuracy: 0.5303 - val_loss: 0.6872 - val_accuracy: 0.7033\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.8876 - accuracy: 0.5291 - val_loss: 0.7462 - val_accuracy: 0.6566\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.8859 - accuracy: 0.5300 - val_loss: 0.7650 - val_accuracy: 0.6551\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.8859 - accuracy: 0.5177 - val_loss: 0.7352 - val_accuracy: 0.6551\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.8853 - accuracy: 0.5314 - val_loss: 0.7402 - val_accuracy: 0.6551\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 2s 16ms/step - loss: 0.8842 - accuracy: 0.5211 - val_loss: 0.7600 - val_accuracy: 0.6551\n",
      "Accuracy: 65.51%\n",
      "Loss: 0.76\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13, 100)           1100      \n",
      "                                                                 \n",
      " layer_gru_1 (GRU)           (None, 94)                55272     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 285       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,657\n",
      "Trainable params: 56,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 3s 12ms/step - loss: 1.0098 - accuracy: 0.4840 - val_loss: 0.6489 - val_accuracy: 0.6988\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8879 - accuracy: 0.5409 - val_loss: 0.7867 - val_accuracy: 0.5030\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8489 - accuracy: 0.5689 - val_loss: 0.7512 - val_accuracy: 0.6069\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8176 - accuracy: 0.5806 - val_loss: 0.7190 - val_accuracy: 0.7711\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7956 - accuracy: 0.5951 - val_loss: 0.7114 - val_accuracy: 0.7681\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7771 - accuracy: 0.6083 - val_loss: 0.7273 - val_accuracy: 0.7620\n",
      "Accuracy: 76.20%\n",
      "Loss: 0.73\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13, 100)           1100      \n",
      "                                                                 \n",
      " layer_gru_1 (GRU)           (None, 13, 249)           262197    \n",
      "                                                                 \n",
      " layer_gru_2 (GRU)           (None, 13, 249)           373500    \n",
      "                                                                 \n",
      " layer_gru_3 (GRU)           (None, 13, 249)           373500    \n",
      "                                                                 \n",
      " layer_gru_4 (GRU)           (None, 13, 249)           373500    \n",
      "                                                                 \n",
      " layer_gru_5 (GRU)           (None, 249)               373500    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 750       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,758,047\n",
      "Trainable params: 1,758,047\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "gp_result = gp_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gp_best_params = {param.name: value for param, value in zip(gp_result.space, gp_result.x)}\n",
    "print(\"Best Hyperparameters:\", gp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_gru_model(gp_best_params['num_gru_layers'], \n",
    "                          gp_best_params['num_gru_units'], \n",
    "                          gp_best_params['learning_rate'], \n",
    "                          gp_best_params['adam_decay']\n",
    "                          )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                    y_train_encoded,\n",
    "                    validation_data=(x_val_encoded, y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train single-GRU\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val single-GRU\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test single-GRU\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_result = gbrt_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
    "print(\"Best Hyperparameters:\", gbrt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_gru_model(gbrt_best_params['num_gru_layers'], \n",
    "                          gbrt_best_params['num_gru_units'], \n",
    "                          gbrt_best_params['learning_rate'], \n",
    "                          gbrt_best_params['adam_decay']\n",
    "                          )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                    y_train_encoded,\n",
    "                    validation_data=(x_val_encoded, y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train single-GRU\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val single-GRU\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test single-GRU\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gru_layersA = Integer(low=1, high=5, name='num_gru_layersA')\n",
    "num_gru_layersB = Integer(low=1, high=5, name='num_gru_layersB')\n",
    "num_gru_unitsA = Integer(low=32, high=256, name='num_gru_unitsA')\n",
    "num_gru_unitsB = Integer(low=32, high=256, name='num_gru_unitsB')\n",
    "\n",
    "search_space = [\n",
    "            num_gru_layersA,\n",
    "            num_gru_layersB,\n",
    "            num_gru_unitsA,\n",
    "            num_gru_unitsB,\n",
    "            learning_rate,\n",
    "            adam_decay\n",
    "            ]\n",
    "\n",
    "# Specify one or more initial points for the search of optimal parameter\n",
    "default_params = [1, \n",
    "                  1, \n",
    "                  32,\n",
    "                  32, \n",
    "                  1e-3,\n",
    "                  1e-3 \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_multi_channel_gru_model(num_gru_layersA, num_gru_layersB, num_gru_unitsA, num_gru_unitsB, learning_rate, adam_decay):\n",
    "    # Vocabulary-based embedding layer\n",
    "    inputsA = Input(shape=(max_seq_length,), name=\"input regular embeddings\")\n",
    "    # Word2Vec embedding layer\n",
    "    inputsB = Input(shape=(max_seq_length,), name=\"input word2vec embeddings\")\n",
    "    \n",
    "    # Define an embedding layer for each input\n",
    "    embeddingsA = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, name=\"embeddingsA\")(inputsA)\n",
    "    embeddingsB = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, weights=[w2v_embedding_vectors], trainable=False, name=\"embeddingsB\")(inputsB)\n",
    "    \n",
    "    # Pass both embeddings through their own GRU layers\n",
    "    gru_layersA = embeddingsA\n",
    "    for i in range(num_gru_layersA):\n",
    "        nameA = 'layer_gruA_{0}'.format(i+1)\n",
    "        if i < num_gru_layers-1:\n",
    "            gru_layersA = GRU(num_gru_unitsA, return_sequences=True, name=nameA)(gru_layersA)\n",
    "        else:\n",
    "            gru_layersA = GRU(num_gru_unitsA, return_sequences=False, name=nameA)(gru_layersA)\n",
    "        \n",
    "    gru_layersB = embeddingsB\n",
    "    for i in range(num_gru_layersB):\n",
    "        nameA = 'layer_gruB_{0}'.format(i+1)\n",
    "        if i < num_gru_layers-1:\n",
    "            gru_layersA = GRU(num_gru_unitsB, return_sequences=True, name=nameA)(gru_layersB)\n",
    "        else:\n",
    "            gru_layersA = GRU(num_gru_unitsB, return_sequences=False, name=nameA)(gru_layersB)\n",
    "        \n",
    "\n",
    "    # Concatenate the two inputs\n",
    "    merged = concatenate([gru_layersA, gru_layersB])\n",
    "\n",
    "    # Dense layer for the merged inputs & output Layer\n",
    "    outputs = Dense(num_output_classes, activation='softmax', name=\"output\")(merged)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[inputsA, inputsB], outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    adam = Adam(learning_rate=learning_rate, decay=adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=search_space)\n",
    "def multi_objective_function(num_gru_layersA, num_gru_layersB, num_gru_unitsA, num_gru_unitsB, learning_rate, adam_decay, batch_size):\n",
    "\n",
    "    model = define_multi_channel_gru_model(num_gru_layersA=num_gru_layersA,\n",
    "                                            num_gru_layersB=num_gru_layersB,\n",
    "                                            num_gru_unitsA=num_gru_unitsA,\n",
    "                                            num_gru_unitsB=num_gru_unitsB,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            adam_decay=adam_decay\n",
    "                                            )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Loss: {loss:.2}\\n\")\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_result = gp_minimize(func=multi_objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gp_best_params = {param.name: value for param, value in zip(gp_result.space, gp_result.x)}\n",
    "print(\"Best Hyperparameters:\", gp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_gru_model(gp_best_params['num_gru_layersA'],\n",
    "                                        gp_best_params['num_gru_layersB'],\n",
    "                                        gp_best_params['num_gru_unitsA'], \n",
    "                                        gp_best_params['num_gru_unitsB'],\n",
    "                                        gp_best_params['learning_rate'], \n",
    "                                        gp_best_params['adam_decay']\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-GRU\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-GRU\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-GRU\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_result = gbrt_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
    "print(\"Best Hyperparameters:\", gbrt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_gru_model(gbrt_best_params['num_gru_layersA'],\n",
    "                                        gbrt_best_params['num_gru_layersB'],\n",
    "                                        gbrt_best_params['num_gru_unitsA'], \n",
    "                                        gbrt_best_params['num_gru_unitsB'],\n",
    "                                        gbrt_best_params['learning_rate'], \n",
    "                                        gbrt_best_params['adam_decay'],\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=20, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-GRU\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-GRU\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-GRU\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
