{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import scipy.sparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import pickle\n",
    "\n",
    "import gensim.downloader as api\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from joblib import dump\n",
    "\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/s3985113/Thesis_Jupyter_Final/src/\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "script_dir = os.path.dirname(os.path.abspath('processor.ipynb'))\n",
    "data_path = os.path.join(script_dir, 'Thesis_Jupyter_Final/src/')\n",
    "os.getcwd()\n",
    "print(data_path)\n",
    "\n",
    "input_folder_path = os.path.join(data_path, 'input')\n",
    "processed_folder_path = os.path.join(data_path, 'input/processed/normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants and other variables\n",
    "NUM_of_CLASSES = 3\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "glove_model = \"glove-twitter-100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    bad superficial speaks fast continually stop k...\n",
      "1                      let grade purchase disappointed\n",
      "2    horrible test sense element described generall...\n",
      "3    least favorite ere far style plot setting deta...\n",
      "4    guess level look easier broader last crowdsour...\n",
      "Name: x, dtype: object\n",
      "\n",
      "Vocab size:  11905\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a csv file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the csv file.\n",
    "\n",
    "    Returns:\n",
    "        x (DataFrame): The reviews.\n",
    "        y (DataFrame): The labels.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    x = df['x']\n",
    "    y = df['y']\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def load_vocab(file_path):\n",
    "    \"\"\"\n",
    "    Load vocabulary from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the pickle file.\n",
    "\n",
    "    Returns:\n",
    "        vocab (list): The loaded vocabulary list.\n",
    "        vocab_size (int): The size of the loaded vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    return vocab, vocab_size\n",
    "    \n",
    "\n",
    "x_train, y_train = load_data(os.path.join(processed_folder_path, \"train.csv\"))\n",
    "x_val, y_val = load_data(os.path.join(processed_folder_path, \"val.csv\"))\n",
    "x_test, y_test = load_data(os.path.join(processed_folder_path, \"test.csv\"))\n",
    "print(x_train[:5])\n",
    "print()\n",
    "\n",
    "vocab_data_filename = \"vocab.pkl\"\n",
    "vocab, vocab_size = load_vocab(os.path.join(processed_folder_path, vocab_data_filename))\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code provided below draws inspiration from the article on Machine Learning Mastery titled \"Develop a Word Embedding Model for Predicting Movie Review Sentiment\" available at the following link: [Machine Learning Mastery - Word Embeddings](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 449\n"
     ]
    }
   ],
   "source": [
    "def find_max_seq_len(data):\n",
    "    \"\"\"\n",
    "    Find the maximum sequence length in the list of reviews.\n",
    "\n",
    "    Args:\n",
    "        data (list): The list of reviews.\n",
    "\n",
    "    Returns:\n",
    "        max_seq_length (int): The maximum sequence length found.\n",
    "    \"\"\"\n",
    "\n",
    "    max_seq_length = max([len(line.split()) for line in data])\n",
    "    print(f'Maximum review length: {max_seq_length}')\n",
    "\n",
    "    return max_seq_length\n",
    "\n",
    "max_seq_length = find_max_seq_len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Data Shape (doc, max_len):\n",
      "* train: (41000, 449)\n",
      "* validation: (11529, 449)\n",
      "* test: (11899, 449)\n",
      "\n",
      "x_train_tfidf:\n",
      "[[  96  549  929 ...    0    0    0]\n",
      " [ 453  240 1125 ...    0    0    0]\n",
      " [1260   67  312 ...    0    0    0]\n",
      " ...\n",
      " [   6  380   88 ...    0    0    0]\n",
      " [ 125 2234  195 ...    0    0    0]\n",
      " [ 761  583  442 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "def fit_tokenizer(data):\n",
    "    \"\"\"\n",
    "    Fit a tokenizer on the review data.\n",
    "\n",
    "    Args:\n",
    "        data (array): Input review data for fitting the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer (Tokenizer): Fitted tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    # Remove default filters, including punctuation\n",
    "    tokenizer.filters = \"\"  \n",
    "    # Disable lowercase conversion\n",
    "    tokenizer.lower = False  \n",
    "    tokenizer.fit_on_texts(data) \n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def encode_text(lines, tokenizer, max_length, filename):\n",
    "    \"\"\"\n",
    "    Encode text data, pad the sequences and save.\n",
    "\n",
    "    Args:\n",
    "        lines (array): Input text review data to be encoded.\n",
    "        tokenizer (Tokenizer): Fitted tokenizer object.\n",
    "        max_length (int): Maximum sequence length to be used for padding.\n",
    "        filename (str): Name of the file to save the encoded sequences.\n",
    "\n",
    "    Returns:\n",
    "        padded (numpy.ndarray): Padded and encoded sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_seq = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Save \n",
    "    with open(os.path.join(processed_folder_path, filename + '.pkl'), 'wb') as file:\n",
    "        pickle.dump(padded, file)\n",
    "\n",
    "    return padded\n",
    "    \n",
    "    \n",
    "tokenizer = fit_tokenizer(x_train)\n",
    "\n",
    "# Encode Data\n",
    "x_train_encoded = encode_text(x_train, tokenizer, max_seq_length, \"x_train_encoded\")\n",
    "x_val_encoded = encode_text(x_val, tokenizer, max_seq_length, \"x_val_encoded\")\n",
    "x_test_encoded = encode_text(x_test, tokenizer, max_seq_length, \"x_test_encoded\")\n",
    "\n",
    "print(\"\\nEncoded Data Shape (doc, max_len):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_encoded.shape, x_val_encoded.shape, x_test_encoded.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y-Encoded Data Shape:\n",
      "* train: (41000, 3)\n",
      "* validation: (11529, 3)\n",
      "* test: (11899, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(y):\n",
    "    \"\"\"\n",
    "    Convert integer y-labels to one-hot encoding.\n",
    "\n",
    "    Args:\n",
    "        y (array): Labels (int).\n",
    "\n",
    "    Returns:\n",
    "        y_encoded (array): One-hot encoded labels.\n",
    "    \"\"\"\n",
    "\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded\n",
    "\n",
    "\n",
    "# Convert sentiment labels to one-hot encoding\n",
    "y_train_encoded = one_hot_encode(y_train)\n",
    "y_val_encoded = one_hot_encode(y_val)\n",
    "y_test_encoded = one_hot_encode(y_test)\n",
    "\n",
    "# Save y-encoded sets\n",
    "with open(os.path.join(processed_folder_path, \"y_train_encoded.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(y_train_encoded, f)\n",
    "with open(os.path.join(processed_folder_path, \"y_val_encoded.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(y_val_encoded, f)\n",
    "with open(os.path.join(processed_folder_path, \"y_test_encoded.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(y_test_encoded, f)\n",
    "\n",
    "   \n",
    "print(\"\\ny-Encoded Data Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_vocab_size:  11395\n",
      "Words in tokenizer but not in vocab:  511\n",
      "Words:  {'cleanly', 'stride', 'knowledg', 'mullins', 'unforgettable', 'worship', 'fundmentals', 'feiler', 'nearby', 'quicky', 'grear', 'fem', 'finely', 'speculative', 'opics', 'asignments', 'esay', 'complementing', 'schopenhauer', 'negotiated', 'methylation', 'domestication', 'garcia', 'differente', 'moocer', 'comlex', 'gr8', 'expend', 'webdevelopment', 'nicest', 'contend', 'imformative', 'geometrically', 'fi', 'denis', 'jerry', 'glove', 'devotes', 'matured', 'sang', 'workaround', 'lung', 'velichkovski', 'psychoteraphy', 'wealthy', 'persevered', 'jorney', 'everythis', 'einstellung', 'aizuri', 'ag', 'swiss', 'devising', 'recruiter', 'developping', 'florida', 'designated', 'rbm', 'marketeers', 'burdensome', 'potent', 'soviet', 'chucky', 'servance', 'improvising', 'swm', 'leart', 'excelentes', 'adversely', 'fearing', 'pediatric', 'frederickson', 'keying', 'dicussion', 'lookup', 'veru', 'epitome', 'whirlwind', 'sor', 'simulated', 'bioinformatic', 'teachergreat', 'objetive', 'csev', 'differing', 'leanred', 'awkwardly', 'meditators', 'repair', 'longterm', 'phenomenally', 'troop', 'mirza', 'fot', 'mee', 'mary', 'rocking', 'colleages', 'wheat', 'sloppier', 'slew', 'tres', 'jessy', 'sigmoid', 'notably', 'denise', 'stuctured', 'crux', 'lovin', 'ge', 'santiago', 'primed', 'aa', 'phl', 'cakewalk', 'parabéns', 'ect', 'evil', 'vectorisation', 'classe', 'footstep', 'misinterpreted', 'casserole', 'diversification', 'cada', 'idle', 'subjet', 'fourier', 'underpinning', 'drafting', 'rhetoric', 'cryptocurrency', 'backround', 'completa', 'berry', 'smiled', 'thee', 'irreverent', 'amnh', 'theorethical', 'lakshmanan', 'aplicable', 'yandex', 'analize', 'enjoied', 'nunes', 'quantitatively', 'lever', 'confession', 'v6', 'interestig', 'usefuls', 'ron', 'practial', 'exploited', 'privy', 'walkthrough', 'awarness', 'melodic', 'criticized', 'uncle', 'mount', 'aloha', 'usful', 'necesary', 'eliciting', 'nanotechnology', 'cfe', 'universiteit', 'erd', 'straigth', 'tfs', 'entropy', 'storyline', 'nihongo', 'staging', 'cellular', 'hoe', 'augmenting', 'begginner', 'chicago', 'retrival', 'codebase', 'ef', 'mips', 'invigorating', 'retroviral', 'intrested', 'videogame', 'shira', 'unicef', 'introdution', 'gusto', 'tolerance', 'assertive', 'powerfull', 'thw', 'clicking', 'chaiking', 'uesful', 'toefl', 'shes', 'experimented', 'enlighted', 'cameo', 'clep', 'dusty', 'weave', 'lenient', 'sanitary', 'cfa', 'therapeutic', 'salvatore', 'warmth', 'microeconomic', 'valueable', 'leraned', 'iwatch', 'expains', 'subsetting', 'definintely', 'recursive', 'thanksi', 'elm', 'didatics', 'adrew', 'johnson', 'cdns', 'subtittles', 'zhang', 'ibt', 'laudable', 'iterables', 'engrossing', 'civic', 'hallmark', 'librarian', 'nail', 'demonstrably', 'uniquely', 'hydraulic', 'biz', 'planner', 'eggar', 'pov', 'chevere', 'ergo', 'fundation', 'rly', 'bulky', 'reaaly', 'tactical', 'intereting', 'beating', 'undermines', 'sewer', 'coursethanks', 'carrot', 'goood', 'urcan', 'personnal', 'ship', 'cautious', 'collapsed', 'playwright', 'gusta', 'liability', 'grappling', 'wortmann', 'stepik', 'interessant', 'avery', 'rearrangement', 'kovach', 'scribona', 'caculus', 'commence', 'civilisation', 'compartir', 'okley', 'bombarding', 'programe', 'est', 'perfecto', 'instruc', 'racket', 'interersting', 'foreach', 'edd', 'rephrased', 'improv', 'usc', 'confidant', 'grok', 'metallurgical', 'difficultly', 'anglo', 'sw', 'comprehensibly', 'mich', 'highway', '@coursera', 'aplication', 'sectional', 'poorer', 'saudi', 'bom', 'striving', 'preached', 'numericals', 'shuttle', 'antiquity', 'rut', 'battling', 'implantology', 'guido', 'accessibly', 'autumn', 'rpi', 'greta', 'mindshift', 'violinist', 'flour', 'reaping', 'enought', 'questioned', 'sociopolitical', 'rag', 'bravery', 'esophageal', 'powered', 'mobil', 'artistically', 'dumping', 'standen', 'miguel', 'summation', 'tick', 'maneja', 'shakespeare', 'reshaped', 'mcdonalds', 'converge', 'flowering', 'hz', 'compendium', 'lull', 'rossum', 'fantasic', 'informati', 'kulikov', 'lifting', 'turtle', 'py4inf', 'daler', 'maestro', 'uart', 'rockstar', 'marketting', 'consulted', 'improvments', 'compiling', 'coursse', 'lambert', 'successive', 'tejas', 'jail', 'andew', 'univers', 'leason', 'lou', 'intensely', 'genious', 'recommeded', 'hao', 'vitally', 'skillet', 'greath', 'va', 'trauma', 'catastrophe', 'insignificant', 'accross', 'breathtaking', 'dope', 'meaningfull', 'thankfull', 'regenerative', 'collage', 'expat', 'examiner', 'thinkable', 'curio', 'honk', 'competative', 'boggling', 'impressivethanks', 'haydn', 'cleanliness', 'gbv', 'kunqu', 'instructur', 'zac', 'tecnologies', 'breed', 'basical', 'televison', 'couser', 'diffculty', 'subtopics', 'exper', 'brillantly', 'quickcheck', 'tnx', 'playfulness', 'sapiro', 'classwork', 'imported', 'logged', 'excelents', 'owesome', 'bow', 'heath', 'outdoor', 'didactive', 'revolutionizing', 'sml', 'eliminates', 'materiel', 'blooper', 'pymks', 'strenghten', 'rotation', 'relaxation', 'academician', 'storage', 'encyclopaedia', 'dissection', 'manhole', 'introduccion', 'cour', 'insist', 'carreer', 'anastasia', 'bind', 'schooler', 'raw_input', 'pooja', 'evey', 'estrategies', 'nodered', 'i̇', 'ct', 'summarization', 'ledeczi', 'forgets', 'snack', 'appraoch', 'positivism', 'pantry', 'hahaha', 'june', 'corp', 'constituent', 'pray', 'othe', 'mare', 'unquestionable', 'luv', 'manolo', 'cerebral', 'espectacular', 'coursevery', 'crush', 'excelence', 'voltage', 'intact', 'microbime', 'fictional', 'tecnical', 'pg', 'dime', 'generator', 'pratice', 'inp', 'rbms', 'dy', 'warp', 'liberating', 'nonprofit', 'progr', 'condensing', 'randles', 'reassured', 'prevailing', 'extrapolate', 'joggesh', 'gc', 'colaborative', 'befuddled', 'importent', 'width', 'guillermo', 'kickoff', 'logging', 'worthington', 'practicali', 'adviser', 'cristina', 'tellement', 'instructivo'}\n"
     ]
    }
   ],
   "source": [
    "# Total vocabulary size plus 0 for unknown words\n",
    "embedding_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"embedding_vocab_size: \", embedding_vocab_size)\n",
    "\n",
    "# Check if there are any words identified via the tokenizer that are not in vocab\n",
    "tokenizer_vocab = set(tokenizer.word_index.keys())\n",
    "vocab_set = set(vocab)\n",
    "tokenizer_only_words = vocab_set.difference(tokenizer_vocab)\n",
    "print(\"Words in tokenizer but not in vocab: \", len(tokenizer_only_words))\n",
    "print(\"Words: \", tokenizer_only_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under-sampling could have potentially removed instances of certain words from the training set, reducing the overall vocabulary that the tokenizer picks up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading w2v model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831/11394 (7.29%) are not defined in the pretrained W2V model and will receive vectors with all 0.\n",
      "W2V Embedding Matrix shape: (11395, 100)\n",
      "Embedding Matrix:\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.19679999e+00 -2.84579992e-02 -2.96110004e-01  4.94709998e-01\n",
      "   1.56049997e-01  5.34389973e-01  8.65369976e-01  3.00430000e-01\n",
      "   1.99340001e-01  5.22000015e-01  4.23209995e-01 -1.15330005e+00\n",
      "  -3.63720012e+00 -2.83510000e-01 -2.40470007e-01  2.08829999e-01\n",
      "  -4.05519992e-01 -1.44109994e-01 -1.83780000e-01  3.11710000e-01\n",
      "   9.60950017e-01 -7.00730026e-01 -5.01860023e-01  7.91310012e-01\n",
      "  -5.86820006e-01  2.17259992e-02 -3.61980014e-02  1.12750001e-01\n",
      "   3.51950005e-02  4.99370009e-01  5.23840010e-01 -1.72670007e-01\n",
      "  -1.06939995e+00  2.59160008e-02  2.33250007e-01  9.09020007e-02\n",
      "  -2.21900001e-01 -1.93959996e-02  8.31250012e-01 -3.68990004e-01\n",
      "  -2.87330002e-01 -1.69740006e-01  2.73400009e-01  8.86730030e-02\n",
      "  -4.20480013e-01 -2.94660002e-01 -2.81629991e-02  1.20889999e-01\n",
      "   9.76670027e-01 -4.43789989e-01  6.89530015e-01  4.11089987e-01\n",
      "  -7.03819990e-01 -2.61000007e-01 -1.27020001e-01  3.16909999e-01\n",
      "   1.49189994e-01  6.46260008e-03 -2.23959997e-01 -4.23020005e-01\n",
      "   4.88310009e-02 -3.17259997e-01  8.08889985e-01  4.29080009e-01\n",
      "   3.06769997e-01 -2.57879999e-02  1.55609995e-01 -1.12620004e-01\n",
      "   2.57380009e-01  1.10459998e-01 -6.19390011e-01  4.24580008e-01\n",
      "   2.17809994e-02 -8.97819996e-01 -4.58420008e-01 -1.88500002e-01\n",
      "   1.76369995e-01  4.01879996e-01 -1.15400001e-01  1.06129996e-01\n",
      "   1.00279999e+00  5.49369991e-01  4.18700010e-01 -7.07220018e-01\n",
      "   1.93140000e-01  3.19350004e-01 -1.11210001e+00  1.76390007e-01\n",
      "  -1.08770002e-02 -1.23719998e-01 -4.85289991e-01  4.60449994e-01\n",
      "  -4.28079993e-01  1.29600003e-01 -4.33919996e-01  4.10299987e-01\n",
      "  -1.29170001e-01  1.60679996e-01  1.69430003e-01 -3.00989985e-01]\n",
      " [ 6.69380009e-01 -1.40200004e-01  8.05130005e-02  1.00819997e-01\n",
      "  -5.61330020e-01  6.76289976e-01  4.19569999e-01 -4.74979997e-01\n",
      "  -1.72959998e-01  5.38929999e-01 -4.82809991e-02 -2.64990002e-01\n",
      "  -3.28889990e+00 -1.00970000e-01  1.01140002e-02  2.40739994e-03\n",
      "  -8.42399970e-02  1.11139998e-01  2.00770006e-01 -2.11610004e-01\n",
      "   8.89460027e-01  1.01559997e-01 -5.61020002e-02  3.60019989e-02\n",
      "  -2.67899990e-01  1.13409996e+00  1.64959997e-01  4.17340010e-01\n",
      "  -7.80079979e-03 -1.48189999e-02  8.08549970e-02  5.13149977e-01\n",
      "  -2.50620008e-01 -2.62529999e-02 -2.67120004e-01  4.43620011e-02\n",
      "   6.33850023e-02 -2.45629996e-01  6.09339997e-02 -1.92460008e-02\n",
      "   4.68169987e-01 -7.65900016e-02  2.62930006e-01 -3.28169987e-02\n",
      "   1.66899994e-01 -3.22369993e-01  3.58610004e-01 -1.52459994e-01\n",
      "  -5.51999986e-01  3.99549991e-01 -3.35480012e-02  3.25239986e-01\n",
      "  -2.14969993e-01 -2.74419993e-01 -5.93379997e-02  4.25830007e-01\n",
      "   2.92210013e-01 -1.16319999e-01  1.60569996e-01 -1.74710006e-01\n",
      "   4.24970001e-01  3.24860007e-01  5.29900007e-02  3.37379992e-01\n",
      "   7.67679989e-01  3.52299988e-01  2.55710006e-01 -9.79759991e-02\n",
      "  -4.71329987e-01  2.44760007e-01 -4.86330003e-01 -3.60199988e-01\n",
      "   4.09330018e-02 -2.66660005e-01  6.98350012e-01 -2.94809997e-01\n",
      "   1.76929999e-02 -1.43879995e-01 -1.34930000e-01 -1.46620005e-01\n",
      "   9.72479999e-01 -1.67030007e-01  1.57430004e-02  1.79839998e-01\n",
      "   6.96449995e-01  9.85240005e-03  3.17550004e-01 -1.80940002e-01\n",
      "  -4.52309996e-01  3.23719997e-03 -2.94710010e-01  2.75510013e-01\n",
      "   1.20940000e-01  3.07229999e-02 -3.80470008e-01 -1.14469998e-01\n",
      "   1.64969996e-01  1.43600002e-01  1.63680002e-01 -2.33789995e-01]\n",
      " [-5.97889982e-02  7.60350004e-02 -7.22080003e-03 -4.47739996e-02\n",
      "  -6.04589999e-01 -4.17679995e-01  4.16500010e-02 -1.05509996e-01\n",
      "  -2.70859987e-01  3.38800013e-01 -1.73070002e-02 -5.21660000e-02\n",
      "  -4.04029989e+00 -9.68530029e-03 -8.91149998e-01  1.51300002e-02\n",
      "   1.55310005e-01  5.85009992e-01 -5.53390011e-02 -4.08609986e-01\n",
      "  -3.63810003e-01 -3.94840002e-01 -2.68539995e-01 -4.04799990e-02\n",
      "  -8.82300019e-01  4.71139997e-01 -1.63650006e-01 -2.55190015e-01\n",
      "   1.00829995e+00 -9.95870009e-02  8.67099985e-02  4.21920009e-02\n",
      "  -9.35620010e-01 -7.13129997e-01  3.69410008e-01  1.94350004e-01\n",
      "  -3.30720007e-01 -1.17140003e-01  2.58230008e-02  9.35349986e-02\n",
      "  -1.68750003e-01 -7.89030015e-01  5.10039985e-01 -6.16219997e-01\n",
      "  -2.58819997e-01  3.96759987e-01 -4.45369989e-01  9.22999978e-01\n",
      "  -1.10480003e-01 -2.95489997e-01 -1.85749996e-02 -1.30129993e-01\n",
      "  -4.40090001e-01 -9.97219980e-01  4.50089991e-01 -1.65140003e-01\n",
      "   1.79409996e-01 -6.56369984e-01  8.34949970e-01 -4.66789991e-01\n",
      "   4.25199986e-01 -4.12499994e-01  2.51630008e-01 -4.72359985e-01\n",
      "  -1.95230007e-01  4.71879989e-01  7.39310011e-02  4.07579988e-01\n",
      "   7.97029972e-01  3.38519990e-01 -6.94869995e-01  1.00199997e-01\n",
      "   6.39789999e-01 -1.17119998e-01  4.42699999e-01 -5.12870014e-01\n",
      "   3.26810002e-01 -7.97270000e-01 -7.33439982e-01 -4.55680013e-01\n",
      "   1.09259999e+00  4.66250002e-01  6.59759998e-01  7.38110006e-01\n",
      "   6.86839998e-01 -2.71459997e-01  2.11050007e-02  3.64720002e-02\n",
      "   4.02689993e-01 -3.65259983e-02  3.47400010e-01  3.45860004e-01\n",
      "  -2.27119997e-01 -1.89439997e-01 -4.48000014e-01  1.61599994e-01\n",
      "   4.96140003e-01  3.53240013e-01  2.66059995e-01 -5.11920005e-02]\n",
      " [ 1.19419999e-01 -1.81559995e-01 -4.10909988e-02  4.75319996e-02\n",
      "  -1.43179998e-01  6.40250027e-01 -6.87920004e-02 -6.20739982e-02\n",
      "  -1.08960003e-01  2.60690004e-01  2.31010005e-01 -7.97689974e-01\n",
      "  -3.67540002e+00  1.26200005e-01  4.67900008e-01  1.87649995e-01\n",
      "   8.57810020e-01 -4.02049989e-01 -5.85789979e-01 -3.74240011e-01\n",
      "   1.28519997e-01  3.48500013e-01 -3.44029993e-01  1.10339999e+00\n",
      "  -4.95539993e-01 -2.83749998e-01 -2.19010003e-03  3.91770005e-01\n",
      "  -1.31070003e-01 -2.47449994e-01 -3.64100009e-01 -1.28500000e-01\n",
      "  -2.16140002e-01  1.29499997e-03  5.72830021e-01  1.86749995e-01\n",
      "   2.68709995e-02  4.64430004e-01  7.09550008e-02 -1.04230002e-01\n",
      "  -8.38039994e-01  4.85619992e-01  5.21409996e-02  1.31050004e-02\n",
      "   2.41029993e-01 -4.69590016e-02  3.21000010e-01  3.70660007e-01\n",
      "  -7.92120025e-02  2.62930006e-01 -2.00049996e-01  4.69179988e-01\n",
      "  -1.54049993e-01 -1.97129995e-01  1.70090005e-01  7.40279973e-01\n",
      "  -5.90839982e-01  2.26510003e-01 -4.94769990e-01  5.73199987e-02\n",
      "  -3.35350007e-01  7.55710006e-01  3.31539989e-01  1.91009998e-01\n",
      "   1.71130002e-01 -3.55619997e-01  2.99199998e-01  2.30140001e-01\n",
      "  -6.44999981e-01 -1.89019993e-01 -5.86650014e-01  4.85190004e-01\n",
      "  -3.13739985e-01 -6.48810029e-01  5.93750000e-01 -2.41589993e-01\n",
      "  -2.66750008e-02 -3.23570013e-01 -4.34199989e-01  6.11749999e-02\n",
      "   6.20620012e-01  9.04600024e-01 -4.86160010e-01 -3.07559997e-01\n",
      "   4.58550006e-01 -6.46139979e-01 -7.24539995e-01  2.74870005e-02\n",
      "  -8.47859979e-02 -2.15969995e-01 -4.08529997e-01 -5.05219996e-01\n",
      "   2.99900007e-02 -4.94490005e-02 -4.09509987e-01 -2.97670007e-01\n",
      "   4.12460007e-02  3.48679990e-01 -1.56540006e-01 -9.64550018e-01]]\n"
     ]
    }
   ],
   "source": [
    "def load_embedding():\n",
    "    \"\"\"\n",
    "    Load the pre-trained GloVe word embeddings and convert them to a dictionary.\n",
    "\n",
    "    Returns:\n",
    "        pretrained_embeddings (dict): A dictionary mapping words to their corresponding embedding vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    glove_model_filename = str(glove_model) + \".txt\"\n",
    "    glove_file_path = os.path.join(processed_folder_path, glove_model_filename)\n",
    "    if not os.path.exists(glove_file_path): \n",
    "        # Check if the pre-trained Word2Vec model is already downloaded. If not, download it.\n",
    "        print(\"GloVe model doesn't exist...\")\n",
    "        model = api.load(glove_model)\n",
    "        model.save_word2vec_format(glove_file_path, binary=False)\n",
    "        # 5186/12465 (41.60%) are not defined with twitter-glove\n",
    "        \n",
    "    # Load embedding into memory, skip first line\n",
    "    print(\"Loading w2v model...\")\n",
    "    file = open(glove_file_path, 'r', encoding='utf8')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "\n",
    "    # Create a map of words to vectors\n",
    "    pretrained_embeddings = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # Set key as string word, value as numpy array for vector\n",
    "        pretrained_embeddings[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "\n",
    "    return pretrained_embeddings\n",
    "\n",
    "\n",
    "def get_embedding_matrix(loaded_embedding, tokenizer, embedding_dim):\n",
    "    \"\"\"\n",
    "    Create an embedding matrix from the loaded/pretrained word embeddings.\n",
    "\n",
    "    Args:\n",
    "        loaded_embedding (dict): A dictionary containing the pre-trained word embeddings.\n",
    "        tokenizer (Tokenizer): The Tokenizer object used to tokenize the text data.\n",
    "        embedding_dim (int): The dimension of the word embeddings.\n",
    "\n",
    "    Returns:\n",
    "        embedding_matrix (ndarray): The embedding matrix for the Embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define weight matrix dimensions (vocab_size + 1 for unknown words) with all 0 \n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "\n",
    "    count_all_words = 0\n",
    "    count_na_words = 0\n",
    "    zero_vector_words = []\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # Map loaded vectors to terms in vocab\n",
    "        if word in loaded_embedding.keys():\n",
    "            embedding_matrix[i] = loaded_embedding.get(word)\n",
    "        else:\n",
    "            # Some terms such as emojis or neg-tagged words are not found in the loaded w2v model, hence they will have vectors with all 0\n",
    "            zero_vector_words.append(word)\n",
    "            count_na_words += 1\n",
    "        count_all_words += 1\n",
    "    print(f'{count_na_words}/{count_all_words} ({((count_na_words/count_all_words)*100):.2f}%) are not defined in the pretrained W2V model and will receive vectors with all 0.')\n",
    "    print(f\"W2V Embedding Matrix shape: {embedding_matrix.shape}\")\n",
    "    print(f\"Embedding Matrix:\\n{embedding_matrix[:5]}\")\n",
    "\n",
    "    # Save unrecognized words that are not present in the GloVe model\n",
    "    file_path = os.path.join(processed_folder_path, \"out_of_glove_words.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(zero_vector_words))\n",
    "\n",
    "    # Save embeddings\n",
    "    file_path = os.path.join(processed_folder_path, \"embedding_matrix.txt\")     # for visualization\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(' '.join(str(x) for x in row) for row in embedding_matrix))\n",
    "    np.savetxt(os.path.join(processed_folder_path, \"embedding_matrix.txt\"), embedding_matrix, fmt='%f')\n",
    "\n",
    "    with open(os.path.join(processed_folder_path, 'embedding_matrix.pkl'), 'wb') as file:   # for saving memory space\n",
    "        pickle.dump(embedding_matrix, file)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "pretrained_embeddings = load_embedding()\n",
    "embedding_vectors = get_embedding_matrix(pretrained_embeddings, tokenizer, EMBEDDING_DIM)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
