{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import pickle\n",
    "\n",
    "import gensim.downloader as api\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from joblib import dump\n",
    "\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/s3985113/Thesis_Jupyter_Final/src/\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname(os.path.abspath('processor.ipynb'))\n",
    "data_path = os.path.join(script_dir, 'Thesis_Jupyter_Final/src/')\n",
    "os.getcwd()\n",
    "print(data_path)\n",
    "\n",
    "input_folder_path = os.path.join(data_path, 'input')\n",
    "processed_folder_path = os.path.join(data_path, 'input/processed/normal')\n",
    "\n",
    "glove_model = \"glove-twitter-100\"\n",
    "#glove_model = \"glove-wiki-gigaword-100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_of_CLASSES = 3\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    bad superficial speaks fast continually stop k...\n",
      "1                      let grade purchase disappointed\n",
      "2    horrible test sense element described generall...\n",
      "3    least favorite ere far style plot setting deta...\n",
      "4    guess level look easier broader last crowdsour...\n",
      "Name: x, dtype: object\n",
      "\n",
      "Vocab size:  11905\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    x = df['x']\n",
    "    y = df['y']\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def load_vocab(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    return vocab, vocab_size\n",
    "    \n",
    "\n",
    "x_train, y_train = load_data(os.path.join(processed_folder_path, \"train.csv\"))\n",
    "x_val, y_val = load_data(os.path.join(processed_folder_path, \"val.csv\"))\n",
    "x_test, y_test = load_data(os.path.join(processed_folder_path, \"test.csv\"))\n",
    "print(x_train[:5])\n",
    "print()\n",
    "\n",
    "\n",
    "vocab_data_filename = \"vocab.pkl\"\n",
    "vocab, vocab_size = load_vocab(os.path.join(processed_folder_path, vocab_data_filename))\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 430\n"
     ]
    }
   ],
   "source": [
    "def find_max_seq_len(data):\n",
    "    # Find maximum sequence length\n",
    "    max_seq_length = max([len(line.split()) for line in data])\n",
    "    print(f'Maximum review length: {max_seq_length}')\n",
    "\n",
    "    return max_seq_length\n",
    "\n",
    "max_seq_length = find_max_seq_len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Data Shape (doc, vocab_size):\n",
      "* train: (41000, 430)\n",
      "* validation: (11529, 430)\n",
      "* test: (11899, 430)\n",
      "\n",
      "x_train_tfidf:\n",
      "[[   93   566   866 ...     0     0     0]\n",
      " [  464   263  1110 ...     0     0     0]\n",
      " [ 1356    74   288 ...     0     0     0]\n",
      " ...\n",
      " [  899    42  8753 ...     0     0     0]\n",
      " [   50    66    65 ...     0     0     0]\n",
      " [ 3674  1738 11382 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "def fit_tokenizer(data):\n",
    "    # Fit tokenizer (on training data)\n",
    "    tokenizer = Tokenizer()\n",
    "    # Remove default filters, including punctuation\n",
    "    tokenizer.filters = \"\"  \n",
    "    # Disable lowercase conversion\n",
    "    tokenizer.lower = False  \n",
    "    tokenizer.fit_on_texts(data) \n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def encode_text(lines, tokenizer, max_length, filename):\n",
    "    # Integer encode\n",
    "    encoded_seq = tokenizer.texts_to_sequences(lines)\n",
    "    # Pad the encoded sequences\n",
    "    padded = pad_sequences(encoded_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Save \n",
    "    with open(os.path.join(processed_folder_path, filename + '.pkl'), 'wb') as file:\n",
    "        pickle.dump(padded, file)\n",
    "\n",
    "    return padded\n",
    "    \n",
    "    \n",
    "tokenizer = fit_tokenizer(x_train)\n",
    "\n",
    "# Encode Data\n",
    "x_train_encoded = encode_text(x_train, tokenizer, max_seq_length, \"x_train_encoded\")\n",
    "x_val_encoded = encode_text(x_val, tokenizer, max_seq_length, \"x_val_encoded\")\n",
    "x_test_encoded = encode_text(x_test, tokenizer, max_seq_length, \"x_test_encoded\")\n",
    "\n",
    "print(\"\\nEncoded Data Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_encoded.shape, x_val_encoded.shape, x_test_encoded.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y-Encoded Data Shape:\n",
      "* train: (41000, 3)\n",
      "* validation: (11529, 3)\n",
      "* test: (11899, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: can't remember if this is used somewhere else, if not save data inside function\n",
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded\n",
    "\n",
    "# Convert sentiment labels to one-hot encoding\n",
    "y_train_encoded = one_hot_encode(y_train)\n",
    "y_val_encoded = one_hot_encode(y_val)\n",
    "y_test_encoded = one_hot_encode(y_test)\n",
    "\n",
    "# Save y-encoded sets\n",
    "np.save(os.path.join(processed_folder_path, \"y_train_encoded.npy\"), np.array(y_train_encoded))\n",
    "np.save(os.path.join(processed_folder_path, \"y_val_encoded.npy\"), np.array(y_val_encoded))\n",
    "np.save(os.path.join(processed_folder_path, \"y_test_encoded.npy\"), np.array(y_test_encoded))\n",
    "   \n",
    "print(\"\\ny-Encoded Data Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_vocab_size:  11383\n",
      "Words in tokenizer but not in vocab:  0\n"
     ]
    }
   ],
   "source": [
    "# Total vocabulary size plus 0 for unknown words\n",
    "embedding_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"embedding_vocab_size: \", embedding_vocab_size)\n",
    "\n",
    "# Check if there are any words identified via the tokenizer that are not in vocab\n",
    "tokenizer_vocab = set(tokenizer.word_index.keys())\n",
    "vocab_set = set(vocab)\n",
    "tokenizer_only_words = tokenizer_vocab.difference(vocab_set)\n",
    "print(\"Words in tokenizer but not in vocab: \", len(tokenizer_only_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading w2v model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825/11382 (7.25%) are not defined in the pretrained W2V model and will receive vectors with all 0.\n",
      "W2V Embedding Matrix shape: (11383, 100)\n",
      "Embedding Matrix:\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.19679999e+00 -2.84579992e-02 -2.96110004e-01  4.94709998e-01\n",
      "   1.56049997e-01  5.34389973e-01  8.65369976e-01  3.00430000e-01\n",
      "   1.99340001e-01  5.22000015e-01  4.23209995e-01 -1.15330005e+00\n",
      "  -3.63720012e+00 -2.83510000e-01 -2.40470007e-01  2.08829999e-01\n",
      "  -4.05519992e-01 -1.44109994e-01 -1.83780000e-01  3.11710000e-01\n",
      "   9.60950017e-01 -7.00730026e-01 -5.01860023e-01  7.91310012e-01\n",
      "  -5.86820006e-01  2.17259992e-02 -3.61980014e-02  1.12750001e-01\n",
      "   3.51950005e-02  4.99370009e-01  5.23840010e-01 -1.72670007e-01\n",
      "  -1.06939995e+00  2.59160008e-02  2.33250007e-01  9.09020007e-02\n",
      "  -2.21900001e-01 -1.93959996e-02  8.31250012e-01 -3.68990004e-01\n",
      "  -2.87330002e-01 -1.69740006e-01  2.73400009e-01  8.86730030e-02\n",
      "  -4.20480013e-01 -2.94660002e-01 -2.81629991e-02  1.20889999e-01\n",
      "   9.76670027e-01 -4.43789989e-01  6.89530015e-01  4.11089987e-01\n",
      "  -7.03819990e-01 -2.61000007e-01 -1.27020001e-01  3.16909999e-01\n",
      "   1.49189994e-01  6.46260008e-03 -2.23959997e-01 -4.23020005e-01\n",
      "   4.88310009e-02 -3.17259997e-01  8.08889985e-01  4.29080009e-01\n",
      "   3.06769997e-01 -2.57879999e-02  1.55609995e-01 -1.12620004e-01\n",
      "   2.57380009e-01  1.10459998e-01 -6.19390011e-01  4.24580008e-01\n",
      "   2.17809994e-02 -8.97819996e-01 -4.58420008e-01 -1.88500002e-01\n",
      "   1.76369995e-01  4.01879996e-01 -1.15400001e-01  1.06129996e-01\n",
      "   1.00279999e+00  5.49369991e-01  4.18700010e-01 -7.07220018e-01\n",
      "   1.93140000e-01  3.19350004e-01 -1.11210001e+00  1.76390007e-01\n",
      "  -1.08770002e-02 -1.23719998e-01 -4.85289991e-01  4.60449994e-01\n",
      "  -4.28079993e-01  1.29600003e-01 -4.33919996e-01  4.10299987e-01\n",
      "  -1.29170001e-01  1.60679996e-01  1.69430003e-01 -3.00989985e-01]\n",
      " [ 6.69380009e-01 -1.40200004e-01  8.05130005e-02  1.00819997e-01\n",
      "  -5.61330020e-01  6.76289976e-01  4.19569999e-01 -4.74979997e-01\n",
      "  -1.72959998e-01  5.38929999e-01 -4.82809991e-02 -2.64990002e-01\n",
      "  -3.28889990e+00 -1.00970000e-01  1.01140002e-02  2.40739994e-03\n",
      "  -8.42399970e-02  1.11139998e-01  2.00770006e-01 -2.11610004e-01\n",
      "   8.89460027e-01  1.01559997e-01 -5.61020002e-02  3.60019989e-02\n",
      "  -2.67899990e-01  1.13409996e+00  1.64959997e-01  4.17340010e-01\n",
      "  -7.80079979e-03 -1.48189999e-02  8.08549970e-02  5.13149977e-01\n",
      "  -2.50620008e-01 -2.62529999e-02 -2.67120004e-01  4.43620011e-02\n",
      "   6.33850023e-02 -2.45629996e-01  6.09339997e-02 -1.92460008e-02\n",
      "   4.68169987e-01 -7.65900016e-02  2.62930006e-01 -3.28169987e-02\n",
      "   1.66899994e-01 -3.22369993e-01  3.58610004e-01 -1.52459994e-01\n",
      "  -5.51999986e-01  3.99549991e-01 -3.35480012e-02  3.25239986e-01\n",
      "  -2.14969993e-01 -2.74419993e-01 -5.93379997e-02  4.25830007e-01\n",
      "   2.92210013e-01 -1.16319999e-01  1.60569996e-01 -1.74710006e-01\n",
      "   4.24970001e-01  3.24860007e-01  5.29900007e-02  3.37379992e-01\n",
      "   7.67679989e-01  3.52299988e-01  2.55710006e-01 -9.79759991e-02\n",
      "  -4.71329987e-01  2.44760007e-01 -4.86330003e-01 -3.60199988e-01\n",
      "   4.09330018e-02 -2.66660005e-01  6.98350012e-01 -2.94809997e-01\n",
      "   1.76929999e-02 -1.43879995e-01 -1.34930000e-01 -1.46620005e-01\n",
      "   9.72479999e-01 -1.67030007e-01  1.57430004e-02  1.79839998e-01\n",
      "   6.96449995e-01  9.85240005e-03  3.17550004e-01 -1.80940002e-01\n",
      "  -4.52309996e-01  3.23719997e-03 -2.94710010e-01  2.75510013e-01\n",
      "   1.20940000e-01  3.07229999e-02 -3.80470008e-01 -1.14469998e-01\n",
      "   1.64969996e-01  1.43600002e-01  1.63680002e-01 -2.33789995e-01]\n",
      " [-1.80620000e-01  2.84069985e-01 -1.62420005e-01 -3.49440007e-03\n",
      "  -4.14590001e-01  8.08510005e-01  6.23179972e-01 -5.61290001e-03\n",
      "   7.35080019e-02  7.19120026e-01 -1.32049993e-01 -3.54739994e-01\n",
      "  -4.64050007e+00 -2.77440012e-01 -3.23329985e-01  9.26589966e-02\n",
      "  -5.12239989e-03 -5.72080016e-01 -4.49539989e-01 -4.44370002e-01\n",
      "   4.30280007e-02  2.69760013e-01 -7.76449963e-02  1.05520003e-01\n",
      "   3.87529992e-02  3.68880004e-01 -2.70999998e-01  1.24820001e-01\n",
      "   1.86299995e-01 -7.54599988e-01  6.24319986e-02 -8.79390001e-01\n",
      "  -4.73769993e-01  4.76330012e-01 -4.60429996e-01 -6.88629970e-02\n",
      "   4.35159989e-02 -5.07650018e-01  1.36890002e-02  2.38930002e-01\n",
      "  -6.65229976e-01 -5.40839992e-02  3.83370012e-01 -9.05940030e-03\n",
      "   2.21400000e-02  1.86260000e-01  3.62320006e-01  6.22889996e-01\n",
      "  -4.41780001e-01  8.85339975e-01 -3.49209994e-01  6.88130021e-01\n",
      "   4.42869991e-01 -2.12349996e-01  2.22489998e-01  5.97960018e-02\n",
      "  -6.68579996e-01 -4.45239991e-02  5.05879998e-01  2.89180011e-01\n",
      "  -1.12580001e-01 -1.67150006e-01  1.81970000e-01 -7.01300025e-01\n",
      "   3.16909999e-01 -4.84340012e-01 -5.21600008e-01  2.72630006e-02\n",
      "   4.87679988e-02  4.09299999e-01  3.38099986e-01  3.99280012e-01\n",
      "   1.87490005e-02 -5.48089981e-01  6.39360011e-01 -5.11810005e-01\n",
      "  -2.01250002e-01 -1.96089998e-01 -2.04300001e-01 -2.03410000e-01\n",
      "   1.59679997e+00 -4.10959981e-02 -4.91360009e-01 -5.32419980e-01\n",
      "   3.57239991e-01  6.55120015e-01  4.82870013e-01  2.24000007e-01\n",
      "  -1.82830006e-01 -1.06169999e-01  1.81209996e-01 -5.44040017e-02\n",
      "   4.55060005e-01  3.35040003e-01 -4.05669987e-01 -4.52479988e-01\n",
      "  -5.17449975e-01 -2.34840006e-01  4.63710010e-01 -6.03179991e-01]\n",
      " [-5.97889982e-02  7.60350004e-02 -7.22080003e-03 -4.47739996e-02\n",
      "  -6.04589999e-01 -4.17679995e-01  4.16500010e-02 -1.05509996e-01\n",
      "  -2.70859987e-01  3.38800013e-01 -1.73070002e-02 -5.21660000e-02\n",
      "  -4.04029989e+00 -9.68530029e-03 -8.91149998e-01  1.51300002e-02\n",
      "   1.55310005e-01  5.85009992e-01 -5.53390011e-02 -4.08609986e-01\n",
      "  -3.63810003e-01 -3.94840002e-01 -2.68539995e-01 -4.04799990e-02\n",
      "  -8.82300019e-01  4.71139997e-01 -1.63650006e-01 -2.55190015e-01\n",
      "   1.00829995e+00 -9.95870009e-02  8.67099985e-02  4.21920009e-02\n",
      "  -9.35620010e-01 -7.13129997e-01  3.69410008e-01  1.94350004e-01\n",
      "  -3.30720007e-01 -1.17140003e-01  2.58230008e-02  9.35349986e-02\n",
      "  -1.68750003e-01 -7.89030015e-01  5.10039985e-01 -6.16219997e-01\n",
      "  -2.58819997e-01  3.96759987e-01 -4.45369989e-01  9.22999978e-01\n",
      "  -1.10480003e-01 -2.95489997e-01 -1.85749996e-02 -1.30129993e-01\n",
      "  -4.40090001e-01 -9.97219980e-01  4.50089991e-01 -1.65140003e-01\n",
      "   1.79409996e-01 -6.56369984e-01  8.34949970e-01 -4.66789991e-01\n",
      "   4.25199986e-01 -4.12499994e-01  2.51630008e-01 -4.72359985e-01\n",
      "  -1.95230007e-01  4.71879989e-01  7.39310011e-02  4.07579988e-01\n",
      "   7.97029972e-01  3.38519990e-01 -6.94869995e-01  1.00199997e-01\n",
      "   6.39789999e-01 -1.17119998e-01  4.42699999e-01 -5.12870014e-01\n",
      "   3.26810002e-01 -7.97270000e-01 -7.33439982e-01 -4.55680013e-01\n",
      "   1.09259999e+00  4.66250002e-01  6.59759998e-01  7.38110006e-01\n",
      "   6.86839998e-01 -2.71459997e-01  2.11050007e-02  3.64720002e-02\n",
      "   4.02689993e-01 -3.65259983e-02  3.47400010e-01  3.45860004e-01\n",
      "  -2.27119997e-01 -1.89439997e-01 -4.48000014e-01  1.61599994e-01\n",
      "   4.96140003e-01  3.53240013e-01  2.66059995e-01 -5.11920005e-02]]\n"
     ]
    }
   ],
   "source": [
    "def load_embedding():\n",
    "    glove_model_filename = str(glove_model) + \".txt\"\n",
    "    glove_file_path = os.path.join(processed_folder_path, glove_model_filename)\n",
    "    if not os.path.exists(glove_file_path): #OTHERWISE CONVERT HERE TO TXT AND ALSO WHEN YOU SAVE\n",
    "        # Check if the pre-trained Word2Vec model is already downloaded. If not, download it.\n",
    "        print(\"GloVe model doesn't exist...\")\n",
    "        model = api.load(glove_model)\n",
    "        model.save_word2vec_format(glove_file_path, binary=False)\n",
    "        # 5186/12465 (41.60%) are not defined with twitter-glove\n",
    "        # 5177/12465 (41.53%) are not defined with wiki\n",
    "\n",
    "    # Load embedding into memory, skip first line\n",
    "    print(\"Loading w2v model...\")\n",
    "    file = open(glove_file_path, 'r', encoding='utf8')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "\n",
    "    # Create a map of words to vectors\n",
    "    pretrained_embeddings = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # Set key as string word, value as numpy array for vector\n",
    "        pretrained_embeddings[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "\n",
    "    return pretrained_embeddings\n",
    "\n",
    "def get_embedding_matrix(loaded_embedding, tokenizer, embedding_dim):\n",
    "    # Create a weight matrix for the Embedding layer from a loaded/pretrained embedding\n",
    "\n",
    "    # Define weight matrix dimensions (vocab_size + 1 for unknown words) with all 0 \n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "\n",
    "    count_all_words = 0\n",
    "    count_na_words = 0\n",
    "    zero_vector_words = []\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # Map loaded vectors to terms in vocab\n",
    "        if word in loaded_embedding.keys():\n",
    "            embedding_matrix[i] = loaded_embedding.get(word)\n",
    "        else:\n",
    "            # Some terms such as emojis or neg-tagged words are not found in the loaded w2v model, hence they will have vectors with all 0\n",
    "            zero_vector_words.append(word)\n",
    "            count_na_words += 1\n",
    "        count_all_words += 1\n",
    "    print(f'{count_na_words}/{count_all_words} ({((count_na_words/count_all_words)*100):.2f}%) are not defined in the pretrained W2V model and will receive vectors with all 0.')\n",
    "    print(f\"W2V Embedding Matrix shape: {embedding_matrix.shape}\")\n",
    "    print(f\"Embedding Matrix:\\n{embedding_matrix[:5]}\")\n",
    "\n",
    "    # Save unrecognized words that are not present in the GloVe model\n",
    "    file_path = os.path.join(processed_folder_path, \"out_of_glove_words.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(zero_vector_words))\n",
    "\n",
    "    # Save embeddings\n",
    "    # TODO: delete\n",
    "    file_path = os.path.join(processed_folder_path, \"embedding_matrix.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(' '.join(str(x) for x in row) for row in embedding_matrix))\n",
    "    np.savetxt(os.path.join(processed_folder_path, \"embedding_matrix.txt\"), embedding_matrix, fmt='%f')\n",
    "\n",
    "    # TODO: keep\n",
    "    with open(os.path.join(processed_folder_path, 'embedding_matrix.pkl'), 'wb') as file:\n",
    "        pickle.dump(embedding_matrix, file)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "pretrained_embeddings = load_embedding()\n",
    "embedding_vectors = get_embedding_matrix(pretrained_embeddings, tokenizer, EMBEDDING_DIM)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFINN - worse - VECTORS NOT SAVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, -2.0], [-3.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0], [0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, -3.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]]\n",
      "[[-3  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [-3  0  0 ...  0  0  0]\n",
      " [ 0  2  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "(41000, 430)\n"
     ]
    }
   ],
   "source": [
    "afinn = Afinn(language='en')\n",
    "\n",
    "def compute_scores(text):\n",
    "    words = text.split() \n",
    "    scores = [afinn.score(word) for word in words]  # compute the AFINN score for each word\n",
    "    return scores\n",
    "\n",
    "# Compute AFINN scores\n",
    "x_train_scores = [compute_scores(text) for text in x_train]\n",
    "x_val_scores = [compute_scores(text) for text in x_val]\n",
    "x_test_scores = [compute_scores(text) for text in x_test]\n",
    "print(x_train_scores[:5])\n",
    "\n",
    "# Pad the sequences with zeros\n",
    "x_train_scores_padded = pad_sequences(x_train_scores, maxlen=max_seq_length, padding='post')\n",
    "x_val_scores_padded = pad_sequences(x_val_scores, maxlen=max_seq_length, padding='post')\n",
    "x_test_scores_padded = pad_sequences(x_test_scores, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "print(x_train_scores_padded[:5])\n",
    "print(x_train_scores_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data zero percentage: 87.03%\n",
      "Validation data zero percentage: 86.77%\n",
      "Test data zero percentage: 86.68%\n"
     ]
    }
   ],
   "source": [
    "def calculate_zero_percentage(score_list):\n",
    "    total_scores = sum([len(scores) for scores in score_list])\n",
    "    total_zeros = sum([scores.count(0) for scores in score_list])\n",
    "    return total_zeros / total_scores * 100\n",
    "\n",
    "# Calculate the percentage of exact zeros\n",
    "train_zero_percentage = calculate_zero_percentage(x_train_scores)\n",
    "val_zero_percentage = calculate_zero_percentage(x_val_scores)\n",
    "test_zero_percentage = calculate_zero_percentage(x_test_scores)\n",
    "\n",
    "print(f'Training data zero percentage: {train_zero_percentage:.2f}%')\n",
    "print(f'Validation data zero percentage: {val_zero_percentage:.2f}%')\n",
    "print(f'Test data zero percentage: {test_zero_percentage:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.875 -0.5    0.    ...  0.     0.     0.   ]\n",
      " [ 0.     0.     0.    ...  0.     0.     0.   ]\n",
      " [-0.625  0.125  0.375 ...  0.     0.     0.   ]\n",
      " [ 0.     0.25   0.    ...  0.     0.     0.   ]\n",
      " [-0.125 -0.25   0.    ...  0.     0.     0.   ]]\n",
      "(41000, 430)\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(word):\n",
    "    synsets = wn.synsets(word) # get set of synonyms\n",
    "    if not synsets:\n",
    "        return 0  # return 0 if the word is not in WordNet\n",
    "    synset = synsets[0] # The first synset is the most common sense (which are orderd by freq)\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "    return swn_synset.pos_score() - swn_synset.neg_score() # Return the overall sentiment polarity\n",
    "\n",
    "def compute_scores(text):\n",
    "    words = text.split()\n",
    "    scores = [get_sentiment(word) for word in words]\n",
    "    return scores\n",
    "\n",
    "def pad_scores(score_list, max_len):\n",
    "    # Pad the sequences to max sequence length\n",
    "    return [scores + [0] * (max_len - len(scores)) for scores in score_list]\n",
    "\n",
    "# Compute sentiment scores\n",
    "x_train_scores = [compute_scores(text) for text in x_train]\n",
    "x_val_scores = [compute_scores(text) for text in x_val]\n",
    "x_test_scores = [compute_scores(text) for text in x_test]\n",
    "\n",
    "# Pad the scores\n",
    "x_train_scores_padded = np.array(pad_scores(x_train_scores, max_seq_length))\n",
    "x_val_scores_padded = np.array(pad_scores(x_val_scores, max_seq_length))\n",
    "x_test_scores_padded = np.array(pad_scores(x_test_scores, max_seq_length))\n",
    "\n",
    "print(x_train_scores_padded[:5])\n",
    "print(x_train_scores_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data zero percentage: 72.06%\n",
      "Validation data zero percentage: 72.24%\n",
      "Test data zero percentage: 72.17%\n"
     ]
    }
   ],
   "source": [
    "def zero_percentage(score_list):\n",
    "    total_scores = sum([len(scores) for scores in score_list])\n",
    "    total_zeros = sum([scores.count(0) for scores in score_list])\n",
    "    return total_zeros / total_scores * 100\n",
    "\n",
    "# Calculate the percentage of exact zeros\n",
    "train_zero_percentage = zero_percentage(x_train_scores)\n",
    "val_zero_percentage = zero_percentage(x_val_scores)\n",
    "test_zero_percentage = zero_percentage(x_test_scores)\n",
    "\n",
    "print(f'Training data zero percentage: {train_zero_percentage:.2f}%')\n",
    "print(f'Validation data zero percentage: {val_zero_percentage:.2f}%')\n",
    "print(f'Test data zero percentage: {test_zero_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the padded scores\n",
    "with open(os.path.join(processed_folder_path, 'x_train_scores_padded.pkl'), 'wb') as file:\n",
    "    pickle.dump(x_train_scores_padded, file)\n",
    "with open(os.path.join(processed_folder_path, 'x_val_scores_padded.pkl'), 'wb') as file:\n",
    "    pickle.dump(x_val_scores_padded, file)\n",
    "with open(os.path.join(processed_folder_path, 'x_test_scores_padded.pkl'), 'wb') as file:\n",
    "    pickle.dump(x_test_scores_padded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save padded scores to text files\n",
    "np.savetxt(os.path.join(processed_folder_path, 'x_train_scores_padded.txt'), x_train_scores_padded, fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
