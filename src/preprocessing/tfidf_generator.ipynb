{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/s3985113/Thesis_Jupyter_Final/src/\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname(os.path.abspath('processor.ipynb'))\n",
    "data_path = os.path.join(script_dir, 'Thesis_Jupyter_Final/src/')\n",
    "os.getcwd()\n",
    "print(data_path)\n",
    "\n",
    "input_folder_path = os.path.join(data_path, 'input')\n",
    "processed_folder_path = os.path.join(data_path, 'input/processed/neg_tagged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    bad superficial explanation speaks fast need s...\n",
      "1    first let grade quiz purchase highly disappointed\n",
      "2    horrible test little sense use element describ...\n",
      "3    least favorite informative_NEG far_NEG style_N...\n",
      "4    guess thing explanation level_NEG assignment_N...\n",
      "Name: x, dtype: object\n",
      "\n",
      "Vocab size:  10573\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    x = df['x']\n",
    "    y = df['y']\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def load_vocab(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    return vocab, vocab_size\n",
    "    \n",
    "\n",
    "x_train, y_train = load_data(os.path.join(processed_folder_path, \"train.csv\"))\n",
    "x_val, y_val = load_data(os.path.join(processed_folder_path, \"val.csv\"))\n",
    "x_test, y_test = load_data(os.path.join(processed_folder_path, \"test.csv\"))\n",
    "print(x_train[:5])\n",
    "print()\n",
    "\n",
    "\n",
    "vocab_data_filename = \"vocab.pkl\"\n",
    "vocab, vocab_size = load_vocab(os.path.join(processed_folder_path, vocab_data_filename))\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Shape (doc, vocab_size):\n",
      "* train: (41000, 10573)\n",
      "* validation: (11540, 10573)\n",
      "* test: (11921, 10573)\n",
      "\n",
      "x_train_tfidf:\n",
      "  (0, 4944)\t0.333611549281506\n",
      "  (0, 2133)\t0.2961339731660829\n",
      "  (0, 1962)\t0.2790813018609294\n",
      "  (0, 1829)\t0.2845491982794516\n",
      "  (0, 1738)\t0.2471451738558906\n",
      "  (0, 1627)\t0.27254171122483684\n",
      "  (0, 784)\t0.24895427348562024\n",
      "  (0, 581)\t0.2096952161674913\n",
      "  (0, 438)\t0.21087359433125996\n",
      "  (0, 400)\t0.21511850812086866\n",
      "  (0, 337)\t0.22498142205493177\n",
      "  (0, 307)\t0.22522954040809726\n",
      "  (0, 223)\t0.21748277299714208\n",
      "  (0, 207)\t0.2005277841281215\n",
      "  (0, 41)\t0.16507539015093728\n",
      "  (0, 37)\t0.3065855262224691\n",
      "  (1, 3132)\t0.48745999184528344\n",
      "  (1, 1117)\t0.3794568593358208\n",
      "  (1, 1065)\t0.41963408220421816\n",
      "  (1, 811)\t0.43121465736774295\n",
      "  (1, 38)\t0.2770028355754138\n",
      "  (1, 22)\t0.27822837453512\n",
      "  (1, 20)\t0.3197105022731662\n",
      "  (2, 3942)\t0.3961231024356585\n",
      "  (2, 2937)\t0.3742032094232427\n",
      "  :\t:\n",
      "  (40997, 8925)\t0.4335846690233216\n",
      "  (40997, 4246)\t0.3889950728579958\n",
      "  (40997, 2499)\t0.36388494758151557\n",
      "  (40997, 1887)\t0.34690683112213216\n",
      "  (40997, 1080)\t0.32385818576697817\n",
      "  (40997, 665)\t0.29932936951200767\n",
      "  (40997, 211)\t0.2333765628328547\n",
      "  (40997, 160)\t0.23121906946151174\n",
      "  (40997, 141)\t0.23627942948322128\n",
      "  (40997, 76)\t0.22292043563693573\n",
      "  (40998, 1848)\t0.43831688266822394\n",
      "  (40998, 913)\t0.3760677807264082\n",
      "  (40998, 641)\t0.3929684005037529\n",
      "  (40998, 196)\t0.5298654315663285\n",
      "  (40998, 168)\t0.2928021224321956\n",
      "  (40998, 76)\t0.2934145805864686\n",
      "  (40998, 38)\t0.24381262539610832\n",
      "  (40999, 4886)\t0.4848468205939975\n",
      "  (40999, 1436)\t0.3843771673680766\n",
      "  (40999, 1403)\t0.36808792285342495\n",
      "  (40999, 985)\t0.344616717963534\n",
      "  (40999, 545)\t0.31961205784997043\n",
      "  (40999, 513)\t0.3176331302501415\n",
      "  (40999, 315)\t0.29990169970303915\n",
      "  (40999, 89)\t0.26447050889486823\n"
     ]
    }
   ],
   "source": [
    "max_features = 7000\n",
    "max_df = 0.95\n",
    "min_df = 5\n",
    "\n",
    "def get_tfidf_vectorizer(vocab, max_features, min_df, max_df):\n",
    "    # Convert vocab to a dict in order to use it in TF-IDF vectorizer\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features, # maximum number of features to keep, check unique vocabs and determine based on that, high causes saprse metrics and low value causes loss in important words/vocab\n",
    "        vocabulary=vocab_dict,\n",
    "        lowercase=False,\n",
    "        ngram_range=(1, 2),  # range of n-grams,\n",
    "        max_df=max_df,  # ignore terms that have a document frequency strictly higher than the threshold\n",
    "        min_df=min_df,  # ignore terms that have a document frequency strictly lower than the threshold.\n",
    "        use_idf=True,  # enable IDF weighting\n",
    "        smooth_idf=True,  # smooth IDF weights --> provides stability, reduces run time errors\n",
    "        sublinear_tf=True  # apply sublinear scaling to term frequencies\n",
    "    )\n",
    "\n",
    "    # Save tfidf vectorizer\n",
    "    file_path = os.path.join(processed_folder_path, 'tfidf_vectorizer.joblib')\n",
    "    dump(tfidf_vectorizer, file_path)\n",
    "\n",
    "    return tfidf_vectorizer\n",
    "\n",
    "def transform_to_tfidf(x_train, x_val, x_test):\n",
    "    # Fit and transform the training set\n",
    "    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "    # Transform the validation and testing set\n",
    "    x_val_tfidf = tfidf_vectorizer.transform(x_val)\n",
    "    x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "    return x_train_tfidf, x_val_tfidf, x_test_tfidf\n",
    "\n",
    "tfidf_vectorizer = get_tfidf_vectorizer(vocab, max_features, min_df, max_df)\n",
    "x_train_tfidf, x_val_tfidf, x_test_tfidf = transform_to_tfidf(x_train, x_val, x_test)\n",
    "\n",
    "# Save data\n",
    "scipy.sparse.save_npz(os.path.join(processed_folder_path, \"train_tfidf.npz\"), x_train_tfidf)\n",
    "scipy.sparse.save_npz(os.path.join(processed_folder_path, \"val_tfidf.npz\"), x_val_tfidf)\n",
    "scipy.sparse.save_npz(os.path.join(processed_folder_path, \"test_tfidf.npz\"), x_test_tfidf)\n",
    "\n",
    "print(\"\\nData Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_tfidf.shape, x_val_tfidf.shape, x_test_tfidf.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: delete\n",
    "def save_tfidf_data(data, filename, feature_names):\n",
    "    # Save the matrix with feature names as a DataFrame\n",
    "    data = pd.DataFrame(data.toarray(), columns=feature_names)\n",
    "    file_path = os.path.join(processed_folder_path, filename)\n",
    "    data.to_csv(file_path, sep=',', index=False) # TODO: if this isn't working, note that you added sep=','\n",
    "\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Save vectorized data\n",
    "#save_tfidf_data(x_train_tfidf, \"train_tfidf.csv\", feature_names)\n",
    "#save_tfidf_data(x_val_tfidf, \"val_tfidf.csv\", feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
