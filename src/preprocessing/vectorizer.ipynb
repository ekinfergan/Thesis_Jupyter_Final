{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "import gensim.downloader as api\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import pad_sequences\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "NUM_of_CLASSES = 3\n",
    "w2v_pretrained_model = \"glove-twitter-100\"\n",
    "#w2v_pretrained_model = \"glove-wiki-gigaword-100\"\n",
    "w2v_pretrained_model_filename = str(w2v_pretrained_model) + \"-word2vec.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/s3985113/Thesis_Jupyter_Final/src/\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname(os.path.abspath('vectorizer.ipynb'))\n",
    "data_path = os.path.join(script_dir, 'Thesis_Jupyter_Final/src/')\n",
    "os.getcwd()\n",
    "print(data_path)\n",
    "\n",
    "input_folder_path = os.path.join(data_path, 'input')\n",
    "processed_folder_path = os.path.join(data_path, 'input/processed')\n",
    "results_folder_path = \"results\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(results_folder_path):\n",
    "    os.makedirs(results_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, process=True):\n",
    "    # Load data\n",
    "    data_file_path = os.path.join(input_folder_path, filename)\n",
    "    df = pd.read_csv(data_file_path)\n",
    "\n",
    "    x = df['x']\n",
    "    y = df['y']\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def load_vocab():\n",
    "    vocab_data_filename = \"vocab.pkl\"\n",
    "    file_path = os.path.join(processed_folder_path, vocab_data_filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    return vocab, vocab_size\n",
    "    \n",
    "\n",
    "x_train, y_train = load_data(\"train.csv\", process=False)\n",
    "x_val, y_val = load_data(\"val.csv\", process=False)\n",
    "x_test, y_test = load_data(\"test.csv\", process=False)\n",
    "\n",
    "vocab, vocab_size = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_npz(matrix, file_path):\n",
    "    scipy.sparse.save_npz(file_path, matrix)\n",
    "\n",
    "def save_to_npy(arr, file_path):\n",
    "    np.save(file_path, np.array(arr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tfidf_vectorizer' (TfidfVectorizer)\n",
      "\n",
      "Data Shape (doc, vocab_size):\n",
      "* train: (41000, 12644)\n",
      "* validation: (11532, 12644)\n",
      "* test: (11905, 12644)\n",
      "\n",
      "x_train_tfidf:\n",
      "  (0, 10526)\t0.3701578628181033\n",
      "  (0, 4894)\t0.32999433018072144\n",
      "  (0, 2083)\t0.29343888501321036\n",
      "  (0, 1912)\t0.2804613515563524\n",
      "  (0, 1779)\t0.2815241272434237\n",
      "  (0, 1688)\t0.24267616713297727\n",
      "  (0, 1577)\t0.27202940388432717\n",
      "  (0, 734)\t0.24386561416344335\n",
      "  (0, 531)\t0.21104954098373907\n",
      "  (0, 388)\t0.20404592571011101\n",
      "  (0, 350)\t0.2142543643095343\n",
      "  (0, 287)\t0.22160277130877745\n",
      "  (0, 257)\t0.2214904145041457\n",
      "  (0, 173)\t0.2183678200398483\n",
      "  (0, 157)\t0.1994293886378569\n",
      "  (1, 3082)\t0.5745930155556883\n",
      "  (1, 1067)\t0.4371293802225125\n",
      "  (1, 1015)\t0.48551153531743024\n",
      "  (1, 761)\t0.4929901829596753\n",
      "  (2, 3892)\t0.42134938973870983\n",
      "  (2, 2887)\t0.40702494790002386\n",
      "  (2, 2474)\t0.41414876264668526\n",
      "  (2, 1363)\t0.3570125709620734\n",
      "  (2, 940)\t0.3888075226900665\n",
      "  (2, 468)\t0.3431422543770557\n",
      "  :\t:\n",
      "  (40996, 170)\t0.13892466275358323\n",
      "  (40996, 162)\t0.25212625713825676\n",
      "  (40996, 52)\t0.13175209630891307\n",
      "  (40996, 34)\t0.1253205751983123\n",
      "  (40996, 15)\t0.1328236908823095\n",
      "  (40997, 6326)\t0.5566542575669581\n",
      "  (40997, 5123)\t0.5090932089796878\n",
      "  (40997, 336)\t0.3695241591742754\n",
      "  (40997, 266)\t0.3363821888471862\n",
      "  (40997, 168)\t0.30464727193181146\n",
      "  (40997, 15)\t0.29740393511311886\n",
      "  (40998, 5019)\t0.4281410600114483\n",
      "  (40998, 4951)\t0.4491216317492248\n",
      "  (40998, 2556)\t0.38004699682709625\n",
      "  (40998, 2079)\t0.35017421726919606\n",
      "  (40998, 1050)\t0.32386160440289535\n",
      "  (40998, 730)\t0.31314786520408344\n",
      "  (40998, 453)\t0.2895069150931092\n",
      "  (40998, 214)\t0.24731581472840988\n",
      "  (40999, 3220)\t0.5600147184718514\n",
      "  (40999, 356)\t0.39952037743475327\n",
      "  (40999, 156)\t0.3870950829172401\n",
      "  (40999, 89)\t0.3383865459290881\n",
      "  (40999, 47)\t0.3311287587136669\n",
      "  (40999, 25)\t0.390861446778149\n"
     ]
    }
   ],
   "source": [
    "def get_tfidf_vectorizer(vocab, max_features, min_df, max_df):\n",
    "    # Convert vocab to a dict in order to use it in TF-IDF vectorizer\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features, # maximum number of features to keep, check unique vocabs and determine based on that, high causes saprse metrics and low value causes loss in important words/vocab\n",
    "        vocabulary=vocab_dict,\n",
    "        lowercase=False,\n",
    "        ngram_range=(1, 3),  # range of n-grams,\n",
    "        max_df=max_df,  # ignore terms that have a document frequency strictly higher than the threshold\n",
    "        min_df=min_df,  # ignore terms that have a document frequency strictly lower than the threshold.\n",
    "        use_idf=True,  # enable IDF weighting\n",
    "        smooth_idf=True,  # smooth IDF weights --> provides stability, reduces run time errors\n",
    "        sublinear_tf=True  # apply sublinear scaling to term frequencies\n",
    "    )\n",
    "\n",
    "    # Save tfidf vectorizer\n",
    "    file_path = os.path.join(processed_folder_path, 'tfidf_vectorizer.joblib')\n",
    "    dump(tfidf_vectorizer, file_path)\n",
    "\n",
    "    return tfidf_vectorizer\n",
    "\n",
    "def transform_to_tfidf(x_train, x_val, x_test):\n",
    "    # Fit and transform the training set\n",
    "    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "    # Transform the validation and testing set\n",
    "    x_val_tfidf = tfidf_vectorizer.transform(x_val)\n",
    "    x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "    # Save data\n",
    "    save_to_npz(x_train_tfidf, os.path.join(processed_folder_path, \"train_tfidf.npz\"))\n",
    "    save_to_npz(x_val_tfidf, os.path.join(processed_folder_path, \"val_tfidf.npz\"))\n",
    "    save_to_npz(x_test_tfidf, os.path.join(processed_folder_path, \"test_tfidf.npz\"))\n",
    "\n",
    "    return x_train_tfidf, x_val_tfidf, x_test_tfidf\n",
    "\n",
    "\n",
    "max_features = 50000\n",
    "max_df = 0.95\n",
    "min_df = 5\n",
    "\n",
    "tfidf_vectorizer = get_tfidf_vectorizer(vocab, max_features, min_df, max_df)\n",
    "x_train_tfidf, x_val_tfidf, x_test_tfidf = transform_to_tfidf(x_train, x_val, x_test)\n",
    "%store tfidf_vectorizer\n",
    "\n",
    "print(\"\\nData Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_tfidf.shape, x_val_tfidf.shape, x_test_tfidf.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: delete\n",
    "def save_tfidf_data(data, filename, feature_names):\n",
    "    # Save the matrix with feature names as a DataFrame\n",
    "    data = pd.DataFrame(data.toarray(), columns=feature_names)\n",
    "    file_path = os.path.join(processed_folder_path, filename)\n",
    "    data.to_csv(file_path, sep=',', index=False) # TODO: if this isn't working, note that you added sep=','\n",
    "\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Save vectorized data\n",
    "#save_tfidf_data(x_train_tfidf, \"train_tfidf.csv\", feature_names)\n",
    "#save_tfidf_data(x_val_tfidf, \"val_tfidf.csv\", feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 476\n"
     ]
    }
   ],
   "source": [
    "def find_max_seq_len(data):\n",
    "    # Find maximum sequence length\n",
    "    max_seq_length = max([len(line.split()) for line in data])\n",
    "    print(f'Maximum review length: {max_seq_length}')\n",
    "\n",
    "    return max_seq_length\n",
    "\n",
    "max_seq_length = find_max_seq_len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Data Shape (doc, vocab_size):\n",
      "* train: (41000, 476)\n",
      "* validation: (11532, 476)\n",
      "* test: (11905, 476)\n",
      "\n",
      "x_train_tfidf:\n",
      "[[ 269  759 1670 ...    0    0    0]\n",
      " [ 942  806 2151 ...    0    0    0]\n",
      " [2101  172  615 ...    0    0    0]\n",
      " ...\n",
      " [ 132  170  425 ...    0    0    0]\n",
      " [1983  645  161 ...    0    0    0]\n",
      " [ 232  205  222 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "def fit_tokenizer(data):\n",
    "    # Fit tokenizer (on training data)\n",
    "    tokenizer = Tokenizer()\n",
    "    # Remove default filters, including punctuation\n",
    "    tokenizer.filters = \"\"  \n",
    "    # Disable lowercase conversion\n",
    "    tokenizer.lower = False  \n",
    "    tokenizer.fit_on_texts(data) \n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def encode_text(lines, tokenizer, max_length, filename):\n",
    "    # Integer encode\n",
    "    encoded_seq = tokenizer.texts_to_sequences(lines)\n",
    "    # Pad the encoded sequences\n",
    "    padded = pad_sequences(encoded_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Save to np.array\n",
    "    save_to_npy(padded, os.path.join(processed_folder_path, filename))\n",
    "\n",
    "    return padded\n",
    "    \n",
    "    \n",
    "tokenizer = fit_tokenizer(x_train)\n",
    "\n",
    "# Encode Data\n",
    "x_train_encoded = encode_text(x_train, tokenizer, max_seq_length, \"train_encoded_x.npy\")\n",
    "x_val_encoded = encode_text(x_val, tokenizer, max_seq_length, \"val_encoded_x.npy\")\n",
    "x_test_encoded = encode_text(x_test, tokenizer, max_seq_length, \"test_encoded_x.npy\")\n",
    "\n",
    "print(\"\\nEncoded Data Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_encoded.shape, x_val_encoded.shape, x_test_encoded.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y-Encoded Data Shape:\n",
      "* train: (41000, 3)\n",
      "* validation: (11532, 3)\n",
      "* test: (11905, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: can't remember if this is used somewhere else, if not save data inside function\n",
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded\n",
    "\n",
    "# Convert sentiment labels to one-hot encoding\n",
    "y_train_encoded = one_hot_encode(y_train)\n",
    "y_val_encoded = one_hot_encode(y_val)\n",
    "y_test_encoded = one_hot_encode(y_test)\n",
    "\n",
    "# Save y-encoded sets\n",
    "save_to_npy(y_train_encoded, os.path.join(processed_folder_path, \"train_encoded_y.npy\"))\n",
    "save_to_npy(y_val_encoded, os.path.join(processed_folder_path, \"val_encoded_y.npy\"))\n",
    "save_to_npy(y_test_encoded, os.path.join(processed_folder_path, \"test_encoded_y.npy\"))\n",
    "   \n",
    "print(\"\\ny-Encoded Data Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_vocab_size:  12466\n",
      "Words in tokenizer but not in vocab:  0\n"
     ]
    }
   ],
   "source": [
    "# Total vocabulary size plus 0 for unknown words\n",
    "embedding_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"embedding_vocab_size: \", embedding_vocab_size)\n",
    "\n",
    "# Check if there are any words identified via the tokenizer that are not in vocab\n",
    "tokenizer_vocab = set(tokenizer.word_index.keys())\n",
    "vocab_set = set(vocab)\n",
    "tokenizer_only_words = tokenizer_vocab.difference(vocab_set)\n",
    "print(\"Words in tokenizer but not in vocab: \", len(tokenizer_only_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading w2v model...\n",
      "5186/12465 (41.60%) are not defined in the pretrained W2V model and will receive vectors with all 0.\n",
      "W2V Embedding Matrix shape: (12466, 100)\n",
      "Embedding Matrix:\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def load_embedding():\n",
    "    w2v_pretrained_file_path = os.path.join(processed_folder_path, w2v_pretrained_model_filename)\n",
    "    if not os.path.exists(w2v_pretrained_file_path):\n",
    "        # Check if the pre-trained Word2Vec model is already downloaded. If not, download it.\n",
    "        print(\"\\nW2v model doesn't exist\")\n",
    "        model = api.load(w2v_pretrained_model)\n",
    "        model.save_word2vec_format(w2v_pretrained_file_path, binary=False)\n",
    "        # 5186/12465 (41.60%) are not defined with twitter-glove\n",
    "        # 5177/12465 (41.53%) are not defined with wiki\n",
    "\n",
    "    # Load embedding into memory, skip first line\n",
    "    print(\"Loading w2v model...\")\n",
    "    file = open(w2v_pretrained_file_path, 'r', encoding='utf8')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "\n",
    "    # Create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # Set key as string word, value as numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def get_emb_matrix(loaded_embedding, tokenizer, embedding_dim):\n",
    "    # Create a weight matrix for the Embedding layer from a loaded/pretrained embedding\n",
    "\n",
    "    # Define weight matrix dimensions (vocab_size + 1 for unknown words) with all 0 \n",
    "    emb_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "\n",
    "    count_all_words = 0\n",
    "    count_na_words = 0\n",
    "    zero_vector_words = []\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # Map loaded vectors to terms in vocab\n",
    "        if word in loaded_embedding.keys():\n",
    "            emb_matrix[i] = loaded_embedding.get(word)\n",
    "        else:\n",
    "            # Some terms such as emojis or neg-tagged words are not found in the loaded w2v model, hence they will have vectors with all 0\n",
    "            zero_vector_words.append(word)\n",
    "            count_na_words += 1\n",
    "        count_all_words += 1\n",
    "    print(f'{count_na_words}/{count_all_words} ({((count_na_words/count_all_words)*100):.2f}%) are not defined in the pretrained W2V model and will receive vectors with all 0.')\n",
    "    print(f\"W2V Embedding Matrix shape: {emb_matrix.shape}\")\n",
    "    print(f\"Embedding Matrix:\\n{emb_matrix[:10, :6]}\")\n",
    "\n",
    "    # Save unrecognized words that are not present in the GloVe model\n",
    "    file_path = os.path.join(processed_folder_path, \"out_of_glove_words.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(zero_vector_words))\n",
    "\n",
    "    # Save embeddings\n",
    "    # TODO: delete\n",
    "    file_path = os.path.join(processed_folder_path, \"embedding_matrix.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(' '.join(str(x) for x in row) for row in emb_matrix))\n",
    "\n",
    "    # TODO: keep\n",
    "    save_to_npy(emb_matrix, (os.path.join(processed_folder_path, \"embedding_matrix.npy\")))\n",
    "    \n",
    "    return emb_matrix\n",
    "\n",
    "pretrained_embedding = load_embedding()\n",
    "embedding_dim = 100\n",
    "w2v_embedding_vectors = get_emb_matrix(pretrained_embedding, tokenizer, embedding_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
