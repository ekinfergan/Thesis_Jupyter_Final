{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 15:17:20.352481: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "import gensim.downloader as api\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "NUM_of_CLASSES = 3\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/\"\n",
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/processed\"\n",
    "w2v_pretrained_model = \"glove-twitter-100\"\n",
    "#w2v_pretrained_model = \"glove-wiki-gigaword-100\"\n",
    "w2v_pretrained_model_filename = str(w2v_pretrained_model) + \"-word2vec.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vocab.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m x_val, y_val \u001b[39m=\u001b[39m load_data(\u001b[39m\"\u001b[39m\u001b[39mval.csv\u001b[39m\u001b[39m\"\u001b[39m, process\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m x_test, y_test \u001b[39m=\u001b[39m load_data(\u001b[39m\"\u001b[39m\u001b[39mtest.csv\u001b[39m\u001b[39m\"\u001b[39m, process\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m vocab, vocab_size \u001b[39m=\u001b[39m load_vocab()\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mload_vocab\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_vocab\u001b[39m():\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mvocab.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     13\u001b[0m         vocab \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m     14\u001b[0m         vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(vocab)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vocab.pkl'"
     ]
    }
   ],
   "source": [
    "def load_data(filename, process=True):\n",
    "    # Load data\n",
    "    data_file_path = os.path.join(input_folder_path, filename)\n",
    "    df = pd.read_csv(data_file_path)\n",
    "\n",
    "    x = df['x']\n",
    "    y = df['y']\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def load_vocab():\n",
    "    vocab_data_filename = \"vocab.pkl\"\n",
    "    file_path = os.path.join(processed_folder_path, vocab_data_filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    return vocab, vocab_size\n",
    "    \n",
    "\n",
    "x_train, y_train = load_data(\"train.csv\", process=False)\n",
    "x_val, y_val = load_data(\"val.csv\", process=False)\n",
    "x_test, y_test = load_data(\"test.csv\", process=False)\n",
    "\n",
    "vocab, vocab_size = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_npz(matrix, file_path):\n",
    "    scipy.sparse.save_npz(file_path, matrix)\n",
    "\n",
    "def save_to_npy(arr, file_path):\n",
    "    np.save(file_path, np.array(arr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tfidf_vectorizer' (TfidfVectorizer)\n",
      "\n",
      "Data Shape (doc, vocab_size):\n",
      "* train: (35000, 4092)\n",
      "* validation: (15677, 4092)\n",
      "* test: (19609, 4092)\n",
      "\n",
      "x_train_tfidf:\n",
      "  (0, 253)\t0.20764539103840507\n",
      "  (0, 252)\t0.2494268576357786\n",
      "  (0, 251)\t0.23090942419103383\n",
      "  (0, 250)\t0.285649407302339\n",
      "  (0, 249)\t0.2874763759700332\n",
      "  (0, 248)\t0.23272233424248345\n",
      "  (0, 247)\t0.22644815130435766\n",
      "  (0, 246)\t0.30157313471831254\n",
      "  (0, 245)\t0.2182688812454513\n",
      "  (0, 244)\t0.28505882859339643\n",
      "  (0, 243)\t0.27599078522342024\n",
      "  (0, 242)\t0.24919098459004396\n",
      "  (0, 241)\t0.20991418029291584\n",
      "  (0, 111)\t0.2123221173075666\n",
      "  (0, 47)\t0.31065651695408875\n",
      "  (0, 37)\t0.16705807094682623\n",
      "  (1, 289)\t0.3787829823814839\n",
      "  (1, 288)\t0.32564127029888723\n",
      "  (1, 287)\t0.4872232126447539\n",
      "  (1, 286)\t0.2723748773926681\n",
      "  (1, 285)\t0.4168255998135274\n",
      "  (1, 284)\t0.4335606003502168\n",
      "  (1, 71)\t0.27782783828447105\n",
      "  (2, 361)\t0.3223434726331368\n",
      "  (2, 360)\t0.37649669058721175\n",
      "  :\t:\n",
      "  (34996, 867)\t0.31956327625564074\n",
      "  (34996, 661)\t0.3354445262329107\n",
      "  (34996, 348)\t0.2895265037953397\n",
      "  (34996, 329)\t0.3347993712010634\n",
      "  (34996, 37)\t0.28312375006611634\n",
      "  (34996, 26)\t0.31905455041855074\n",
      "  (34997, 1142)\t0.3706688229394561\n",
      "  (34997, 1115)\t0.5584642649193934\n",
      "  (34997, 723)\t0.3838505605685771\n",
      "  (34997, 465)\t0.4286041276096925\n",
      "  (34997, 51)\t0.3194606870708648\n",
      "  (34997, 20)\t0.34296414808556036\n",
      "  (34998, 3672)\t0.571473481578715\n",
      "  (34998, 1764)\t0.3754528266456212\n",
      "  (34998, 1192)\t0.29644900449860834\n",
      "  (34998, 633)\t0.2669216509678035\n",
      "  (34998, 596)\t0.2723312009034004\n",
      "  (34998, 254)\t0.3752094188849729\n",
      "  (34998, 79)\t0.27305920409492995\n",
      "  (34998, 30)\t0.28951050904255166\n",
      "  (34999, 3473)\t0.5572135504323728\n",
      "  (34999, 1619)\t0.48176274065087915\n",
      "  (34999, 1242)\t0.37469942830543695\n",
      "  (34999, 942)\t0.4299582646988865\n",
      "  (34999, 568)\t0.3635298474400751\n"
     ]
    }
   ],
   "source": [
    "def get_tfidf_vectorizer(vocab, max_features, min_df, max_df):\n",
    "    # Convert vocab to a dict in order to use it in TF-IDF vectorizer\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features, # maximum number of features to keep, check unique vocabs and determine based on that, high causes saprse metrics and low value causes loss in important words/vocab\n",
    "        vocabulary=vocab_dict,\n",
    "        lowercase=False,\n",
    "        ngram_range=(1, 1),  # range of n-grams, only unigrams now\n",
    "        max_df=max_df,  # ignore terms that have a document frequency strictly higher than the threshold\n",
    "        min_df=min_df,  # ignore terms that have a document frequency strictly lower than the threshold.\n",
    "        use_idf=True,  # enable IDF weighting\n",
    "        smooth_idf=True,  # smooth IDF weights --> provides stability, reduces run time errors\n",
    "        sublinear_tf=True  # apply sublinear scaling to term frequencies\n",
    "    )\n",
    "\n",
    "    return tfidf_vectorizer\n",
    "\n",
    "\n",
    "def transform_to_tfidf(x_train, x_val, x_test):\n",
    "    # Fit and transform the training set\n",
    "    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "    # Transform the validation and testing set\n",
    "    x_val_tfidf = tfidf_vectorizer.transform(x_val)\n",
    "    x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "    # Save data\n",
    "    save_to_npz(x_train_tfidf, os.path.join(processed_folder_path, \"train_tfidf.npz\"))\n",
    "    save_to_npz(x_val_tfidf, os.path.join(processed_folder_path, \"val_tfidf.npz\"))\n",
    "    save_to_npz(x_test_tfidf, os.path.join(processed_folder_path, \"test_tfidf.npz\"))\n",
    "\n",
    "    return x_train_tfidf, x_val_tfidf, x_test_tfidf\n",
    "\n",
    "\n",
    "max_features = 10000\n",
    "max_df = 0.95\n",
    "min_df = 5\n",
    "\n",
    "tfidf_vectorizer = get_tfidf_vectorizer(vocab, max_features, min_df, max_df)\n",
    "x_train_tfidf, x_val_tfidf, x_test_tfidf = transform_to_tfidf(x_train, x_val, x_test)\n",
    "%store tfidf_vectorizer\n",
    "\n",
    "print(\"\\nData Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_tfidf.shape, x_val_tfidf.shape, x_test_tfidf.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: delete\n",
    "def save_tfidf_data(data, filename, feature_names):\n",
    "    # Save the matrix with feature names as a DataFrame\n",
    "    data = pd.DataFrame(data.toarray(), columns=feature_names)\n",
    "    file_path = os.path.join(processed_folder_path, filename)\n",
    "    data.to_csv(file_path, sep=',', index=False) # TODO: if this isn't working, note that you added sep=','\n",
    "\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Save vectorized data\n",
    "save_tfidf_data(x_train_tfidf, \"train_tfidf.csv\", feature_names)\n",
    "save_tfidf_data(x_val_tfidf, \"val_tfidf.csv\", feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 544\n",
      "Stored 'max_seq_length' (int)\n"
     ]
    }
   ],
   "source": [
    "def find_max_seq_len(data):\n",
    "    # Find maximum sequence length\n",
    "    max_seq_length = max([len(line.split()) for line in data])\n",
    "    print(f'Maximum review length: {max_seq_length}')\n",
    "\n",
    "    return max_seq_length\n",
    "\n",
    "max_seq_length = find_max_seq_len(x_train)\n",
    "%store max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(data):\n",
    "    # Fit tokenizer (on training data)\n",
    "    tokenizer = Tokenizer()\n",
    "    # Remove default filters, including punctuation\n",
    "    tokenizer.filters = \"\"  \n",
    "    # Disable lowercase conversion\n",
    "    tokenizer.lower = False  \n",
    "    tokenizer.fit_on_texts(data) \n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = fit_tokenizer(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Shape (doc, vocab_size):\n",
      "* train: (35000, 544)\n",
      "* validation: (15677, 544)\n",
      "* test: (19609, 544)\n",
      "\n",
      "x_train_tfidf:\n",
      "[[ 240  819   78 ...    0    0    0]\n",
      " [  29 1039  770 ...    0    0    0]\n",
      " [1989  214   33 ...    0    0    0]\n",
      " ...\n",
      " [ 149  482  201 ...    0    0    0]\n",
      " [ 488 3888   33 ...    0    0    0]\n",
      " [ 178  409   36 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "def encode_text(lines, tokenizer, max_length, filename):\n",
    "    # Integer encode\n",
    "    encoded_seq = tokenizer.texts_to_sequences(lines)\n",
    "    # Pad the encoded sequences\n",
    "    padded = pad_sequences(encoded_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Save to np.array\n",
    "    save_to_npy(padded, os.path.join(processed_folder_path, filename))\n",
    "\n",
    "    return padded\n",
    "    \n",
    "\n",
    "# Encode Data\n",
    "x_train_encoded = encode_text(x_train, tokenizer, max_seq_length, \"train_encoded_x.npy\")\n",
    "x_val_encoded = encode_text(x_val, tokenizer, max_seq_length, \"val_encoded_x.npy\")\n",
    "x_test_encoded = encode_text(x_test, tokenizer, max_seq_length, \"test_encoded_x.npy\")\n",
    "\n",
    "print(\"\\nData Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_encoded.shape, x_val_encoded.shape, x_test_encoded.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'get_y_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1929926/2256864913.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_y_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_y_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_y_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_y_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5571\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5572\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5573\u001b[0m         ):\n\u001b[1;32m   5574\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'get_y_values'"
     ]
    }
   ],
   "source": [
    "# TODO: necessary?\n",
    "def get_y_values(y):\n",
    "    return y.values\n",
    "\n",
    "\n",
    "y_train = y_train.get_y_values(y_train)\n",
    "y_val = y_val.get_y_values(y_val)\n",
    "y_test = y_test.get_y_values(y_test)\n",
    "\n",
    "print(\"target-data shapes:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train.shape, y_val.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y-encoded Data Shape:\n",
      "* train: (35000, 3)\n",
      "* validation: (15677, 3)\n",
      "* test: (19609, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: can't remember if this is used somewhere else, if not save data inside function\n",
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded\n",
    "\n",
    "# Convert sentiment labels to one-hot encoding\n",
    "y_train_encoded = one_hot_encode(y_train)\n",
    "y_val_encoded = one_hot_encode(y_val)\n",
    "y_test_encoded = one_hot_encode(y_test)\n",
    "\n",
    "\n",
    "save_to_npy(y_train_encoded, os.path.join(processed_folder_path, \"train_encoded_y.npy\"))\n",
    "save_to_npy(y_val_encoded, os.path.join(processed_folder_path, \"val_encoded_y.npy\"))\n",
    "save_to_npy(y_test_encoded, os.path.join(processed_folder_path, \"test_encoded_y.npy\"))\n",
    "   \n",
    "print(\"\\ny-encoded Data Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'embedding_vocab_size' (int)\n",
      "embedding_vocab_size:  4093\n"
     ]
    }
   ],
   "source": [
    "# TODO: may delete?\n",
    "# Total vocabulary size plus 0 for unknown words\n",
    "embedding_vocab_size = len(tokenizer.word_index) + 1\n",
    "%store embedding_vocab_size\n",
    "print(\"embedding_vocab_size: \", embedding_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in tokenizer but not in vocab:  0\n"
     ]
    }
   ],
   "source": [
    "# TODO: may delete?\n",
    "# Check if there are any words identified via the tokenizer that are not in vocab\n",
    "tokenizer_vocab = set(tokenizer.word_index.keys())\n",
    "vocab_set = set(vocab)\n",
    "tokenizer_only_words = tokenizer_vocab.difference(vocab_set)\n",
    "print(\"Words in tokenizer but not in vocab: \", len(tokenizer_only_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading w2v model...\n",
      "1440/4092 (35.19%) are not defined in the pretrained W2V model and will receive vectors with all 0.\n",
      "W2V Embedding Matrix shape: (4093, 100)\n",
      "Embedding Matrix:\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.13095    -0.17110001  0.21895    -0.53894001 -0.13356     0.21934   ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.33802     0.78784001  0.23646    -0.059737   -0.14753    -0.15067001]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "Stored 'w2v_embedding_vectors' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "def load_embedding():\n",
    "    w2v_pretrained_file_path = os.path.join(processed_folder_path, w2v_pretrained_model_filename)\n",
    "    if not os.path.exists(w2v_pretrained_file_path):\n",
    "        # Check if the pre-trained Word2Vec model is already downloaded. If not, download it.\n",
    "        print(\"\\nW2v model doesn't exist\")\n",
    "        model = api.load(w2v_pretrained_model)\n",
    "        model.save_word2vec_format(w2v_pretrained_file_path, binary=False)\n",
    "\n",
    "    # Load embedding into memory, skip first line\n",
    "    print(\"Loading w2v model...\")\n",
    "    file = open(w2v_pretrained_file_path, 'r', encoding='utf8')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "\n",
    "    # Create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # Set key as string word, value as numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def get_emb_matrix(loaded_embedding, tokenizer, embedding_dim):\n",
    "    # Create a weight matrix for the Embedding layer from a loaded/pretrained embedding\n",
    "\n",
    "    # Define weight matrix dimensions (vocab_size + 1 for unknown words) with all 0 \n",
    "    emb_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "\n",
    "    count_all_words = 0\n",
    "    count_na_words = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # Map loaded vectors to terms in vocab\n",
    "        if word in loaded_embedding.keys():\n",
    "            emb_matrix[i] = loaded_embedding.get(word)\n",
    "        else:\n",
    "            # Some terms such as emojis or neg-tagged words are not found in the loaded w2v model, hence they will have vectors with all 0\n",
    "            count_na_words += 1\n",
    "        count_all_words += 1\n",
    "    print(f'{count_na_words}/{count_all_words} ({((count_na_words/count_all_words)*100):.2f}%) are not defined in the pretrained W2V model and will receive vectors with all 0.')\n",
    "    print(f\"W2V Embedding Matrix shape: {emb_matrix.shape}\")\n",
    "    print(f\"Embedding Matrix:\\n{emb_matrix[:10, :6]}\")\n",
    "\n",
    "    # Save W2V model\n",
    "    # TODO: delete\n",
    "    file_path = os.path.join(processed_folder_path, \"embedding_matrix.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(' '.join(str(x) for x in row) for row in emb_matrix))\n",
    "\n",
    "    # TODO: keep\n",
    "    save_to_npy(emb_matrix, (os.path.join(processed_folder_path, \"embedding_matrix.npy\")))\n",
    "    \n",
    "    return emb_matrix\n",
    "\n",
    "pretrained_embedding = load_embedding()\n",
    "embedding_dim = 100\n",
    "w2v_embedding_vectors = get_emb_matrix(pretrained_embedding, tokenizer, embedding_dim)\n",
    "%store w2v_embedding_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
