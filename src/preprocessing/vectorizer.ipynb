{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "import gensim.downloader as api\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import pad_sequences\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "NUM_of_CLASSES = 3\n",
    "w2v_pretrained_model = \"glove-twitter-100\"\n",
    "#w2v_pretrained_model = \"glove-wiki-gigaword-100\"\n",
    "w2v_pretrained_model_filename = str(w2v_pretrained_model) + \"-word2vec.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/s3985113/Thesis_Jupyter_Final/src/\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname(os.path.abspath('vectorizer.ipynb'))\n",
    "data_path = os.path.join(script_dir, 'Thesis_Jupyter_Final/src/')\n",
    "os.getcwd()\n",
    "print(data_path)\n",
    "\n",
    "input_folder_path = os.path.join(data_path, 'input')\n",
    "processed_folder_path = os.path.join(data_path, 'input/processed')\n",
    "results_folder_path = \"results\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(results_folder_path):\n",
    "    os.makedirs(results_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, process=True):\n",
    "    # Load data\n",
    "    data_file_path = os.path.join(input_folder_path, filename)\n",
    "    df = pd.read_csv(data_file_path)\n",
    "\n",
    "    x = df['x']\n",
    "    y = df['y']\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def load_vocab():\n",
    "    vocab_data_filename = \"vocab.pkl\"\n",
    "    file_path = os.path.join(processed_folder_path, vocab_data_filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    return vocab, vocab_size\n",
    "    \n",
    "\n",
    "x_train, y_train = load_data(\"train.csv\", process=False)\n",
    "x_val, y_val = load_data(\"val.csv\", process=False)\n",
    "x_test, y_test = load_data(\"test.csv\", process=False)\n",
    "\n",
    "vocab, vocab_size = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_npz(matrix, file_path):\n",
    "    scipy.sparse.save_npz(file_path, matrix)\n",
    "\n",
    "def save_to_npy(arr, file_path):\n",
    "    np.save(file_path, np.array(arr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tfidf_vectorizer' (TfidfVectorizer)\n",
      "\n",
      "Data Shape (doc, vocab_size):\n",
      "* train: (41000, 32271)\n",
      "* validation: (11541, 32271)\n",
      "* test: (11927, 32271)\n",
      "\n",
      "x_train_tfidf:\n",
      "  (0, 16667)\t0.3340626292951124\n",
      "  (0, 10586)\t0.3340626292951124\n",
      "  (0, 4954)\t0.2832062570364835\n",
      "  (0, 2143)\t0.24622299074589576\n",
      "  (0, 1972)\t0.24158921518154958\n",
      "  (0, 1839)\t0.24306190367690358\n",
      "  (0, 1748)\t0.21018143824743002\n",
      "  (0, 1637)\t0.23137019421515947\n",
      "  (0, 794)\t0.20963592597578756\n",
      "  (0, 591)\t0.1794775743569737\n",
      "  (0, 448)\t0.17828290066587452\n",
      "  (0, 410)\t0.1840822468807772\n",
      "  (0, 347)\t0.19165520106708653\n",
      "  (0, 317)\t0.19186375805660139\n",
      "  (0, 233)\t0.18811255223068046\n",
      "  (0, 217)\t0.17003426400216445\n",
      "  (0, 51)\t0.1414731906551496\n",
      "  (0, 47)\t0.2596634168344216\n",
      "  (0, 7)\t0.23175049169670642\n",
      "  (1, 3142)\t0.49792756873210103\n",
      "  (1, 1127)\t0.3742871262374112\n",
      "  (1, 1075)\t0.4170384752631189\n",
      "  (1, 821)\t0.42938547104480307\n",
      "  (1, 48)\t0.2740555055811665\n",
      "  (1, 32)\t0.2768839018778363\n",
      "  :\t:\n",
      "  (40996, 243)\t0.13917589764075816\n",
      "  (40996, 241)\t0.13237590790751455\n",
      "  (40996, 214)\t0.22491664991776356\n",
      "  (40996, 178)\t0.1212018974639381\n",
      "  (40996, 176)\t0.13331215579426486\n",
      "  (40996, 161)\t0.1094659802198736\n",
      "  (40996, 143)\t0.1242511651241684\n",
      "  (40996, 110)\t0.12302246902295963\n",
      "  (40996, 103)\t0.11433049629000266\n",
      "  (40996, 94)\t0.11622247928971599\n",
      "  (40996, 57)\t0.11850142690121916\n",
      "  (40996, 56)\t0.10863201802018187\n",
      "  (40997, 415)\t0.46583744068518407\n",
      "  (40997, 312)\t0.4487967078153373\n",
      "  (40997, 249)\t0.43153758626009475\n",
      "  (40997, 175)\t0.3979020657843364\n",
      "  (40997, 61)\t0.358315717388545\n",
      "  (40997, 0)\t0.3295999063587455\n",
      "  (40998, 930)\t0.6373215843861528\n",
      "  (40998, 159)\t0.43350942689091815\n",
      "  (40998, 45)\t0.45979757974716007\n",
      "  (40998, 14)\t0.44099542007926273\n",
      "  (40999, 846)\t0.6226305548223549\n",
      "  (40999, 575)\t0.589564779872217\n",
      "  (40999, 14)\t0.5145333444353543\n"
     ]
    }
   ],
   "source": [
    "def get_tfidf_vectorizer(vocab, max_features, min_df, max_df):\n",
    "    # Convert vocab to a dict in order to use it in TF-IDF vectorizer\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features, # maximum number of features to keep, check unique vocabs and determine based on that, high causes saprse metrics and low value causes loss in important words/vocab\n",
    "        vocabulary=vocab_dict,\n",
    "        lowercase=False,\n",
    "        ngram_range=(1, 3),  # range of n-grams,\n",
    "        max_df=max_df,  # ignore terms that have a document frequency strictly higher than the threshold\n",
    "        min_df=min_df,  # ignore terms that have a document frequency strictly lower than the threshold.\n",
    "        use_idf=True,  # enable IDF weighting\n",
    "        smooth_idf=True,  # smooth IDF weights --> provides stability, reduces run time errors\n",
    "        sublinear_tf=True  # apply sublinear scaling to term frequencies\n",
    "    )\n",
    "\n",
    "    file_path = os.path.join(processed_folder_path, 'tfidf_vectorizer.joblib')\n",
    "    dump(tfidf_vectorizer, file_path)\n",
    "\n",
    "    return tfidf_vectorizer\n",
    "\n",
    "def transform_to_tfidf(x_train, x_val, x_test):\n",
    "    # Fit and transform the training set\n",
    "    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "    # Transform the validation and testing set\n",
    "    x_val_tfidf = tfidf_vectorizer.transform(x_val)\n",
    "    x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "    # Save data\n",
    "    save_to_npz(x_train_tfidf, os.path.join(processed_folder_path, \"train_tfidf.npz\"))\n",
    "    save_to_npz(x_val_tfidf, os.path.join(processed_folder_path, \"val_tfidf.npz\"))\n",
    "    save_to_npz(x_test_tfidf, os.path.join(processed_folder_path, \"test_tfidf.npz\"))\n",
    "\n",
    "    return x_train_tfidf, x_val_tfidf, x_test_tfidf\n",
    "\n",
    "\n",
    "max_features = 50000\n",
    "max_df = 0.95\n",
    "min_df = 5\n",
    "\n",
    "tfidf_vectorizer = get_tfidf_vectorizer(vocab, max_features, min_df, max_df)\n",
    "x_train_tfidf, x_val_tfidf, x_test_tfidf = transform_to_tfidf(x_train, x_val, x_test)\n",
    "%store tfidf_vectorizer\n",
    "\n",
    "print(\"\\nData Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_tfidf.shape, x_val_tfidf.shape, x_test_tfidf.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: delete\n",
    "def save_tfidf_data(data, filename, feature_names):\n",
    "    # Save the matrix with feature names as a DataFrame\n",
    "    data = pd.DataFrame(data.toarray(), columns=feature_names)\n",
    "    file_path = os.path.join(processed_folder_path, filename)\n",
    "    data.to_csv(file_path, sep=',', index=False) # TODO: if this isn't working, note that you added sep=','\n",
    "\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Save vectorized data\n",
    "#save_tfidf_data(x_train_tfidf, \"train_tfidf.csv\", feature_names)\n",
    "#save_tfidf_data(x_val_tfidf, \"val_tfidf.csv\", feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 561\n",
      "Stored 'max_seq_length' (int)\n"
     ]
    }
   ],
   "source": [
    "def find_max_seq_len(data):\n",
    "    # Find maximum sequence length\n",
    "    max_seq_length = max([len(line.split()) for line in data])\n",
    "    print(f'Maximum review length: {max_seq_length}')\n",
    "\n",
    "    return max_seq_length\n",
    "\n",
    "max_seq_length = find_max_seq_len(x_train)\n",
    "%store max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(data):\n",
    "    # Fit tokenizer (on training data)\n",
    "    tokenizer = Tokenizer()\n",
    "    # Remove default filters, including punctuation\n",
    "    tokenizer.filters = \"\"  \n",
    "    # Disable lowercase conversion\n",
    "    tokenizer.lower = False  \n",
    "    tokenizer.fit_on_texts(data) \n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = fit_tokenizer(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Shape (doc, vocab_size):\n",
      "* train: (41000, 561)\n",
      "* validation: (11541, 561)\n",
      "* test: (11927, 561)\n",
      "\n",
      "x_train_tfidf:\n",
      "[[ 293  835   77 ...    0    0    0]\n",
      " [  29  992  817 ...    0    0    0]\n",
      " [2321  227   33 ...    0    0    0]\n",
      " ...\n",
      " [ 203   88  482 ...    0    0    0]\n",
      " [ 120  102  165 ...    0    0    0]\n",
      " [ 120  370  501 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "def encode_text(lines, tokenizer, max_length, filename):\n",
    "    # Integer encode\n",
    "    encoded_seq = tokenizer.texts_to_sequences(lines)\n",
    "    # Pad the encoded sequences\n",
    "    padded = pad_sequences(encoded_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Save to np.array\n",
    "    save_to_npy(padded, os.path.join(processed_folder_path, filename))\n",
    "\n",
    "    return padded\n",
    "    \n",
    "\n",
    "# Encode Data\n",
    "x_train_encoded = encode_text(x_train, tokenizer, max_seq_length, \"train_encoded_x.npy\")\n",
    "x_val_encoded = encode_text(x_val, tokenizer, max_seq_length, \"val_encoded_x.npy\")\n",
    "x_test_encoded = encode_text(x_test, tokenizer, max_seq_length, \"test_encoded_x.npy\")\n",
    "\n",
    "print(\"\\nData Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_encoded.shape, x_val_encoded.shape, x_test_encoded.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y-encoded Data Shape:\n",
      "* train: (41000, 3)\n",
      "* validation: (11541, 3)\n",
      "* test: (11927, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: can't remember if this is used somewhere else, if not save data inside function\n",
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded\n",
    "\n",
    "# Convert sentiment labels to one-hot encoding\n",
    "y_train_encoded = one_hot_encode(y_train)\n",
    "y_val_encoded = one_hot_encode(y_val)\n",
    "y_test_encoded = one_hot_encode(y_test)\n",
    "\n",
    "\n",
    "save_to_npy(y_train_encoded, os.path.join(processed_folder_path, \"train_encoded_y.npy\"))\n",
    "save_to_npy(y_val_encoded, os.path.join(processed_folder_path, \"val_encoded_y.npy\"))\n",
    "save_to_npy(y_test_encoded, os.path.join(processed_folder_path, \"test_encoded_y.npy\"))\n",
    "   \n",
    "print(\"\\ny-encoded Data Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'embedding_vocab_size' (int)\n",
      "embedding_vocab_size:  24921\n"
     ]
    }
   ],
   "source": [
    "# TODO: may delete?\n",
    "# Total vocabulary size plus 0 for unknown words\n",
    "embedding_vocab_size = len(tokenizer.word_index) + 1\n",
    "%store embedding_vocab_size\n",
    "print(\"embedding_vocab_size: \", embedding_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in tokenizer but not in vocab:  0\n"
     ]
    }
   ],
   "source": [
    "# TODO: may delete?\n",
    "# Check if there are any words identified via the tokenizer that are not in vocab\n",
    "tokenizer_vocab = set(tokenizer.word_index.keys())\n",
    "vocab_set = set(vocab)\n",
    "tokenizer_only_words = tokenizer_vocab.difference(vocab_set)\n",
    "print(\"Words in tokenizer but not in vocab: \", len(tokenizer_only_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading w2v model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12614/24920 (50.62%) are not defined in the pretrained W2V model and will receive vectors with all 0.\n",
      "W2V Embedding Matrix shape: (24921, 100)\n",
      "Embedding Matrix:\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.13095    -0.17110001  0.21895    -0.53894001 -0.13356     0.21934   ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.33802     0.78784001  0.23646    -0.059737   -0.14753    -0.15067001]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "Stored 'w2v_embedding_vectors' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "def load_embedding():\n",
    "    w2v_pretrained_file_path = os.path.join(processed_folder_path, w2v_pretrained_model_filename)\n",
    "    if not os.path.exists(w2v_pretrained_file_path):\n",
    "        # Check if the pre-trained Word2Vec model is already downloaded. If not, download it.\n",
    "        print(\"\\nW2v model doesn't exist\")\n",
    "        model = api.load(w2v_pretrained_model)\n",
    "        model.save_word2vec_format(w2v_pretrained_file_path, binary=False)\n",
    "\n",
    "    # Load embedding into memory, skip first line\n",
    "    print(\"Loading w2v model...\")\n",
    "    file = open(w2v_pretrained_file_path, 'r', encoding='utf8')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "\n",
    "    # Create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # Set key as string word, value as numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def get_emb_matrix(loaded_embedding, tokenizer, embedding_dim):\n",
    "    # Create a weight matrix for the Embedding layer from a loaded/pretrained embedding\n",
    "\n",
    "    # Define weight matrix dimensions (vocab_size + 1 for unknown words) with all 0 \n",
    "    emb_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "\n",
    "    count_all_words = 0\n",
    "    count_na_words = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # Map loaded vectors to terms in vocab\n",
    "        if word in loaded_embedding.keys():\n",
    "            emb_matrix[i] = loaded_embedding.get(word)\n",
    "        else:\n",
    "            # Some terms such as emojis or neg-tagged words are not found in the loaded w2v model, hence they will have vectors with all 0\n",
    "            count_na_words += 1\n",
    "        count_all_words += 1\n",
    "    print(f'{count_na_words}/{count_all_words} ({((count_na_words/count_all_words)*100):.2f}%) are not defined in the pretrained W2V model and will receive vectors with all 0.')\n",
    "    print(f\"W2V Embedding Matrix shape: {emb_matrix.shape}\")\n",
    "    print(f\"Embedding Matrix:\\n{emb_matrix[:10, :6]}\")\n",
    "\n",
    "    # Save W2V model\n",
    "    # TODO: delete\n",
    "    file_path = os.path.join(processed_folder_path, \"embedding_matrix.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(' '.join(str(x) for x in row) for row in emb_matrix))\n",
    "\n",
    "    # TODO: keep\n",
    "    save_to_npy(emb_matrix, (os.path.join(processed_folder_path, \"embedding_matrix.npy\")))\n",
    "    \n",
    "    return emb_matrix\n",
    "\n",
    "pretrained_embedding = load_embedding()\n",
    "embedding_dim = 100\n",
    "w2v_embedding_vectors = get_emb_matrix(pretrained_embedding, tokenizer, embedding_dim)\n",
    "%store w2v_embedding_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
