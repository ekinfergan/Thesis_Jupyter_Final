{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekinfergan/Thesis_Jupyter_Final/blob/main/src/models/rnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9A-Xt2ne4djh"
      },
      "source": [
        "# Deep Learning - LSTM & GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks2ckCio4nE6",
        "outputId": "1cbfbea5-38a3-4cbe-c25a-044df8ce2993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Thesis_Jupyter_Final'...\n",
            "remote: Enumerating objects: 1059, done.\u001b[K\n",
            "remote: Counting objects: 100% (479/479), done.\u001b[K\n",
            "remote: Compressing objects: 100% (307/307), done.\u001b[K\n",
            "remote: Total 1059 (delta 229), reused 404 (delta 168), pack-reused 580\u001b[K\n",
            "Receiving objects: 100% (1059/1059), 192.14 MiB | 28.48 MiB/s, done.\n",
            "Resolving deltas: 100% (547/547), done.\n",
            "Updating files: 100% (92/92), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ekinfergan/Thesis_Jupyter_Final.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm57pwr44ffU",
        "outputId": "e55728f1-b9a8-4a76-c405-fa34bac80cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Thesis_Jupyter_Final\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd Thesis_Jupyter_Final\n",
        "!git pull\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZq9XZCv4yYu",
        "outputId": "9e9ac813-32df-4fd0-89b7-a0801ff6dd2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/scikit-optimize/scikit-optimize.git\n",
            "  Cloning https://github.com/scikit-optimize/scikit-optimize.git to /tmp/pip-req-build-n75sf6n0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/scikit-optimize/scikit-optimize.git /tmp/pip-req-build-n75sf6n0\n",
            "  Resolved https://github.com/scikit-optimize/scikit-optimize.git to commit a2369ddbc332d16d8ff173b12404b03fea472492\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.9.0) (1.2.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize==0.9.0)\n",
            "  Using cached pyaml-23.5.9-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.9.0) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.9.0) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.9.0) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize==0.9.0) (6.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize==0.9.0) (3.1.0)\n",
            "Building wheels for collected packages: scikit-optimize\n",
            "  Building wheel for scikit-optimize (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-optimize: filename=scikit_optimize-0.9.0-py2.py3-none-any.whl size=100250 sha256=465514bcaa6f2f714c562abc8431175e093d98e5cf3cb2e06f582962f8149cef\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o6qt2c41/wheels/2f/f0/ed/db529a96372d05bd34f6c3a2fa7c08ef7a8314315ac46e49d7\n",
            "Successfully built scikit-optimize\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-23.5.9 scikit-optimize-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/scikit-optimize/scikit-optimize.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PbAXzVr-4djj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-21 20:39:49.836797: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing\n",
        "import pickle\n",
        "from numpy import asarray\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, RocCurveDisplay, confusion_matrix, classification_report\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from itertools import cycle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, concatenate, Dense, LSTM, GRU\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers.legacy import Adam, SGD, RMSprop, Adagrad\n",
        "\n",
        "import skopt\n",
        "from skopt import gbrt_minimize, gp_minimize\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EynHCpQ04djk",
        "outputId": "97e307a2-245f-43be-d6cb-ee667a925491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home2/s3985113/Thesis_Jupyter_Final/src/\n"
          ]
        }
      ],
      "source": [
        "# Set paths\n",
        "script_dir = os.path.dirname(os.path.abspath('rnns.ipynb'))\n",
        "data_path = os.path.join(script_dir, 'Thesis_Jupyter_Final/src/')\n",
        "os.getcwd()\n",
        "print(data_path)\n",
        "\n",
        "input_folder_path = os.path.join(data_path, 'input')\n",
        "processed_folder_path = os.path.join(data_path, 'input/processed/normal')\n",
        "results_folder_path =  os.path.join(data_path, \"results\")\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(results_folder_path):\n",
        "    os.makedirs(results_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FVinrA7E6ISQ"
      },
      "outputs": [],
      "source": [
        "# Set other constants and variables\n",
        "senti_labels_dict = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
        "senti_labels_names = list(senti_labels_dict.values())\n",
        "senti_labels_nums = list(senti_labels_dict.keys())\n",
        "NUM_of_CLASSES = 3\n",
        "\n",
        "VOCAB_SIZE = 11395\n",
        "MAX_SEQ_LEN = 449\n",
        "EMBEDDING_DIM = 100\n",
        "NUM_OUTPUT_CLASSES = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOIpxE654djm",
        "outputId": "e1822def-c6a7-4b1e-b988-148cb874d30f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train_encoded:\n",
            "[[  96  549  929 ...    0    0    0]\n",
            " [ 453  240 1125 ...    0    0    0]\n",
            " [1260   67  312 ...    0    0    0]\n",
            " [ 127 1352 6694 ...    0    0    0]\n",
            " [ 529   10   69 ...    0    0    0]]\n",
            "\n",
            "embedding vectors: [-0.57674998 -0.42304999  0.27188    -0.31986001  0.18842   ]...\n",
            "\n",
            "y_train_encoded:\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv(os.path.join(processed_folder_path, \"train.csv\"))\n",
        "val = pd.read_csv(os.path.join(processed_folder_path, \"val.csv\"))\n",
        "test = pd.read_csv(os.path.join(processed_folder_path, \"test.csv\"))\n",
        "\n",
        "x_train = train['x']\n",
        "y_train = train['y']\n",
        "x_val = val['x']\n",
        "y_val = val['y']\n",
        "x_test = test['x']\n",
        "y_test = test['y']\n",
        "\n",
        "# Load encoded sequences\n",
        "with open(os.path.join(processed_folder_path, \"x_train_encoded.pkl\"), \"rb\") as f:\n",
        "    x_train_encoded = pickle.load(f)\n",
        "with open(os.path.join(processed_folder_path, \"x_val_encoded.pkl\"), \"rb\") as f:\n",
        "    x_val_encoded = pickle.load(f)\n",
        "with open(os.path.join(processed_folder_path, \"x_test_encoded.pkl\"), \"rb\") as f:\n",
        "    x_test_encoded = pickle.load(f)\n",
        "print(f\"x_train_encoded:\\n{x_train_encoded[:5]}\\n\")\n",
        "\n",
        "# Load embedding vectors\n",
        "with open(os.path.join(processed_folder_path, \"embedding_matrix.pkl\"), \"rb\") as f:\n",
        "    w2v_embedding_vectors = pickle.load(f)\n",
        "print(f\"embedding vectors: {w2v_embedding_vectors[10][:5]}...\\n\")\n",
        "\n",
        "# Load encoded labels\n",
        "with open(os.path.join(processed_folder_path, \"y_train_encoded.pkl\"), \"rb\") as f:\n",
        "    y_train_encoded = pickle.load(f)\n",
        "with open(os.path.join(processed_folder_path, \"y_val_encoded.pkl\"), \"rb\") as f:\n",
        "    y_val_encoded = pickle.load(f)\n",
        "with open(os.path.join(processed_folder_path, \"y_test_encoded.pkl\"), \"rb\") as f:\n",
        "    y_test_encoded = pickle.load(f)\n",
        "print(f\"y_train_encoded:\\n{y_train_encoded[:5]}\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rsJ4d-mY4djn"
      },
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7hXUlQ8t4djn"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(y):\n",
        "    \"\"\"\n",
        "    One-hot encode the labels.\n",
        "\n",
        "    Args:\n",
        "        y (array): Labels to be encoded.\n",
        "\n",
        "    Returns:\n",
        "        array: One-hot encoded labels.\n",
        "    \"\"\"\n",
        "\n",
        "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
        "    for i, label in enumerate(y):\n",
        "        y_encoded[i, label - 1] = 1\n",
        "\n",
        "    return y_encoded\n",
        "\n",
        "\n",
        "def calculate_classification_report(y, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the classification report.\n",
        "\n",
        "    Args:\n",
        "        y (array): True labels.\n",
        "        y_pred (array): Predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        str: Classification report.\n",
        "    \"\"\"\n",
        "\n",
        "    report = classification_report(y, y_pred, labels=senti_labels_nums, target_names=senti_labels_names)\n",
        "    return report\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(model_name, y_true, y_pred, res_path):\n",
        "    \"\"\"\n",
        "    Plot the confusion matrix.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model.\n",
        "        y_true (array): True labels.\n",
        "        y_pred (array): Predicted labels.\n",
        "        res_path (str): Path to save the plot.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
        "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=senti_labels_names)\n",
        "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
        "    plt.title(f'Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(res_path, f\"{model_name}_confusion_matrix.png\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def get_results(y_pred, y, x, score, model, model_name, params, res_path, only_metrics=True):\n",
        "    \"\"\"\n",
        "    Generate and print the results of a provided model.\n",
        "\n",
        "    Args:\n",
        "        y_pred (array): Predicted labels.\n",
        "        y (array): True labels.\n",
        "        x: Data.\n",
        "        score: Model evaluation score (e.g., loss and accuracy).\n",
        "        model: The trained model.\n",
        "        model_name (str): The name of the model.\n",
        "        params: Optimizer parameter.\n",
        "        res_path (str): Path to store the results.\n",
        "        only_metrics (bool, optional): Whether to only print metrics or include additional visualizations. Defaults is True.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(res_path):\n",
        "        os.makedirs(res_path)\n",
        "\n",
        "    # Convert to one hot vectors\n",
        "    y_classes = np.argmax(y, axis=1) + 1\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1) + 1\n",
        "\n",
        "    print(y_pred.shape)\n",
        "    print(y_classes.shape)\n",
        "    print(y_pred_classes.shape)\n",
        "\n",
        "    print(f\"Accuracy: {score[1]:.2%}\")\n",
        "    print(f\"Loss: {score[0]:.2f}\")\n",
        "\n",
        "    with open(os.path.join(res_path, f\"{model_name}_results.txt\"), \"w\") as f:\n",
        "        f.write(f\"*{model_name}\\n\")\n",
        "        f.write(f\"Optimizer Params: {params}\\n\\n\")\n",
        "\n",
        "        f.write(f\"Accuracy: {score[1]:.2f}%\\n\")\n",
        "        f.write(f\"Loss: {score[0]:.2f}\\n\")\n",
        "\n",
        "        report = calculate_classification_report(y_classes, y_pred_classes)\n",
        "        if report is not None:\n",
        "            f.write(\"Classification Report:\\n\")\n",
        "            f.write(report)\n",
        "        else:\n",
        "            print(\"Failed to generate classification report\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        if not only_metrics:\n",
        "            plot_confusion_matrix(model_name, y_classes, y_pred_classes, res_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BE2Q4ObwwEpo"
      },
      "outputs": [],
      "source": [
        "def plot_development(history, model_name, res_path):\n",
        "    \"\"\"\n",
        "    Plot the training and validation accuracy and loss curves.\n",
        "\n",
        "    Args:\n",
        "        history (History): The training history object containing accuracy and loss values.\n",
        "        model_name (str): The name of the model being trained.\n",
        "        res_path (str): The path to save the development plot.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    acc =  history['accuracy']\n",
        "    val_acc = history['val_accuracy']\n",
        "\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "    plt.title(f\"{model_name} Training and Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(res_path, f\"{model_name}_accuracy_plot.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "    plt.title(f\"{model_name} Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(res_path, f\"{model_name}_loss_plot.png\"))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The provided code for the the implementation of the Gradient Boosted Regression Trees used as a surrogate model within Bayesian Optimization was inspired from the article titled \"Parameter Hyperparameter Tuning with Bayesian Optimization\" by crawftv on Medium. The implementation is available at the following link: [Gradient Boost Model](https://medium.com/@crawftv/parameter-hyperparameter-tuning-with-bayesian-optimization-7acf42d348e1). And the model architecture takes inspiration from the implementation provided in the Kaggle notebook titled \"Twitter Sentiment Analysis with Word2Vec LSTM\", which is available at the link: [Word2Vec - LSTM](https://www.kaggle.com/code/caiyutiansg/twitter-sentiment-analysis-with-word2vec-lstm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uryP_5tgwEpq"
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "\n",
        "batch_size= 16\n",
        "epochs=1\n",
        "\n",
        "\n",
        "num_units = Categorical([32, 64, 128], name='num_units')\n",
        "learning_rate = Categorical([0.1, 1e-2], name='learning_rate')\n",
        "\n",
        "search_space = [\n",
        "            learning_rate\n",
        "            ]\n",
        "\n",
        "# Set initial points for the search of optimal parameter\n",
        "default_params = [\n",
        "                  0.1\n",
        "                  ]\n",
        "\n",
        "def define_model(learning_rate, layer_type):\n",
        "    \"\"\"\n",
        "    Define and compile the model with the given hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        layer_type (class): Type of recurrent layer (LSTM or GRU).\n",
        "\n",
        "    Returns:\n",
        "        model (Model): Compiled Keras model.\n",
        "    \"\"\"\n",
        "\n",
        "    print(layer_type)\n",
        "    # Set the input layer\n",
        "    inputs = Input(shape=(MAX_SEQ_LEN,), name=\"input glove embeddings\")\n",
        "\n",
        "    # Set an embedding layer for the input\n",
        "    embeddings = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_SEQ_LEN, weights=[w2v_embedding_vectors], trainable=False, name=\"embeddings\")(inputs)\n",
        "\n",
        "    # Pass embeddings through their own LSTM or GRU layers\n",
        "    layers = layer_type(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, name=\"layer_1\")(embeddings)\n",
        "    layers = layer_type(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=False, name=\"layer_2\")(layers)\n",
        "\n",
        "    # Dense layer for the merged inputs & output layer\n",
        "    outputs = Dense(NUM_OUTPUT_CLASSES, activation='softmax', name=\"output\")(layers)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    rmsprop = RMSprop(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=rmsprop, loss='mean_squared_error', metrics=['accuracy']) # default learning rate = 0.001\n",
        "    print(model.summary())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VH3zXV_N4djo"
      },
      "outputs": [],
      "source": [
        "def get_objective_function(layer_type):\n",
        "    \"\"\"\n",
        "    Define the objective function for Bayesian Optimization.\n",
        "\n",
        "    Args:\n",
        "        layer_type (class): Type of recurrent layer (LSTM or GRU).\n",
        "\n",
        "    Returns:\n",
        "        objective_function (function): Objective function for Bayesian Optimization.\n",
        "    \"\"\"\n",
        "    \n",
        "    @use_named_args(dimensions=search_space)\n",
        "    def objective_function(learning_rate):\n",
        "        model = define_model(learning_rate=learning_rate,\n",
        "                            layer_type=layer_type\n",
        "                            )\n",
        "\n",
        "        print(\"Optimization, starting training...\")\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "        history = model.fit(x_train_encoded,\n",
        "                            y_train_encoded,\n",
        "                            validation_data=(x_val_encoded, y_val_encoded),\n",
        "                            epochs=epochs,\n",
        "                            batch_size=batch_size,\n",
        "                            callbacks=[early_stopping],\n",
        "                            verbose=2\n",
        "                            )\n",
        "        print(\"Training Complete\")\n",
        "        # Return the validation accuracy for the last epoch\n",
        "        accuracy = history.history['val_accuracy'][-1]\n",
        "        loss = history.history['val_loss'][-1]\n",
        "\n",
        "        # Print the classification accuracy\n",
        "        print(f\"Accuracy: {accuracy:.2%}\")\n",
        "        print(f\"Loss: {loss:.2}\\n\")\n",
        "\n",
        "        del model,\n",
        "\n",
        "        print('Model deleted. Clearing session...')\n",
        "\n",
        "        # Session clearing\n",
        "        K.clear_session()\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "\n",
        "        # Return negative accuracy, since it is the lowest score\n",
        "        return -accuracy\n",
        "\n",
        "    return objective_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pBlA5cDzwEpr"
      },
      "outputs": [],
      "source": [
        "def perform_bayesian_opt(objective_function):\n",
        "    \"\"\"\n",
        "    Perform Bayesian Optimization to find the best hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        objective_function: The objective function to optimize.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the best hyperparameters found.\n",
        "\n",
        "    \"\"\"\n",
        "    gbrt_result = gbrt_minimize(func=objective_function,\n",
        "                                dimensions=search_space,\n",
        "                                n_calls=12,\n",
        "                                n_jobs=-1,\n",
        "                                x0=default_params)\n",
        "\n",
        "    gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
        "    print(\"Best Hyperparameters:\", gbrt_best_params)\n",
        "    print()\n",
        "\n",
        "    return gbrt_best_params\n",
        "\n",
        "\n",
        "def fit_model(model, model_name, x_train, y_train, x_val, y_val, res_path):\n",
        "    \"\"\"\n",
        "    Fit a model to the training data.\n",
        "\n",
        "    Args:\n",
        "        model (object): The model to fit.\n",
        "        model_name (str): The name of the model.\n",
        "        x_train (array): The training data.\n",
        "        y_train (array): The training labels.\n",
        "        x_val (array): The validation data.\n",
        "        y_val (array): The validation labels.\n",
        "        res_path (str): The path to save the model and results.\n",
        "\n",
        "    Returns:\n",
        "        object: The fitted model.\n",
        "        object: The training history.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model_file = f\"model_{model_name}.h5\"\n",
        "    history_file = f\"history_{model_name}.pkl\"\n",
        "    model_path = os.path.join(res_path, model_file)\n",
        "    history_path = os.path.join(res_path, history_file) \n",
        "\n",
        "    # Check if the model exists\n",
        "    if os.path.exists(model_path):\n",
        "        # If the model exists, then load it\n",
        "        model = load_model(model_path)\n",
        "        print(f\"Model {model_name} loaded successfully...\")\n",
        "        # Load the training history\n",
        "        with open(history_path, 'rb') as file:\n",
        "            history_saved = pickle.load(file)\n",
        "    else:\n",
        "        print(\"Fitting best model...\")\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "        history = model.fit(x_train,\n",
        "                            y_train,\n",
        "                            validation_data=(x_val, y_val),\n",
        "                            epochs=2,\n",
        "                            batch_size=batch_size,\n",
        "                            callbacks=[early_stopping],\n",
        "                            verbose=2)\n",
        "\n",
        "        model.save(model_path)\n",
        "        print(f\"Model {model_name} saved at {model_path}\")\n",
        "\n",
        "        # Save the training history\n",
        "        history_saved = history.history\n",
        "        with open(history_path, 'wb') as file:\n",
        "            pickle.dump(history_saved, file)\n",
        "\n",
        "    return model, history_saved\n",
        "\n",
        "\n",
        "def init_results(model, model_name, x_train, y_train, x_val, y_val, x_test, y_test, res_path):\n",
        "    \"\"\"\n",
        "    Initialize and save the results for the best model.\n",
        "\n",
        "    Args:\n",
        "        model (object): The best model.\n",
        "        model_name (str): The name of the model.\n",
        "        x_train (array): The training data.\n",
        "        y_train (array): The training labels.\n",
        "        x_val (array): The validation data.\n",
        "        y_val (array): The validation labels.\n",
        "        x_test (array): The test data.\n",
        "        y_test (array): The test labels.\n",
        "        res_path (str): The path to save the results.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Saving best results...\")\n",
        "\n",
        "    y_pred = model.predict(x_train)\n",
        "    score = model.evaluate(x_train, y_train)\n",
        "    get_results(y_pred, y_train, x_train, score, model, f\"Train-{model_name}\", model.optimizer.get_config(), res_path)\n",
        "\n",
        "    y_pred = model.predict(x_val)\n",
        "    score = model.evaluate(x_val, y_val)\n",
        "    get_results(y_pred, y_val, x_val, score, model, f\"Val-{model_name}\", model.optimizer.get_config(), res_path)\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "    score = model.evaluate(x_test, y_test)\n",
        "    get_results(y_pred, y_test, x_test, score, model, f\"Test-{model_name}\", model.optimizer.get_config(), res_path, only_metrics=False)\n",
        "\n",
        "\n",
        "def setup_dl(model_name):\n",
        "    \"\"\"\n",
        "    Setup the Deep Learning settings for the given model name.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the model.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    \"\"\"\n",
        "    '''\n",
        "    if model_name == \"LSTM\":\n",
        "        objective_function = get_objective_function(LSTM)\n",
        "    else:\n",
        "        objective_function = get_objective_function(GRU)\n",
        "\n",
        "    best_params = perform_bayesian_opt(objective_function)\n",
        "    '''\n",
        "    \n",
        "    subfolder_path = f\"{model_name}_results\"\n",
        "    res_path = os.path.join(results_folder_path, subfolder_path)\n",
        "\n",
        "    # Fit best model\n",
        "    model = define_model(0.01,  #best_params['learning_rate'], \n",
        "                         LSTM if model_name == \"LSTM\" else GRU)\n",
        "\n",
        "    best_model, history = fit_model(model, model_name, x_train_encoded, y_train_encoded, x_val_encoded, y_val_encoded, res_path)\n",
        "    plot_development(history, model_name, res_path)\n",
        "\n",
        "    # Get results\n",
        "    init_results(best_model, model_name, x_train_encoded, y_train_encoded, x_val_encoded, y_val_encoded, x_test_encoded, y_test_encoded, res_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WdwlQpA1wEps",
        "outputId": "9d461312-3df5-41ca-b744-66745d31ed5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'keras.layers.rnn.lstm.LSTM'>\n",
            "WARNING:tensorflow:Layer layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input glove embeddings (Inp  [(None, 449)]            0         \n",
            " utLayer)                                                        \n",
            "                                                                 \n",
            " embeddings (Embedding)      (None, 449, 100)          1139500   \n",
            "                                                                 \n",
            " layer_1 (LSTM)              (None, 449, 64)           42240     \n",
            "                                                                 \n",
            " layer_2 (LSTM)              (None, 32)                12416     \n",
            "                                                                 \n",
            " output (Dense)              (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,194,255\n",
            "Trainable params: 54,755\n",
            "Non-trainable params: 1,139,500\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:Layer layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model LSTM loaded successfully...\n",
            "Saving best results...\n",
            "1282/1282 [==============================] - 247s 192ms/step\n",
            "1282/1282 [==============================] - 252s 196ms/step - loss: 0.2104 - accuracy: 0.4878\n",
            "(41000, 3)\n",
            "(41000,)\n",
            "(41000,)\n",
            "Accuracy: 48.78%\n",
            "Loss: 0.21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home2/s3985113/venv/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home2/s3985113/venv/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home2/s3985113/venv/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "361/361 [==============================] - 70s 195ms/step\n",
            "361/361 [==============================] - 71s 196ms/step - loss: 0.1505 - accuracy: 0.8674\n",
            "(11529, 3)\n",
            "(11529,)\n",
            "(11529,)\n",
            "Accuracy: 86.74%\n",
            "Loss: 0.15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home2/s3985113/venv/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home2/s3985113/venv/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home2/s3985113/venv/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "372/372 [==============================] - 72s 193ms/step\n",
            "372/372 [==============================] - 72s 194ms/step - loss: 0.1547 - accuracy: 0.8404\n",
            "(11899, 3)\n",
            "(11899,)\n",
            "(11899,)\n",
            "Accuracy: 84.04%\n",
            "Loss: 0.15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home2/s3985113/venv/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home2/s3985113/venv/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home2/s3985113/venv/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "setup_dl(\"LSTM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'keras.layers.rnn.gru.GRU'>\n",
            "WARNING:tensorflow:Layer layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-26 01:53:41.093017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38306 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:17:00.0, compute capability: 8.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input glove embeddings (Inp  [(None, 449)]            0         \n",
            " utLayer)                                                        \n",
            "                                                                 \n",
            " embeddings (Embedding)      (None, 449, 100)          1139500   \n",
            "                                                                 \n",
            " layer_1 (GRU)               (None, 449, 64)           31872     \n",
            "                                                                 \n",
            " layer_2 (GRU)               (None, 32)                9408      \n",
            "                                                                 \n",
            " output (Dense)              (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,180,879\n",
            "Trainable params: 41,379\n",
            "Non-trainable params: 1,139,500\n",
            "_________________________________________________________________\n",
            "None\n",
            "Fitting best model...\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-26 01:53:47.328547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2563/2563 - 4372s - loss: 0.2119 - accuracy: 0.4844 - val_loss: 0.1164 - val_accuracy: 0.8674 - 4372s/epoch - 2s/step\n",
            "Epoch 2/2\n",
            "2563/2563 - 5763s - loss: 0.2052 - accuracy: 0.4971 - val_loss: 0.1088 - val_accuracy: 0.8447 - 5763s/epoch - 2s/step\n",
            "Model GRU saved at /home2/s3985113/Thesis_Jupyter_Final/src/results/GRU_results/model_GRU.h5\n",
            "Saving best results...\n",
            "1282/1282 [==============================] - 280s 218ms/step\n",
            "1282/1282 [==============================] - 282s 220ms/step - loss: 0.2165 - accuracy: 0.4969\n",
            "(41000, 3)\n",
            "(41000,)\n",
            "(41000,)\n",
            "Accuracy: 49.69%\n",
            "Loss: 0.22\n",
            "361/361 [==============================] - 78s 217ms/step\n",
            "361/361 [==============================] - 80s 220ms/step - loss: 0.1088 - accuracy: 0.8449\n",
            "(11529, 3)\n",
            "(11529,)\n",
            "(11529,)\n",
            "Accuracy: 84.49%\n",
            "Loss: 0.11\n",
            "372/372 [==============================] - 80s 215ms/step\n",
            "372/372 [==============================] - 82s 220ms/step - loss: 0.1161 - accuracy: 0.8231\n",
            "(11899, 3)\n",
            "(11899,)\n",
            "(11899,)\n",
            "Accuracy: 82.31%\n",
            "Loss: 0.12\n",
            "372/372 [==============================] - 80s 216ms/step\n"
          ]
        }
      ],
      "source": [
        "setup_dl(\"GRU\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
