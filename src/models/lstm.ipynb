{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from numpy import asarray\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, RocCurveDisplay, confusion_matrix, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.legacy import Adam, SGD, RMSprop, Adagrad\n",
    "\n",
    "import skopt\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer  \n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "# DATASET\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "# Define a dictionary to map sentiment values to category names\n",
    "senti_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_categories = list(senti_labels.values())\n",
    "NUM_of_CLASSES = 3\n",
    "\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/\"\n",
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/src/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.072519    0.43042001  0.29549    ... -0.33405     0.23035\n",
      "  -0.29227999]\n",
      " ...\n",
      " [ 0.031817    0.56676    -0.25497001 ... -0.085164    0.24883001\n",
      "  -0.76543999]\n",
      " [ 0.0099032  -0.096089   -0.67510998 ... -0.20811    -0.44536999\n",
      "   0.020689  ]\n",
      " [-0.87027001  0.30937999  0.36063999 ... -0.31865001 -0.32418999\n",
      "  -0.0088682 ]]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(input_folder_path, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(input_folder_path, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(input_folder_path, \"test.csv\"))\n",
    "\n",
    "x_train = train['x']\n",
    "y_train = train['y']\n",
    "x_val = val['x']\n",
    "y_val = val['y']\n",
    "x_test = test['x']\n",
    "y_test = test['y']\n",
    "\n",
    "x_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_x.npy\"))\n",
    "y_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_y.npy\"))\n",
    "x_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_x.npy\"))\n",
    "y_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_y.npy\"))\n",
    "x_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_x.npy\"))\n",
    "y_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_y.npy\"))\n",
    "\n",
    "w2v_embedding_vectors = np.load(os.path.join(processed_folder_path, \"embedding_w2v_matrix.npy\"))\n",
    "print(w2v_embedding_vectors)\n",
    "\n",
    "%store -r embedding_vocab_size\n",
    "%store -r EMBEDDING_DIM\n",
    "%store -r max_seq_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(score):\n",
    "    acc =  score[1]\n",
    "    loss = score[0]\n",
    "\n",
    "    print(f\"Accuracy: {acc:.2%}\")\n",
    "    print(f\"Loss: {loss:.2f}\")\n",
    "    \n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_report(y, y_pred, labels):\n",
    "    report = classification_report(y, y_pred, labels=labels)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
    "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=labels)\n",
    "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, x_encoded, y_encoded, y=None, only_metrics=True):    \n",
    "    y_pred_prob = model.predict(x_encoded)\n",
    "\n",
    "    print(f\"*{model_name}\")\n",
    "    \n",
    "    score = model.evaluate(x_encoded, y_encoded, verbose=0)\n",
    "    calculate_metrics(score)\n",
    "    \n",
    "    senti_labels = ['negative', 'neutral', 'positive'] #TODO: to constants\n",
    "    \n",
    "    if not only_metrics:\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1) + 1\n",
    "        calculate_classification_report(y, y_pred, labels=senti_labels)\n",
    "        plot_confusion_matrix(y, y_pred, labels=senti_labels)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(prob_test_vec, y_test, labels):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels\n",
    "    colors = cycle(['limegreen', 'dodgerblue', 'red'])\n",
    "    for senti, color in zip(range(NUM_of_CLASSES), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_test[:, senti],\n",
    "            prob_test_vec[:, senti],\n",
    "            name=f\"ROC curve for {labels[senti]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_OvR_roc_auc_score(model, x, y, x_test, y_test, labels): #average??\n",
    "    #y = one_hot_encode(y)\n",
    "    #y_test = one_hot_encode(y_test)\n",
    "\n",
    "    ovr_model = OneVsRestClassifier(model).fit(x, y)\n",
    "    prob_test_vec = ovr_model.predict_proba(x_test)\n",
    "    \n",
    "    fpr, tpr, thresholds, auc_score = [], [], [], []\n",
    "    for _ in range(NUM_of_CLASSES):\n",
    "        fpr.append(0)\n",
    "        tpr.append(0)\n",
    "        thresholds.append(0)\n",
    "        auc_score.append(0)\n",
    "    \n",
    "    for i in range(NUM_of_CLASSES):\n",
    "        fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], prob_test_vec[:, i])\n",
    "        auc_score[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    print(f\"AUC score: {auc_score}\")\n",
    "    averaged_auc_score = (sum(auc_score) / NUM_of_CLASSES)\n",
    "    print(f\"Averaged AUC score: {averaged_auc_score:.2f}\")\n",
    "    \n",
    "    plot_roc_curve(prob_test_vec, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_development(history):\n",
    "    acc =  history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "    \n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a basic neural net to see the baseline for accuracy with minimum tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_classes = 3\n",
    "batch_size= 64\n",
    "epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_layer (Embedding)  (None, 294, 100)         72400     \n",
      "                                                                 \n",
      " hidden_layer (LSTM)         (None, 64)                42240     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114,835\n",
      "Trainable params: 114,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 69s 71ms/step - loss: 0.8712 - accuracy: 0.6662 - val_loss: 0.5230 - val_accuracy: 0.9031\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 83s 88ms/step - loss: 0.8684 - accuracy: 0.6667 - val_loss: 0.5354 - val_accuracy: 0.9031\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 83s 88ms/step - loss: 0.8682 - accuracy: 0.6667 - val_loss: 0.5517 - val_accuracy: 0.9031\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 83s 89ms/step - loss: 0.8680 - accuracy: 0.6667 - val_loss: 0.5512 - val_accuracy: 0.9031\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 83s 89ms/step - loss: 0.8680 - accuracy: 0.6667 - val_loss: 0.5298 - val_accuracy: 0.9031\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 83s 89ms/step - loss: 0.8679 - accuracy: 0.6667 - val_loss: 0.5457 - val_accuracy: 0.9031\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 84s 89ms/step - loss: 0.8680 - accuracy: 0.6667 - val_loss: 0.5385 - val_accuracy: 0.9031\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 84s 89ms/step - loss: 0.8678 - accuracy: 0.6667 - val_loss: 0.5348 - val_accuracy: 0.9031\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 84s 89ms/step - loss: 0.8679 - accuracy: 0.6667 - val_loss: 0.5392 - val_accuracy: 0.9031\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 84s 89ms/step - loss: 0.8679 - accuracy: 0.6667 - val_loss: 0.5407 - val_accuracy: 0.9031\n",
      "640/640 [==============================] - 14s 21ms/step - loss: 0.5329 - accuracy: 0.9088\n",
      "Naive model Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# input layer is sequence of integers (words)\n",
    "model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, name=\"embedding_layer\")) # part of input layer as it transforms integers into dense vectors, input shape = (None, max_seq_length)\n",
    "model.add(LSTM(64, name='hidden_layer')) # hidden layer\n",
    "model.add(Dense(num_output_classes, activation='softmax', name=\"output_layer\"))\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "basic_history = model.fit(x_train_encoded, y_train_encoded, validation_data=(x_val_encoded, y_val_encoded), batch_size=batch_size, epochs=10)\n",
    "\n",
    "accuracy = model.evaluate(x_test_encoded, y_test_encoded)[1]\n",
    "print(f\"Naive model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "del model\n",
    "\n",
    "backend.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our naive model, we get an accuracy of x%. # TODO: x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm_layers = Integer(low=1, high=5, name='num_lstm_layers')\n",
    "num_lstm_units = Integer(low=32, high=256, name='num_lstm_units') # TODO: step Keras Tuner\n",
    "learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "adam_decay = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n",
    "#batch_size = Integer(low=1, high=128, name='batch_size')\n",
    "\n",
    "search_space = [\n",
    "            num_lstm_layers,\n",
    "            num_lstm_units,\n",
    "            learning_rate,\n",
    "            adam_decay\n",
    "            ]\n",
    "\n",
    "# Specify one or more initial points for the search of optimal parameter\n",
    "default_params = [1,\n",
    "                  32,\n",
    "                  1e-3, \n",
    "                  1e-3,\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_lstm_model(num_lstm_layers, num_lstm_units, learning_rate, adam_decay):\n",
    "    # Start the model making process and create our first layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length))\n",
    "\n",
    "    # Create a loop making a new LSTM layer for the amount passed to this model.\n",
    "    # Naming the layers helps avoid tensorflow error deep in the stack trace.\n",
    "    for i in range(num_lstm_layers):\n",
    "        name = 'layer_lstm_{0}'.format(i+1)\n",
    "        if i < num_lstm_layers-1:\n",
    "            model.add(LSTM(num_lstm_units, return_sequences=True, name=name))\n",
    "        else:\n",
    "            model.add(LSTM(num_lstm_units, return_sequences=False, name=name))\n",
    "\n",
    "    # Add our classification layer.\n",
    "    model.add(Dense(num_output_classes, activation='softmax'))\n",
    "\n",
    "    # Setup our optimizer and compile\n",
    "    adam = Adam(learning_rate=learning_rate, decay=adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=search_space)\n",
    "def objective_function(num_lstm_layers, num_lstm_units, learning_rate, adam_decay):\n",
    "\n",
    "    model = define_lstm_model(num_lstm_layers=num_lstm_layers,\n",
    "                         num_lstm_units=num_lstm_units,\n",
    "                         learning_rate=learning_rate,\n",
    "                         adam_decay=adam_decay\n",
    "                         )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    loss = history.history['val_loss'][-1]\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Loss: {loss:.2}\\n\")\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 294, 100)          72400     \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 32)                17024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,523\n",
      "Trainable params: 89,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "938/938 [==============================] - 68s 70ms/step - loss: 0.8708 - accuracy: 0.6662 - val_loss: 0.5565 - val_accuracy: 0.9031\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 66s 71ms/step - loss: 0.8683 - accuracy: 0.6667 - val_loss: 0.5335 - val_accuracy: 0.9031\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 67s 71ms/step - loss: 0.8680 - accuracy: 0.6667 - val_loss: 0.5468 - val_accuracy: 0.9031\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 66s 71ms/step - loss: 0.8679 - accuracy: 0.6667 - val_loss: 0.5361 - val_accuracy: 0.9031\n",
      "Epoch 5/30\n",
      "820/938 [=========================>....] - ETA: 7s - loss: 0.8683 - accuracy: 0.6663"
     ]
    }
   ],
   "source": [
    "gp_result = gp_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gp_best_params = {param.name: value for param, value in zip(gp_result.space, gp_result.x)}\n",
    "print(\"Best Hyperparameters:\", gp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_lstm_model(gp_best_params['num_lstm_layers'], \n",
    "                          gp_best_params['num_lstm_units'], \n",
    "                          gp_best_params['learning_rate'], \n",
    "                          gp_best_params['adam_decay']\n",
    "                          )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                    y_train_encoded,\n",
    "                    validation_data=(x_val_encoded, y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train single-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val single-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test single-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_result = gbrt_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
    "print(\"Best Hyperparameters:\", gbrt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_lstm_model(gbrt_best_params['num_lstm_layers'], \n",
    "                          gbrt_best_params['num_lstm_units'], \n",
    "                          gbrt_best_params['learning_rate'], \n",
    "                          gbrt_best_params['adam_decay']\n",
    "                          )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                    y_train_encoded,\n",
    "                    validation_data=(x_val_encoded, y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train single-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val single-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test single-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm_layersA = Integer(low=1, high=5, name='num_lstm_layersA')\n",
    "num_lstm_layersB = Integer(low=1, high=5, name='num_lstm_layersB')\n",
    "num_lstm_unitsA = Integer(low=32, high=256, name='num_lstm_unitsA')\n",
    "num_lstm_unitsB = Integer(low=32, high=256, name='num_lstm_unitsB')\n",
    "\n",
    "search_space = [\n",
    "            num_lstm_layersA,\n",
    "            num_lstm_layersB,\n",
    "            num_lstm_unitsA,\n",
    "            num_lstm_unitsB,\n",
    "            learning_rate,\n",
    "            adam_decay\n",
    "            ]\n",
    "\n",
    "# Specify one or more initial points for the search of optimal parameter\n",
    "default_params = [1, \n",
    "                  1, \n",
    "                  32,\n",
    "                  32, \n",
    "                  1e-3,\n",
    "                  1e-3 \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_multi_channel_lstm_model(num_lstm_layersA, num_lstm_layersB, num_lstm_unitsA, num_lstm_unitsB, learning_rate, adam_decay):\n",
    "    # Vocabulary-based embedding layer\n",
    "    inputsA = Input(shape=(max_seq_length,), name=\"input regular embeddings\")\n",
    "    # Word2Vec embedding layer\n",
    "    inputsB = Input(shape=(max_seq_length,), name=\"input word2vec embeddings\")\n",
    "    \n",
    "    # Define an embedding layer for each input\n",
    "    embeddingsA = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, name=\"embeddingsA\")(inputsA)\n",
    "    embeddingsB = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, weights=[w2v_embedding_vectors], trainable=False, name=\"embeddingsB\")(inputsB)\n",
    "    \n",
    "    # Pass both embeddings through their own LSTM layers\n",
    "    lstm_layersA = embeddingsA\n",
    "    for i in range(num_lstm_layersA):\n",
    "        nameA = 'layer_lstmA_{0}'.format(i+1)\n",
    "        if i < num_lstm_layers-1:\n",
    "            lstm_layersA = LSTM(num_lstm_unitsA, return_sequences=True, name=nameA)(lstm_layersA)\n",
    "        else:\n",
    "            lstm_layersA = LSTM(num_lstm_unitsA, return_sequences=False, name=nameA)(lstm_layersA)\n",
    "        \n",
    "    lstm_layersB = embeddingsB\n",
    "    for i in range(num_lstm_layersB):\n",
    "        nameA = 'layer_lstmB_{0}'.format(i+1)\n",
    "        if i < num_lstm_layers-1:\n",
    "            lstm_layersA = LSTM(num_lstm_unitsB, return_sequences=True, name=nameA)(lstm_layersB)\n",
    "        else:\n",
    "            lstm_layersA = LSTM(num_lstm_unitsB, return_sequences=False, name=nameA)(lstm_layersB)\n",
    "        \n",
    "\n",
    "    # Concatenate the two inputs\n",
    "    merged = concatenate([lstm_layersA, lstm_layersB])\n",
    "\n",
    "    # Dense layer for the merged inputs & output Layer\n",
    "    outputs = Dense(num_output_classes, activation='softmax', name=\"output\")(merged)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[inputsA, inputsB], outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    adam = Adam(learning_rate=learning_rate, decay=adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=search_space)\n",
    "def multi_objective_function(num_lstm_layersA, num_lstm_layersB, num_lstm_unitsA, num_lstm_unitsB, learning_rate, adam_decay, batch_size):\n",
    "\n",
    "    model = define_multi_channel_lstm_model(num_lstm_layersA=num_lstm_layersA,\n",
    "                                            num_lstm_layersB=num_lstm_layersB,\n",
    "                                            num_lstm_unitsA=num_lstm_unitsA,\n",
    "                                            num_lstm_unitsB=num_lstm_unitsB,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            adam_decay=adam_decay\n",
    "                                            )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Loss: {loss:.2}\\n\")\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_result = gp_minimize(func=multi_objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gp_best_params = {param.name: value for param, value in zip(gp_result.space, gp_result.x)}\n",
    "print(\"Best Hyperparameters:\", gp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_lstm_model(gp_best_params['num_lstm_layersA'],\n",
    "                                        gp_best_params['num_lstm_layersB'],\n",
    "                                        gp_best_params['num_lstm_unitsA'], \n",
    "                                        gp_best_params['num_lstm_unitsB'],\n",
    "                                        gp_best_params['learning_rate'], \n",
    "                                        gp_best_params['adam_decay']\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_result = gbrt_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
    "print(\"Best Hyperparameters:\", gbrt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_lstm_model(gbrt_best_params['num_lstm_layersA'],\n",
    "                                        gbrt_best_params['num_lstm_layersB'],\n",
    "                                        gbrt_best_params['num_lstm_unitsA'], \n",
    "                                        gbrt_best_params['num_lstm_unitsB'],\n",
    "                                        gbrt_best_params['learning_rate'], \n",
    "                                        gbrt_best_params['adam_decay'],\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=20, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
