{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, RocCurveDisplay, confusion_matrix, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "senti_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_categories = list(senti_labels.values())\n",
    "NUM_of_CLASSES = 3\n",
    "\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/\"\n",
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/src/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(lowercase=False, max_df=0.95, max_features=10000, min_df=5,\n",
      "                sublinear_tf=True,\n",
      "                vocabulary={'..': 564, '...': 169, '..._NEG': 124, ':)': 506,\n",
      "                            'able': 194, 'able_NEG': 245, 'absolutely': 517,\n",
      "                            'accessible': 525, 'actually': 87,\n",
      "                            'actually_NEG': 667, 'add': 94, 'additional': 503,\n",
      "                            'advanced': 372, 'algorithm': 417, 'almost': 555,\n",
      "                            'along': 595, 'already': 223, 'already_NEG': 288,\n",
      "                            'also': 75, 'also_NEG': 218, 'although': 345,\n",
      "                            'always': 435, 'always_NEG': 530, 'amazing': 22,\n",
      "                            'amount': 264, 'analysis': 376, 'andrew': 319,\n",
      "                            'another': 617, 'another_NEG': 243, 'answer': 588, ...})\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(input_folder_path, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(input_folder_path, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(input_folder_path, \"test.csv\"))\n",
    "\n",
    "x_train = pd.read_csv(os.path.join(processed_folder_path, \"train_tfidf.csv\"))\n",
    "y_train = train['y']\n",
    "x_val = pd.read_csv(os.path.join(processed_folder_path, \"val_tfidf.csv\"))\n",
    "y_val = val['y']\n",
    "x_test = pd.read_csv(os.path.join(processed_folder_path, \"test_tfidf.csv\"))\n",
    "y_test = test['y']\n",
    "\n",
    "%store -r tfidf_vectorizer\n",
    "print(tfidf_vectorizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    # TODO: not anymore?? Handle the zero-division error when there are no predicted samples for a label\n",
    "    # only interested in labels that were predicted at least once\n",
    "    precision = precision_score(y, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    \n",
    "    print(f\"Accuracy: {(accuracy * 100):.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"f1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_report(y, y_pred, labels):\n",
    "    report = classification_report(y, y_pred, labels=labels)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
    "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=labels)\n",
    "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can I use for svm?\n",
    "def plot_feature_imp(model):\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    feature_importances.nlargest(20).plot.bar(ax=ax)\n",
    "    ax.set_title(\"Top 20 Most Predictive Features\")\n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Importance')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, x, y, params, only_metrics):    \n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    print(f\"*{model_name}\")\n",
    "\n",
    "    calculate_metrics(y, y_pred)\n",
    "    print(f\"Params: {params}\\n\")\n",
    "    \n",
    "    senti_labels = ['negative', 'neutral', 'positive'] #TODO: to constants\n",
    "    \n",
    "    if not only_metrics:\n",
    "        calculate_classification_report(y, y_pred, labels=senti_labels)\n",
    "        plot_confusion_matrix(y, y_pred, labels=senti_labels)\n",
    "        plot_feature_imp(model) #TODO: for especially RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(prob_test_vec, y_test, labels):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels\n",
    "    colors = cycle(['limegreen', 'dodgerblue', 'red'])\n",
    "    for senti, color in zip(range(NUM_of_CLASSES), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_test[:, senti],\n",
    "            prob_test_vec[:, senti],\n",
    "            name=f\"ROC curve for {labels[senti]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_OvR_roc_auc_score(model, x, y, x_test, y_test, labels): #average??\n",
    "    y = one_hot_encode(y)\n",
    "    y_test = one_hot_encode(y_test)\n",
    "\n",
    "    ovr_model = OneVsRestClassifier(model).fit(x, y)\n",
    "    prob_test_vec = ovr_model.predict_proba(x_test)\n",
    "    \n",
    "    fpr, tpr, thresholds, auc_score = [], [], [], []\n",
    "    for _ in range(NUM_of_CLASSES):\n",
    "        fpr.append(0)\n",
    "        tpr.append(0)\n",
    "        thresholds.append(0)\n",
    "        auc_score.append(0)\n",
    "    \n",
    "    for i in range(NUM_of_CLASSES):\n",
    "        fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], prob_test_vec[:, i])\n",
    "        auc_score[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    print(f\"AUC score: {auc_score}\")\n",
    "    averaged_auc_score = (sum(auc_score) / NUM_of_CLASSES)\n",
    "    print(f\"Averaged AUC score: {averaged_auc_score:.2f}\")\n",
    "    \n",
    "    plot_roc_curve(prob_test_vec, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top3_models(top3_models):    \n",
    "    # Print the sorted list of mean test scores and standard deviation of test scores\n",
    "    print(\"\\nTop 3 parameter combinations ranked by performance (from best to worst):\")\n",
    "    for index, row in top3_models.iterrows():\n",
    "        mean_score = row['mean_test_score']\n",
    "        std_score = row['std_test_score']\n",
    "        params = row['params']\n",
    "        print(f\"Mean Test Score: {mean_score:.4f} (Â±{std_score:.4f}) for {params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Random Forest model\n",
    "rf_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [20, 50, 100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10, 30, 50],\n",
    "    'min_samples_leaf': [1, 5, 20, 50],  # Minimum number of samples required to be at a leaf node\n",
    "    #'max_features': ['auto', 'sqrt'],  # Number of features to consider when looking for the best split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=rf_param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "top3_models = sorted_results[:3]\n",
    "print_top3_models(top3_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_params = top3_models['params'].values\n",
    "res_filename = \"rf_results\"\n",
    "\n",
    "# Evaluate the top 3 models on the validation set\n",
    "rf_cand_0 = RandomForestClassifier(**top3_params[0])\n",
    "rf_cand_0.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_0, \"Training-RF-0\", x_train, y_train, top3_params[0], only_metrics=True)\n",
    "evaluate_model(rf_cand_0, \"Validation-RF-0\", x_val, y_val, top3_params[0], only_metrics=True)\n",
    "\n",
    "rf_cand_1 = RandomForestClassifier(**top3_params[1])\n",
    "rf_cand_1.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_1, \"Training-RF-1\", x_train, y_train, top3_params[1], only_metrics=True)\n",
    "evaluate_model(rf_cand_1, \"Validation-RF-1\", x_val, y_val, top3_params[1], only_metrics=True)\n",
    "\n",
    "rf_cand_2 = RandomForestClassifier(**top3_params[2])\n",
    "rf_cand_2.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_2, \"Training-RF-2\", x_train, y_train, top3_params[2], only_metrics=True)\n",
    "evaluate_model(rf_cand_2, \"Validation-RF-2\", x_val, y_val, top3_params[2], only_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "rf_best = rf_cand_0\n",
    "y_pred = rf_best.predict(x_test)\n",
    "evaluate_model(rf_best, \"RF-best\", x_test, y_test, rf_best.get_params(), only_metrics=False)\n",
    "# Calculate OvR AUC ROC score\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "calculate_OvR_roc_auc_score(rf_best, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Naive Bayes model & fit on training data\n",
    "nb_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "nb_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict on the training data\n",
    "y_train_pred = nb_model.predict(x_train)\n",
    "\n",
    "# Calculate the training accuracy score\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "print(nb_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "nb_param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0],  # Smoothing parameter for MultinomialNB\n",
    "    'fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=nb_model, param_grid=nb_param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "top3_models = sorted_results[:3]\n",
    "print_top3_models(top3_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_params = top3_models['params'].values\n",
    "\n",
    "# Evaluate the top 3 models on the validation set\n",
    "nb_cand_0 = MultinomialNB(**top3_params[0])\n",
    "nb_cand_0.fit(x_train, y_train)\n",
    "evaluate_model(nb_cand_0, \"Training-NB-0\", x_train, y_train, top3_params[0], only_metrics=True)\n",
    "evaluate_model(nb_cand_0, \"Validation-NB-0\", x_val, y_val, top3_params[0], only_metrics=True)\n",
    "\n",
    "nb_cand_1 = MultinomialNB(**top3_params[1])\n",
    "nb_cand_1.fit(x_train, y_train)\n",
    "evaluate_model(nb_cand_1, \"Training-NB-1\", x_train, y_train, top3_params[1], only_metrics=True)\n",
    "evaluate_model(nb_cand_1, \"Validation-NB-1\", x_val, y_val, top3_params[1], only_metrics=True)\n",
    "\n",
    "nb_cand_2 = MultinomialNB(**top3_params[2])\n",
    "nb_cand_2.fit(x_train, y_train)\n",
    "evaluate_model(nb_cand_2, \"Training-NB-2\", x_train, y_train, top3_params[2], only_metrics=True)\n",
    "evaluate_model(nb_cand_2, \"Validation-NB-2\", x_val, y_val, top3_params[2], only_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "nb_best = nb_cand_0\n",
    "y_pred = nb_best.predict(x_test)\n",
    "evaluate_model(rf_best, \"NB-best\", x_test, y_test, nb_best.get_params(), only_metrics=False)\n",
    "calculate_OvR_roc_auc_score(nb_best, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training &  Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the SVM model\n",
    "svm_model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': [0.1, 1, 'scale']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(svm_model, param_grid=svm_param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "top3_models = sorted_results[:3]\n",
    "print_top3_models(top3_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_params = top3_models['params'].values\n",
    "\n",
    "# Evaluate the top 3 models on the validation set\n",
    "svm_cand_0 = SVC(**top3_params[0])\n",
    "svm_cand_0.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_0, \"Training-SVM-0\", x_train, y_train, top3_params[0], only_metrics=True)\n",
    "evaluate_model(rf_cand_0, \"Validation-SVM-0\", x_val, y_val, top3_params[0], only_metrics=True)\n",
    "\n",
    "svm_cand_1 = SVC(**top3_params[1])\n",
    "svm_cand_1.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_1, \"Training-SVM-1\", x_train, y_train, top3_params[1], only_metrics=True)\n",
    "evaluate_model(rf_cand_1, \"Validation-SVM-1\", x_val, y_val, top3_params[1], only_metrics=True)\n",
    "\n",
    "\n",
    "svm_cand_2 = SVC(**top3_params[2])\n",
    "svm_cand_2.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_2, \"Training-SVM-2\", x_train, y_train, top3_params[2], only_metrics=True)\n",
    "evaluate_model(rf_cand_2, \"Validation-SVM-2\", x_val, y_val, top3_params[2], only_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "svm_best = svm_cand_0\n",
    "y_pred = svm_best.predict(x_test)\n",
    "evaluate_model(rf_best, \"SVM-best\", x_test, y_test, svm_best.get_params(), only_metrics=False)\n",
    "calculate_OvR_roc_auc_score(svm_best, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
