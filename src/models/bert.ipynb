{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/\"\n",
    "\n",
    "def drop_missing(data):\n",
    "    # Remove any rows with missing values and reset the index\n",
    "    rows_before = data.shape[0] \n",
    "\n",
    "    data.replace('', np.nan, inplace=True)\n",
    "    data = data.dropna()\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    rows_after = data.shape[0]\n",
    "    # Count the number of rows removed\n",
    "    print(f\"Number of removed rows: {rows_before - rows_after}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_data(filename, process=True):\n",
    "    # Load data\n",
    "    data_file_path = os.path.join(input_folder_path, filename)\n",
    "    df = pd.read_csv(data_file_path)\n",
    "\n",
    "    if process:\n",
    "        # Set ID as index\n",
    "        df.set_index('Id', inplace=True, drop=True)\n",
    "        df = drop_missing(df)\n",
    "\n",
    "        print(df.info())\n",
    "        print()\n",
    "        print(f'Dataset shape: {df.shape}\\n')\n",
    "\n",
    "    return df\n",
    "\n",
    "data_filename = \"original_data.csv\"\n",
    "df_original = load_data(data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 659\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
