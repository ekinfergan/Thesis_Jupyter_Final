{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical/Traditional ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.sparse import load_npz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/s3985113/Thesis_Jupyter_Final/src/\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname(os.path.abspath('processor.ipynb'))\n",
    "data_path = os.path.join(script_dir, 'Thesis_Jupyter_Final/src/')\n",
    "os.getcwd()\n",
    "print(data_path)\n",
    "\n",
    "input_folder_path = os.path.join(data_path, 'input')\n",
    "processed_folder_path = os.path.join(data_path, 'input/processed/neg_tagged')\n",
    "results_folder_path = \"results\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(results_folder_path):\n",
    "    os.makedirs(results_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_labels_dict = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_labels = list(senti_labels_dict.values())\n",
    "NUM_of_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4944)\t0.333611549281506\n",
      "  (0, 2133)\t0.2961339731660829\n",
      "  (0, 1962)\t0.2790813018609294\n",
      "  (0, 1829)\t0.2845491982794516\n",
      "  (0, 1738)\t0.2471451738558906\n",
      "  (0, 1627)\t0.27254171122483684\n",
      "  (0, 784)\t0.24895427348562024\n",
      "  (0, 581)\t0.2096952161674913\n",
      "  (0, 438)\t0.21087359433125996\n",
      "  (0, 400)\t0.21511850812086866\n",
      "  (0, 337)\t0.22498142205493177\n",
      "  (0, 307)\t0.22522954040809726\n",
      "  (0, 223)\t0.21748277299714208\n",
      "  (0, 207)\t0.2005277841281215\n",
      "  (0, 41)\t0.16507539015093728\n",
      "  (0, 37)\t0.3065855262224691\n",
      "  (1, 3132)\t0.48745999184528344\n",
      "  (1, 1117)\t0.3794568593358208\n",
      "  (1, 1065)\t0.41963408220421816\n",
      "  (1, 811)\t0.43121465736774295\n",
      "  (1, 38)\t0.2770028355754138\n",
      "  (1, 22)\t0.27822837453512\n",
      "  (1, 20)\t0.3197105022731662\n",
      "  (2, 3942)\t0.3961231024356585\n",
      "  (2, 2937)\t0.3742032094232427\n",
      "  :\t:\n",
      "  (40997, 8925)\t0.4335846690233216\n",
      "  (40997, 4246)\t0.3889950728579958\n",
      "  (40997, 2499)\t0.36388494758151557\n",
      "  (40997, 1887)\t0.34690683112213216\n",
      "  (40997, 1080)\t0.32385818576697817\n",
      "  (40997, 665)\t0.29932936951200767\n",
      "  (40997, 211)\t0.2333765628328547\n",
      "  (40997, 160)\t0.23121906946151174\n",
      "  (40997, 141)\t0.23627942948322128\n",
      "  (40997, 76)\t0.22292043563693573\n",
      "  (40998, 1848)\t0.43831688266822394\n",
      "  (40998, 913)\t0.3760677807264082\n",
      "  (40998, 641)\t0.3929684005037529\n",
      "  (40998, 196)\t0.5298654315663285\n",
      "  (40998, 168)\t0.2928021224321956\n",
      "  (40998, 76)\t0.2934145805864686\n",
      "  (40998, 38)\t0.24381262539610832\n",
      "  (40999, 4886)\t0.4848468205939975\n",
      "  (40999, 1436)\t0.3843771673680766\n",
      "  (40999, 1403)\t0.36808792285342495\n",
      "  (40999, 985)\t0.344616717963534\n",
      "  (40999, 545)\t0.31961205784997043\n",
      "  (40999, 513)\t0.3176331302501415\n",
      "  (40999, 315)\t0.29990169970303915\n",
      "  (40999, 89)\t0.26447050889486823\n",
      "[1 1 1 ... 3 3 3]\n",
      "(41000, 10573) (41000,)\n",
      "(11540, 10573) (11540,)\n",
      "(11921, 10573) (11921,)\n"
     ]
    }
   ],
   "source": [
    "def load_tfidf_data():\n",
    "    train = pd.read_csv(os.path.join(processed_folder_path, \"train.csv\"))\n",
    "    val = pd.read_csv(os.path.join(processed_folder_path, \"val.csv\"))\n",
    "    test = pd.read_csv(os.path.join(processed_folder_path, \"test.csv\"))\n",
    "    y_train = train['y'].values\n",
    "    y_val = val['y'].values\n",
    "    y_test = test['y'].values\n",
    "\n",
    "    with open(os.path.join(processed_folder_path, \"train_tfidf.pickle\"), \"rb\") as file:\n",
    "        x_train_tfidf = pickle.load(file)\n",
    "    with open(os.path.join(processed_folder_path, \"val_tfidf.pickle\"), \"rb\") as file:\n",
    "        x_val_tfidf = pickle.load(file)\n",
    "    with open(os.path.join(processed_folder_path, \"test_tfidf.pickle\"), \"rb\") as file:\n",
    "        x_test_tfidf = pickle.load(file)\n",
    "\n",
    "    return x_train_tfidf, y_train, x_val_tfidf, y_val, x_test_tfidf, y_test\n",
    "\n",
    "x_train_tfidf, y_train, x_val_tfidf, y_val, x_test_tfidf, y_test = load_tfidf_data()\n",
    "print(x_train_tfidf)\n",
    "print(y_train)\n",
    "print(x_train_tfidf.shape, y_train.shape)\n",
    "print(x_val_tfidf.shape, y_val.shape)\n",
    "print(x_test_tfidf.shape, y_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded\n",
    "\n",
    "def calculate_metrics(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, Recall: {recall:.2f}, f1-score: {f1:.2f}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def calculate_classification_report(y, y_pred):\n",
    "    return classification_report(y, y_pred)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, res_path):\n",
    "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
    "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=labels)\n",
    "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.savefig(os.path.join(res_path, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(prob_test_vec, y_test, labels, res_path):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels\n",
    "    colors = cycle(['limegreen', 'dodgerblue', 'red'])\n",
    "    for senti, color in zip(range(NUM_of_CLASSES), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_test[:, senti],\n",
    "            prob_test_vec[:, senti],\n",
    "            name=f\"ROC curve for {labels[senti]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )\n",
    "    plt.savefig(os.path.join(res_path, \"roc_curve.png\"))\n",
    "    plt.close()\n",
    "        \n",
    "def calculate_OvR_roc_auc_score(model, model_name, x, y, x_test, y_test, labels, res_path): #average??\n",
    "    y = one_hot_encode(y)\n",
    "    y_test = one_hot_encode(y_test)\n",
    "\n",
    "    ovr_model = OneVsRestClassifier(model).fit(x, y)\n",
    "    prob_test_vec = ovr_model.predict_proba(x_test)\n",
    "    \n",
    "    fpr, tpr, thresholds, auc_score = [], [], [], []\n",
    "    for _ in range(NUM_of_CLASSES):\n",
    "        fpr.append(0)\n",
    "        tpr.append(0)\n",
    "        thresholds.append(0)\n",
    "        auc_score.append(0)\n",
    "    \n",
    "    for i in range(NUM_of_CLASSES):\n",
    "        fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], prob_test_vec[:, i])\n",
    "        auc_score[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    averaged_auc_score = (sum(auc_score) / NUM_of_CLASSES)\n",
    "    # Save AUC to results.txt\n",
    "    with open(os.path.join(res_path, f\"{model_name}_results.txt\"), \"a\") as f:\n",
    "        f.write(f\"AUC score: {auc_score}\\n\")\n",
    "        f.write(f\"Averaged AUC score: {averaged_auc_score:.2f}\\n\")\n",
    "\n",
    "    plot_roc_curve(prob_test_vec, y_test, labels, res_path=res_path)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "def plot_feature_imp(model, res_path):\n",
    "    processed_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/processed\"\n",
    "    vect_file_path = os.path.join(processed_folder_path, 'tfidf_vectorizer.joblib')\n",
    "    loaded_tfidf_vectorizer = joblib.load(vect_file_path)\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = loaded_tfidf_vectorizer.get_feature_names_out()\n",
    "    feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    feature_importances.nlargest(20).plot.bar(ax=ax)\n",
    "    ax.set_title(\"Top 20 Most Predictive Features\")\n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Importance')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(res_path, \"feature_importance.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_model(y_pred, model_name, x, y, params, labels, res_path, only_metrics, model=None):\n",
    "    if not os.path.exists(res_path):\n",
    "        os.makedirs(res_path)\n",
    "\n",
    "    with open(os.path.join(res_path, f\"{model_name}_results.txt\"), \"w\") as f:\n",
    "        f.write(f\"*{model_name}\\n\")\n",
    "        f.write(f\"Params: {params}\\n\\n\")\n",
    "\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(y, y_pred)\n",
    "        f.write(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
    "        f.write(f\"Precision: {precision:.2f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.2f}\\n\")\n",
    "        f.write(f\"f1-score: {f1:.2f}\\n\\n\")\n",
    "\n",
    "        if not only_metrics:\n",
    "            report = calculate_classification_report(y, y_pred)\n",
    "            f.write(\"Classification Report:\\n\")\n",
    "            f.write(report)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            plot_confusion_matrix(y, y_pred, labels=labels, res_path=res_path)\n",
    "\n",
    "            if model_name == 'RF':\n",
    "                plot_feature_imp(model, res_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top3_models(top3_models):  \n",
    "    # Print the sorted list of mean test scores and standard deviation of test scores\n",
    "    print(\"Top 3 Param Combinations (on training set) (best to worst):\")\n",
    "    for index, row in top3_models.iterrows():\n",
    "        mean_score = row['mean_test_score']\n",
    "        std_score = row['std_test_score']\n",
    "        params = row['params']\n",
    "        print(f\"Mean Test Score: {mean_score:.4f} (±{std_score:.4f}) for {params}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def perform_grid_search(model, param_grid, x_train, y_train):\n",
    "    print(\"*Performing grid search...\")\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=5)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "    top3_models = sorted_results[:3]\n",
    "    print_top3_models(top3_models)\n",
    "    top3_params = top3_models['params'].values\n",
    "\n",
    "    return top3_params\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(model_type, top3_params, x_train, y_train, x_val, y_val):\n",
    "    print(\"*Train with Top 3 Params and Evaluate:\")\n",
    "    trained_models = []\n",
    "    best_accuracy = 0\n",
    "    for i in range(3):\n",
    "        if model_type == \"RF\":\n",
    "            model = RandomForestClassifier(**top3_params[i])\n",
    "        elif model_type == \"NB\":\n",
    "            model = MultinomialNB(**top3_params[i])\n",
    "        elif model_type == \"SVM\":\n",
    "            model = SVC(**top3_params[i])\n",
    "        else:\n",
    "            print(f\"Unknown model type: {model_type}\")\n",
    "            return\n",
    "        model.fit(x_train, y_train)\n",
    "        # Get accuracy for the validation set (.score calls .predict() internally)\n",
    "        val_accuracy = model.score(x_val, y_val)\n",
    "        if val_accuracy > best_accuracy:\n",
    "            # Store the best model\n",
    "            best_model = model\n",
    "            best_params = top3_params[i]\n",
    "            best_accuracy = val_accuracy\n",
    "            idx = i\n",
    "        trained_models.append((model, top3_params[i]))\n",
    "    \n",
    "    print(f\"--> Model {idx} - {best_params} gives highest val accuracy {best_accuracy:.2f}%\\n\")\n",
    "\n",
    "    # Return the fitted models and their respective params for more in-depth evaluation\n",
    "    return trained_models, best_model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ml(model, model_name, param_grid, x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "    # Perform grid search and save top 3 models\n",
    "    top3_params = perform_grid_search(model, param_grid, x_train, y_train) \n",
    "\n",
    "    # Fit the top 3 models, get the model with highest validation accuracy\n",
    "    trained_models, best_model, best_params = train_and_evaluate_models(model_name, top3_params, x_train, y_train, x_val, y_val)\n",
    "\n",
    "    # Observe evaluation on models with top 3 params and save results in dir\n",
    "    subfolder_path = f\"{model_name}_results/{model_name}_trained\"\n",
    "    res_path = os.path.join(results_folder_path, subfolder_path)\n",
    "    \n",
    "    for i, (model, params) in enumerate(trained_models):\n",
    "\n",
    "        y_pred_train = model.predict(x_train)\n",
    "        y_pred_val = model.predict(x_val)\n",
    "\n",
    "        print(f\"Model {i}: {params}\")\n",
    "        print(\"- Training: \")\n",
    "        evaluate_model(y_pred_train, f\"Training-{model_name}-{i}\", x_train, y_train, params, senti_labels, res_path, only_metrics=True)\n",
    "        print(\"- Validation:\")\n",
    "        evaluate_model(y_pred_val, f\"Validation-{model_name}-{i}\", x_val, y_val, params, senti_labels, res_path, only_metrics=True)\n",
    "        print()\n",
    "\n",
    "\n",
    "    # Observe the best model\n",
    "    for i, (model, params) in enumerate(trained_models):\n",
    "        if model == best_model:\n",
    "            print(f\"Best Model = model {i} with {params}\")\n",
    "            print()\n",
    "            break\n",
    "    \n",
    "    subfolder_path = f\"{model_name}_results/{model_name}_best\"\n",
    "    res_path = os.path.join(results_folder_path, subfolder_path)\n",
    "    model_type = f\"{model_name}-best\"\n",
    "    \n",
    "    y_pred = best_model.predict(x_test)\n",
    "    print(f\"Class Predictions: {np.bincount(y_pred)}\")\n",
    "\n",
    "    print(\"Test Evaluation: \")\n",
    "    evaluate_model(y_pred, model_type, x_test, y_test, best_params, senti_labels, res_path, only_metrics=False)\n",
    "\n",
    "    if model_name != \"SVM\":\n",
    "        calculate_OvR_roc_auc_score(best_model, model_type, x_train, y_train, x_test, y_test, senti_labels, res_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Performing grid search...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Param Combinations (on training set) (best to worst):\n",
      "Mean Test Score: 0.7752 (±0.0079) for {'alpha': 0.001, 'fit_prior': False}\n",
      "Mean Test Score: 0.7683 (±0.0074) for {'alpha': 0.01, 'fit_prior': False}\n",
      "Mean Test Score: 0.7549 (±0.0055) for {'alpha': 0.001, 'fit_prior': True}\n",
      "\n",
      "*Train with Top 3 Params and Evaluate:\n",
      "--> Model 2 - {'alpha': 0.001, 'fit_prior': True} gives highest val accuracy 0.84%\n",
      "\n",
      "Model 0: {'alpha': 0.001, 'fit_prior': False}\n",
      "- Training: \n",
      "Accuracy: 0.81%, Precision: 0.81, Recall: 0.81, f1-score: 0.81\n",
      "- Validation:\n",
      "Accuracy: 0.80%, Precision: 0.86, Recall: 0.80, f1-score: 0.82\n",
      "\n",
      "Model 1: {'alpha': 0.01, 'fit_prior': False}\n",
      "- Training: \n",
      "Accuracy: 0.80%, Precision: 0.80, Recall: 0.80, f1-score: 0.80\n",
      "- Validation:\n",
      "Accuracy: 0.79%, Precision: 0.86, Recall: 0.79, f1-score: 0.82\n",
      "\n",
      "Model 2: {'alpha': 0.001, 'fit_prior': True}\n",
      "- Training: \n",
      "Accuracy: 0.78%, Precision: 0.78, Recall: 0.78, f1-score: 0.78\n",
      "- Validation:\n",
      "Accuracy: 0.84%, Precision: 0.85, Recall: 0.84, f1-score: 0.85\n",
      "\n",
      "Best Model = model 2 with {'alpha': 0.001, 'fit_prior': True}\n",
      "\n",
      "Class Predictions: [   0 1276  907 9738]\n",
      "Test Evaluation: \n",
      "Accuracy: 0.82%, Precision: 0.83, Recall: 0.82, f1-score: 0.83\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb_param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1],  # Smoothing parameter for MultinomialNB\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "setup_ml(nb, \"NB\", nb_param_grid, x_train_tfidf, y_train, x_val_tfidf, y_val, x_test_tfidf, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Performing grid search...\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m svm \u001b[39m=\u001b[39m SVC()\n\u001b[1;32m      2\u001b[0m svm_param_grid \u001b[39m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m100\u001b[39m],\n\u001b[1;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m setup_ml(svm, \u001b[39m\"\u001b[39;49m\u001b[39mSVM\u001b[39;49m\u001b[39m\"\u001b[39;49m, svm_param_grid, x_train_tfidf, y_train, x_val_tfidf, y_val, x_test_tfidf, y_test)\n",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m, in \u001b[0;36msetup_ml\u001b[0;34m(model, model_name, param_grid, x_train, y_train, x_val, y_val, x_test, y_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup_ml\u001b[39m(model, model_name, param_grid, x_train, y_train, x_val, y_val, x_test, y_test):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Perform grid search and save top 3 models\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     top3_params \u001b[39m=\u001b[39m perform_grid_search(model, param_grid, x_train, y_train) \n\u001b[1;32m      5\u001b[0m     \u001b[39m# Fit the top 3 models, get the model with highest validation accuracy\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     trained_models, best_model, best_params \u001b[39m=\u001b[39m train_and_evaluate_models(model_name, top3_params, x_train, y_train, x_val, y_val)\n",
      "Cell \u001b[0;32mIn[35], line 16\u001b[0m, in \u001b[0;36mperform_grid_search\u001b[0;34m(model, param_grid, x_train, y_train)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Perform grid search\u001b[39;00m\n\u001b[1;32m     15\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mmodel, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Get the mean test scores and standard deviations of test scores for all parameter combinations\u001b[39;00m\n\u001b[1;32m     19\u001b[0m results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(grid_search\u001b[39m.\u001b[39mcv_results_)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/Python/3.10.8-GCCcore-12.2.0/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/Python/3.10.8-GCCcore-12.2.0/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "svm = SVC(probability=True)\n",
    "svm_param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [0.01, 0.1, 1, 'scale'],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "}\n",
    "setup_ml(svm, \"SVM\", svm_param_grid, x_train_tfidf, y_train, x_val_tfidf, y_val, x_test_tfidf, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "rf_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 3, 5, 7, 10],  # Limit maximum depth of the trees\n",
    "        'min_samples_split': [2, 5, 10, 20],  # Higher values will prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "        'min_samples_leaf': [1, 2, 5, 10, 15],  # Higher values prevent a model from getting too complex\n",
    "}\n",
    "setup_ml(rf, \"RF\", rf_param_grid, x_train_tfidf, y_train, x_val_tfidf, y_val, x_test_tfidf, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
