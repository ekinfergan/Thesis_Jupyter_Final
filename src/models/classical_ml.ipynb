{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical/Traditional ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.sparse import load_npz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "senti_labels_dict = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_labels = list(senti_labels_dict.values())\n",
    "NUM_of_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/s3985113/Thesis_Jupyter_Final/src/\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname(os.path.abspath('classical_ml.ipynb'))\n",
    "data_path = os.path.join(script_dir, 'Thesis_Jupyter_Final/src/')\n",
    "os.getcwd()\n",
    "print(data_path)\n",
    "\n",
    "input_folder_path = os.path.join(data_path, 'input')\n",
    "processed_folder_path = os.path.join(data_path, 'input/processed')\n",
    "results_folder_path = \"results\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(results_folder_path):\n",
    "    os.makedirs(results_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16667)\t0.3340626292951124\n",
      "  (0, 10586)\t0.3340626292951124\n",
      "  (0, 4954)\t0.2832062570364835\n",
      "  (0, 2143)\t0.24622299074589576\n",
      "  (0, 1972)\t0.24158921518154958\n",
      "  (0, 1839)\t0.24306190367690358\n",
      "  (0, 1748)\t0.21018143824743002\n",
      "  (0, 1637)\t0.23137019421515947\n",
      "  (0, 794)\t0.20963592597578756\n",
      "  (0, 591)\t0.1794775743569737\n",
      "  (0, 448)\t0.17828290066587452\n",
      "  (0, 410)\t0.1840822468807772\n",
      "  (0, 347)\t0.19165520106708653\n",
      "  (0, 317)\t0.19186375805660139\n",
      "  (0, 233)\t0.18811255223068046\n",
      "  (0, 217)\t0.17003426400216445\n",
      "  (0, 51)\t0.1414731906551496\n",
      "  (0, 47)\t0.2596634168344216\n",
      "  (0, 7)\t0.23175049169670642\n",
      "  (1, 3142)\t0.49792756873210103\n",
      "  (1, 1127)\t0.3742871262374112\n",
      "  (1, 1075)\t0.4170384752631189\n",
      "  (1, 821)\t0.42938547104480307\n",
      "  (1, 48)\t0.2740555055811665\n",
      "  (1, 32)\t0.2768839018778363\n",
      "  :\t:\n",
      "  (40996, 243)\t0.13917589764075816\n",
      "  (40996, 241)\t0.13237590790751455\n",
      "  (40996, 214)\t0.22491664991776356\n",
      "  (40996, 178)\t0.1212018974639381\n",
      "  (40996, 176)\t0.13331215579426486\n",
      "  (40996, 161)\t0.1094659802198736\n",
      "  (40996, 143)\t0.1242511651241684\n",
      "  (40996, 110)\t0.12302246902295963\n",
      "  (40996, 103)\t0.11433049629000266\n",
      "  (40996, 94)\t0.11622247928971599\n",
      "  (40996, 57)\t0.11850142690121916\n",
      "  (40996, 56)\t0.10863201802018187\n",
      "  (40997, 415)\t0.46583744068518407\n",
      "  (40997, 312)\t0.4487967078153373\n",
      "  (40997, 249)\t0.43153758626009475\n",
      "  (40997, 175)\t0.3979020657843364\n",
      "  (40997, 61)\t0.358315717388545\n",
      "  (40997, 0)\t0.3295999063587455\n",
      "  (40998, 930)\t0.6373215843861528\n",
      "  (40998, 159)\t0.43350942689091815\n",
      "  (40998, 45)\t0.45979757974716007\n",
      "  (40998, 14)\t0.44099542007926273\n",
      "  (40999, 846)\t0.6226305548223549\n",
      "  (40999, 575)\t0.589564779872217\n",
      "  (40999, 14)\t0.5145333444353543\n",
      "[1 1 1 ... 3 3 3]\n",
      "(41000, 32271) (41000,)\n",
      "(11541, 32271) (11541,)\n",
      "(11927, 32271) (11927,)\n"
     ]
    }
   ],
   "source": [
    "def load_tfidf_data():\n",
    "    train = pd.read_csv(os.path.join(input_folder_path, \"train.csv\"))\n",
    "    val = pd.read_csv(os.path.join(input_folder_path, \"val.csv\"))\n",
    "    test = pd.read_csv(os.path.join(input_folder_path, \"test.csv\"))\n",
    "    y_train = train['y'].values\n",
    "    y_val = val['y'].values\n",
    "    y_test = test['y'].values\n",
    "\n",
    "    x_train = load_npz(os.path.join(processed_folder_path, \"train_tfidf.npz\"))\n",
    "    x_val = load_npz(os.path.join(processed_folder_path, \"val_tfidf.npz\"))\n",
    "    x_test = load_npz(os.path.join(processed_folder_path, \"test_tfidf.npz\"))\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "x_train_tfidf, y_train, x_val_tfidf, y_val, x_test_tfidf, y_test = load_tfidf_data()\n",
    "print(x_train_tfidf)\n",
    "print(y_train)\n",
    "print(x_train_tfidf.shape, y_train.shape)\n",
    "print(x_val_tfidf.shape, y_val.shape)\n",
    "print(x_test_tfidf.shape, y_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded\n",
    "\n",
    "def calculate_metrics(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, Recall: {recall:.2f}, f1-score: {f1:.2f}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def calculate_classification_report(y, y_pred):\n",
    "    return classification_report(y, y_pred)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, res_path):\n",
    "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
    "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=labels)\n",
    "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.savefig(os.path.join(res_path, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(prob_test_vec, y_test, labels, res_path):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels\n",
    "    colors = cycle(['limegreen', 'dodgerblue', 'red'])\n",
    "    for senti, color in zip(range(NUM_of_CLASSES), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_test[:, senti],\n",
    "            prob_test_vec[:, senti],\n",
    "            name=f\"ROC curve for {labels[senti]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )\n",
    "    plt.savefig(os.path.join(res_path, \"roc_curve.png\"))\n",
    "    plt.close()\n",
    "        \n",
    "def calculate_OvR_roc_auc_score(model, model_name, x, y, x_test, y_test, labels, res_path): #average??\n",
    "    y = one_hot_encode(y)\n",
    "    y_test = one_hot_encode(y_test)\n",
    "\n",
    "    ovr_model = OneVsRestClassifier(model).fit(x, y)\n",
    "    prob_test_vec = ovr_model.predict_proba(x_test)\n",
    "    \n",
    "    fpr, tpr, thresholds, auc_score = [], [], [], []\n",
    "    for _ in range(NUM_of_CLASSES):\n",
    "        fpr.append(0)\n",
    "        tpr.append(0)\n",
    "        thresholds.append(0)\n",
    "        auc_score.append(0)\n",
    "    \n",
    "    for i in range(NUM_of_CLASSES):\n",
    "        fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], prob_test_vec[:, i])\n",
    "        auc_score[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    averaged_auc_score = (sum(auc_score) / NUM_of_CLASSES)\n",
    "    # Save AUC to results.txt\n",
    "    with open(os.path.join(res_path, f\"{model_name}_results.txt\"), \"a\") as f:\n",
    "        f.write(f\"AUC score: {auc_score}\\n\")\n",
    "        f.write(f\"Averaged AUC score: {averaged_auc_score:.2f}\\n\")\n",
    "\n",
    "    plot_roc_curve(prob_test_vec, y_test, labels, res_path=res_path)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "def plot_feature_imp(model, res_path):\n",
    "    processed_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/processed\"\n",
    "    vect_file_path = os.path.join(processed_folder_path, 'tfidf_vectorizer.joblib')\n",
    "    loaded_tfidf_vectorizer = load(vect_file_path)\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = loaded_tfidf_vectorizer.get_feature_names_out()\n",
    "    feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    feature_importances.nlargest(20).plot.bar(ax=ax)\n",
    "    ax.set_title(\"Top 20 Most Predictive Features\")\n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Importance')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(res_path, \"feature_importance.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_model(y_pred, model_name, x, y, params, labels, res_path, only_metrics, model=None):\n",
    "    if not os.path.exists(res_path):\n",
    "        os.makedirs(res_path)\n",
    "\n",
    "    with open(os.path.join(res_path, f\"{model_name}_results.txt\"), \"w\") as f:\n",
    "        f.write(f\"*{model_name}\\n\")\n",
    "        f.write(f\"Params: {params}\\n\\n\")\n",
    "\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(y, y_pred)\n",
    "        f.write(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
    "        f.write(f\"Precision: {precision:.2f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.2f}\\n\")\n",
    "        f.write(f\"f1-score: {f1:.2f}\\n\\n\")\n",
    "\n",
    "        if not only_metrics:\n",
    "            report = calculate_classification_report(y, y_pred)\n",
    "            f.write(\"Classification Report:\\n\")\n",
    "            f.write(report)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            plot_confusion_matrix(y, y_pred, labels=labels, res_path=res_path)\n",
    "\n",
    "            if model_name == 'RF':\n",
    "                plot_feature_imp(model, res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top3_models(top3_models):    \n",
    "    # Print the sorted list of mean test scores and standard deviation of test scores\n",
    "    print(\"\\nTop 3 parameter combinations ranked by performance (from best to worst):\")\n",
    "    for index, row in top3_models.iterrows():\n",
    "        mean_score = row['mean_test_score']\n",
    "        std_score = row['std_test_score']\n",
    "        params = row['params']\n",
    "        print(f\"Mean Test Score: {mean_score:.4f} (±{std_score:.4f}) for {params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top3_models(top3_models):  \n",
    "    print(\"*Printing top 3 models...\")\n",
    "    # Print the sorted list of mean test scores and standard deviation of test scores\n",
    "    print(\"Top 3 parameter combinations ranked by performance (from best to worst):\")\n",
    "    for index, row in top3_models.iterrows():\n",
    "        mean_score = row['mean_test_score']\n",
    "        std_score = row['std_test_score']\n",
    "        params = row['params']\n",
    "        print(f\"Mean Test Score: {mean_score:.4f} (±{std_score:.4f}) for {params}\")\n",
    "\n",
    "\n",
    "def perform_grid_search(model, param_grid, x_train, y_train):\n",
    "    print(\"*Performing grid search...\")\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=5)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "    top3_models = sorted_results[:3]\n",
    "    print_top3_models(top3_models)\n",
    "    top3_params = top3_models['params'].values\n",
    "\n",
    "    return top3_params\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(model_type, top3_params, x_train, y_train, x_val, y_val):\n",
    "    print(\"*Training and Evaluating Top 3 Models...\")\n",
    "    trained_models = []\n",
    "    best_accuracy = 0\n",
    "    for i in range(3):\n",
    "        if model_type == \"RF\":\n",
    "            model = RandomForestClassifier(**top3_params[i])\n",
    "        elif model_type == \"NB\":\n",
    "            model = MultinomialNB(**top3_params[i])\n",
    "        elif model_type == \"SVM\":\n",
    "            model = SVC(**top3_params[i])\n",
    "        else:\n",
    "            print(f\"Unknown model type: {model_type}\")\n",
    "            return\n",
    "        model.fit(x_train, y_train)\n",
    "        # Get accuracy for the validation set (.score calls .predict() internally)\n",
    "        val_accuracy = model.score(x_val, y_val)\n",
    "        if val_accuracy > best_accuracy:\n",
    "            # Store the best model\n",
    "            best_model = model\n",
    "            best_params = top3_params[i]\n",
    "            best_accuracy = val_accuracy\n",
    "            idx = i\n",
    "        trained_models.append((model, top3_params[i]))\n",
    "    \n",
    "    print(f\"Model {idx}-{best_params} gives highest validation accuracy {best_accuracy:.2f}%\")\n",
    "\n",
    "    # Return the fitted models and their respective params for more in-depth evaluation\n",
    "    return trained_models, best_model, best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Performing grid search...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Printing top 3 models...\n",
      "Top 3 parameter combinations ranked by performance (from best to worst):\n",
      "Mean Test Score: 0.8100 (±0.0063) for {'alpha': 0.001, 'fit_prior': False}\n",
      "Mean Test Score: 0.8034 (±0.0067) for {'alpha': 0.01, 'fit_prior': False}\n",
      "Mean Test Score: 0.8028 (±0.0080) for {'alpha': 0.001, 'fit_prior': True}\n",
      "*Training and Evaluating Top 3 Models...\n",
      "Model 2-{'alpha': 0.001, 'fit_prior': True} gives highest validation accuracy 0.84%\n",
      "0\n",
      "-Training: \n",
      "Accuracy: 0.86%, Precision: 0.86, Recall: 0.86, f1-score: 0.86\n",
      "-Validation:\n",
      "Accuracy: 0.80%, Precision: 0.86, Recall: 0.80, f1-score: 0.82\n",
      "1\n",
      "-Training: \n",
      "Accuracy: 0.85%, Precision: 0.85, Recall: 0.85, f1-score: 0.85\n",
      "-Validation:\n",
      "Accuracy: 0.80%, Precision: 0.86, Recall: 0.80, f1-score: 0.82\n",
      "2\n",
      "-Training: \n",
      "Accuracy: 0.85%, Precision: 0.85, Recall: 0.85, f1-score: 0.84\n",
      "-Validation:\n",
      "Accuracy: 0.84%, Precision: 0.86, Recall: 0.84, f1-score: 0.85\n",
      "\n",
      "*Best model: MultinomialNB(alpha=0.001)\n",
      "[   0 1281  856 9790]\n",
      "Accuracy: 0.82%, Precision: 0.83, Recall: 0.82, f1-score: 0.82\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "nb_param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1],  # Smoothing parameter for MultinomialNB\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "top3_params = perform_grid_search(nb, nb_param_grid, x_train_tfidf, y_train) # Save top 3 models\n",
    "\n",
    "# Fit the top 3 models, find the model among top 3 with the highest validation accuracy, and store it\n",
    "trained_models, nb_best_model, nb_best_params = train_and_evaluate_models(\"NB\", top3_params, x_train_tfidf, y_train, x_val_tfidf, y_val)\n",
    "\n",
    "# Evaluate and print metrics for each model in fitted models for more in-depth analysis\n",
    "subfolder_path = \"NB_results/NB_trained\"\n",
    "res_path = os.path.join(results_folder_path, subfolder_path)\n",
    "for i, (model, params) in enumerate(trained_models):\n",
    "    y_pred = model.predict(x_train_tfidf)\n",
    "    print(i)\n",
    "    print(\"-Training: \")\n",
    "    evaluate_model(y_pred, f\"Training-NB-{i}\", x_train_tfidf, y_train, params, senti_labels, res_path, only_metrics=True)\n",
    "    y_pred = model.predict(x_val_tfidf)\n",
    "    print(\"-Validation:\")\n",
    "    evaluate_model(y_pred, f\"Validation-NB-{i}\", x_val_tfidf, y_val, params, senti_labels, res_path, only_metrics=True)\n",
    "print()\n",
    "\n",
    "# Use the best model to evaluate on the test set\n",
    "print(f\"*Best model: {nb_best_model}\")\n",
    "y_pred = nb_best_model.predict(x_test_tfidf)\n",
    "print(np.bincount(y_pred))\n",
    "subfolder_path = \"NB_results/NB_best\"\n",
    "res_path = os.path.join(results_folder_path, \"NB_results/NB_best\")\n",
    "model_type = \"NB-best\"\n",
    "evaluate_model(y_pred, model_type, x_test_tfidf, y_test, nb_best_params, senti_labels, res_path, only_metrics=False)\n",
    "calculate_OvR_roc_auc_score(nb_best_model, model_type, x_train_tfidf, y_train, x_test_tfidf, y_test, senti_labels, res_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Performing grid search...\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END ......alpha=0.001, fit_prior=False;, score=0.801 total time=   0.0s\n",
      "[CV 3/3] END ........alpha=0.01, fit_prior=True;, score=0.801 total time=   0.0s\n",
      "[CV 3/3] END .........alpha=0.1, fit_prior=True;, score=0.780 total time=   0.0s\n",
      "[CV 3/3] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.721 total time= 1.6min\n",
      "[CV 3/3] END .....C=0.1, gamma=1, kernel=linear;, score=0.721 total time= 1.6min\n",
      "[CV 3/3] END .C=0.1, gamma=scale, kernel=linear;, score=0.721 total time= 1.5min\n",
      "[CV 2/3] END .....C=1, gamma=0.1, kernel=linear;, score=0.854 total time= 1.8min\n",
      "[CV 3/3] END .......C=1, gamma=1, kernel=linear;, score=0.847 total time= 1.7min\n",
      "[CV 2/3] END ...C=1, gamma=scale, kernel=linear;, score=0.854 total time= 1.8min\n",
      "[CV 2/3] END ......C=1, gamma=scale, kernel=rbf;, score=0.954 total time= 3.8min\n",
      "[CV 1/3] END ......C=10, gamma=1, kernel=linear;, score=0.871 total time= 2.4min\n",
      "[CV 2/3] END .........C=10, gamma=1, kernel=rbf;, score=0.971 total time= 3.7min\n",
      "[CV 3/3] END .......alpha=0.001, fit_prior=True;, score=0.808 total time=   0.0s\n",
      "[CV 3/3] END .......alpha=0.01, fit_prior=False;, score=0.810 total time=   0.0s\n",
      "[CV 3/3] END ........alpha=0.1, fit_prior=False;, score=0.792 total time=   0.0s\n",
      "[CV 3/3] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.537 total time= 2.1min\n",
      "[CV 3/3] END ........C=0.1, gamma=1, kernel=rbf;, score=0.630 total time= 2.1min\n",
      "[CV 3/3] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.630 total time= 2.1min\n",
      "[CV 1/3] END .......C=1, gamma=1, kernel=linear;, score=0.840 total time= 1.5min\n",
      "[CV 2/3] END ..........C=1, gamma=1, kernel=rbf;, score=0.954 total time= 3.8min\n",
      "[CV 3/3] END ....C=10, gamma=0.1, kernel=linear;, score=0.891 total time= 2.7min\n",
      "[CV 3/3] END ......C=10, gamma=1, kernel=linear;, score=0.891 total time= 2.7min\n",
      "[CV 3/3] END ..C=10, gamma=scale, kernel=linear;, score=0.891 total time= 2.8min\n",
      "[CV 1/3] END .......alpha=0.001, fit_prior=True;, score=0.791 total time=   0.0s\n",
      "[CV 1/3] END ........alpha=0.01, fit_prior=True;, score=0.784 total time=   0.0s\n",
      "[CV 2/3] END .........alpha=0.1, fit_prior=True;, score=0.781 total time=   0.0s\n",
      "[CV 2/3] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.717 total time= 1.6min\n",
      "[CV 2/3] END .....C=0.1, gamma=1, kernel=linear;, score=0.717 total time= 1.6min\n",
      "[CV 2/3] END .C=0.1, gamma=scale, kernel=linear;, score=0.717 total time= 1.6min\n",
      "[CV 3/3] END .....C=1, gamma=0.1, kernel=linear;, score=0.847 total time= 1.7min\n",
      "[CV 2/3] END .......C=1, gamma=1, kernel=linear;, score=0.854 total time= 1.8min\n",
      "[CV 3/3] END ...C=1, gamma=scale, kernel=linear;, score=0.847 total time= 1.7min\n",
      "[CV 3/3] END ......C=1, gamma=scale, kernel=rbf;, score=0.948 total time= 3.8min\n",
      "[CV 3/3] END .......C=10, gamma=0.1, kernel=rbf;, score=0.892 total time= 2.5min\n",
      "[CV 3/3] END .........C=10, gamma=1, kernel=rbf;, score=0.972 total time= 3.6min\n",
      "[CV 3/3] END ......alpha=0.001, fit_prior=False;, score=0.816 total time=   0.0s\n",
      "[CV 2/3] END ........alpha=0.01, fit_prior=True;, score=0.802 total time=   0.0s\n",
      "[CV 1/3] END .........alpha=0.1, fit_prior=True;, score=0.764 total time=   0.0s\n",
      "[CV 1/3] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.717 total time= 1.5min\n",
      "[CV 1/3] END .....C=0.1, gamma=1, kernel=linear;, score=0.717 total time= 1.5min\n",
      "[CV 1/3] END .C=0.1, gamma=scale, kernel=linear;, score=0.717 total time= 1.6min\n",
      "[CV 1/3] END .....C=1, gamma=0.1, kernel=linear;, score=0.840 total time= 1.5min\n",
      "[CV 1/3] END ........C=1, gamma=0.1, kernel=rbf;, score=0.768 total time= 1.7min\n",
      "[CV 1/3] END ..........C=1, gamma=1, kernel=rbf;, score=0.927 total time= 3.6min\n",
      "[CV 1/3] END ....C=10, gamma=0.1, kernel=linear;, score=0.871 total time= 2.4min\n",
      "[CV 2/3] END .......C=10, gamma=0.1, kernel=rbf;, score=0.894 total time= 2.5min\n",
      "[CV 1/3] END ..C=10, gamma=scale, kernel=linear;, score=0.871 total time= 2.4min\n",
      "[CV 1/3] END .....C=10, gamma=scale, kernel=rbf;, score=0.942 total time= 3.6min\n",
      "[CV 2/3] END ......alpha=0.001, fit_prior=False;, score=0.813 total time=   0.0s\n",
      "[CV 2/3] END .......alpha=0.01, fit_prior=False;, score=0.807 total time=   0.0s\n",
      "[CV 2/3] END ........alpha=0.1, fit_prior=False;, score=0.792 total time=   0.0s\n",
      "[CV 2/3] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.537 total time= 2.1min\n",
      "[CV 2/3] END ........C=0.1, gamma=1, kernel=rbf;, score=0.629 total time= 2.1min\n",
      "[CV 2/3] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.629 total time= 2.1min\n",
      "[CV 3/3] END ........C=1, gamma=0.1, kernel=rbf;, score=0.770 total time= 1.7min\n",
      "[CV 1/3] END ...C=1, gamma=scale, kernel=linear;, score=0.840 total time= 1.6min\n",
      "[CV 1/3] END ......C=1, gamma=scale, kernel=rbf;, score=0.927 total time= 3.6min\n",
      "[CV 1/3] END .......C=10, gamma=0.1, kernel=rbf;, score=0.878 total time= 2.5min\n",
      "[CV 1/3] END .........C=10, gamma=1, kernel=rbf;, score=0.942 total time= 3.5min\n",
      "[CV 2/3] END .....C=10, gamma=scale, kernel=rbf;, score=0.971 total time= 3.4min\n",
      "[CV 2/3] END .......alpha=0.001, fit_prior=True;, score=0.809 total time=   0.0s\n",
      "[CV 1/3] END .......alpha=0.01, fit_prior=False;, score=0.794 total time=   0.0s\n",
      "[CV 1/3] END ........alpha=0.1, fit_prior=False;, score=0.776 total time=   0.0s\n",
      "[CV 1/3] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.537 total time= 2.1min\n",
      "[CV 1/3] END ........C=0.1, gamma=1, kernel=rbf;, score=0.634 total time= 2.1min\n",
      "[CV 1/3] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.634 total time= 2.1min\n",
      "[CV 2/3] END ........C=1, gamma=0.1, kernel=rbf;, score=0.772 total time= 1.8min\n",
      "[CV 3/3] END ..........C=1, gamma=1, kernel=rbf;, score=0.948 total time= 3.6min\n",
      "[CV 2/3] END ....C=10, gamma=0.1, kernel=linear;, score=0.893 total time= 2.6min\n",
      "[CV 2/3] END ......C=10, gamma=1, kernel=linear;, score=0.893 total time= 2.6min\n",
      "[CV 2/3] END ..C=10, gamma=scale, kernel=linear;, score=0.893 total time= 2.6min\n",
      "[CV 3/3] END .....C=10, gamma=scale, kernel=rbf;, score=0.972 total time= 3.5min\n",
      "*Printing top 3 models...\n",
      "Top 3 parameter combinations ranked by performance (from best to worst):\n",
      "Mean Test Score: 0.9615 (±0.0140) for {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Mean Test Score: 0.9615 (±0.0140) for {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "Mean Test Score: 0.9432 (±0.0118) for {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "*Training and Evaluating Top 3 Models...\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "svm_param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': [0.1, 1, 'scale']\n",
    "}\n",
    "\n",
    "top3_params = perform_grid_search(svm, svm_param_grid, x_train_tfidf, y_train) # Save top 3 models\n",
    "\n",
    "# Fit the top 3 models, find the model among top 3 with the highest validation accuracy, and store it\n",
    "trained_models, svm_best_model, svm_best_params = train_and_evaluate_models(\"SVM\", top3_params, x_train_tfidf, y_train, x_val_tfidf, y_val)\n",
    "\n",
    "# Evaluate and print metrics for each model in fitted models for more in-depth analysis\n",
    "subfolder_path = \"SVM_results/SVM_trained\"\n",
    "res_path = os.path.join(results_folder_path, subfolder_path)\n",
    "for i, (model, params) in enumerate(trained_models):\n",
    "    y_pred = model.predict(x_train_tfidf)\n",
    "    print(i)\n",
    "    print(\"-Training: \")\n",
    "    evaluate_model(y_pred, f\"Training-SVM-{i}\", x_train_tfidf, y_train, params, senti_labels, res_path, only_metrics=True)\n",
    "    y_pred = model.predict(x_val_tfidf)\n",
    "    print(\"-Validation:\")\n",
    "    evaluate_model(y_pred, f\"Validation-SVM-{i}\", x_val_tfidf, y_val, params, senti_labels, res_path, only_metrics=True)\n",
    "print()\n",
    "\n",
    "# Use the best model to evaluate on the test set\n",
    "print(f\"*Best model: {svm_best_model}\")\n",
    "y_pred = svm_best_model.predict(x_test_tfidf)\n",
    "print(np.bincount(y_pred))\n",
    "subfolder_path = \"SVM_results/SVM_best\"\n",
    "res_path = os.path.join(results_folder_path, subfolder_path)\n",
    "model_type = \"SVM-best\"\n",
    "evaluate_model(y_pred, model_type, x_test_tfidf, y_test, svm_best_params, senti_labels, res_path, only_metrics=False)\n",
    "#calculate_OvR_roc_auc_score(svm_best_model, model_type, x_train_tfidf, y_train, x_test_tfidf, y_test, senti_labels, res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://node40:8888/'. Verify the server is running and reachable. (The Jupyter notebook server failed to launch in time)."
     ]
    }
   ],
   "source": [
    "#calculate_OvR_roc_auc_score(svm_best_model, model_type, x_train_tfidf, y_train, x_test_tfidf, y_test, senti_labels, res_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://node40:8888/'. Verify the server is running and reachable. (The Jupyter notebook server failed to launch in time)."
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "rf_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 3, 5, 7, 10],  # Limit maximum depth of the trees\n",
    "        'min_samples_split': [2, 5, 10, 20],  # Higher values will prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "        'min_samples_leaf': [1, 2, 5, 10, 15],  # Higher values prevent a model from getting too complex\n",
    "}\n",
    "\n",
    "top3_params = perform_grid_search(rf, rf_param_grid, x_train_tfidf, y_train) # Save top 3 models\n",
    "\n",
    "# Fit the top 3 models, find the model among top 3 with the highest validation accuracy, and store it\n",
    "trained_models, rf_best_model, rf_best_params = train_and_evaluate_models(\"RF\", top3_params, x_train_tfidf, y_train, x_val_tfidf, y_val)\n",
    "\n",
    "# Evaluate and print metrics for each model in fitted models for more in-depth analysis\n",
    "subfolder_path = \"RF_results/RF_trained\"\n",
    "res_path = os.path.join(results_folder_path, subfolder_path)\n",
    "for i, (model, params) in enumerate(trained_models):\n",
    "    y_pred = model.predict(x_train_tfidf)\n",
    "    print(i)\n",
    "    print(\"-Training: \")\n",
    "    evaluate_model(y_pred, f\"Training-RF-{i}\", x_train_tfidf, y_train, params, senti_labels, res_path, only_metrics=True, model=model)\n",
    "    y_pred = model.predict(x_val_tfidf)\n",
    "    print(\"-Validation:\")\n",
    "    evaluate_model(y_pred, f\"Validation-RF-{i}\", x_val_tfidf, y_val, params, senti_labels, res_path, only_metrics=True, model=model)\n",
    "print()\n",
    "\n",
    "# Use the best model to evaluate on the test set\n",
    "print(f\"*Best model: {rf_best_model}\")\n",
    "y_pred = rf_best_model.predict(x_test_tfidf)\n",
    "print(np.bincount(y_pred))\n",
    "subfolder_path =  \"RF_results/RF_best\"\n",
    "res_path = os.path.join(results_folder_path, subfolder_path)\n",
    "model_type = \"RF-best\"\n",
    "evaluate_model(y_pred, model_type, x_test_tfidf, y_test, rf_best_params, senti_labels, res_path, only_metrics=False, model=rf_best_model)\n",
    "calculate_OvR_roc_auc_score(rf_best_model, model_type, x_train_tfidf, y_train, x_test_tfidf, y_test, senti_labels, res_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
