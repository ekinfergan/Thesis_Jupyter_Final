{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Channel LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 01:51:25.949447: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from numpy import asarray\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, RocCurveDisplay, confusion_matrix, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.legacy import Adam, SGD, RMSprop, Adagrad\n",
    "\n",
    "import skopt\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer  \n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "# DATASET\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "# Define a dictionary to map sentiment values to category names\n",
    "senti_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_categories = list(senti_labels.values())\n",
    "NUM_of_CLASSES = 3\n",
    "\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/\"\n",
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# just checkıng gpu ıs avaılable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.2965     -0.25454     0.52863002 ... -0.57709002  0.11623\n",
      "   0.59446001]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.089755    0.045177    0.14989001 ... -0.12161    -1.33650005\n",
      "  -0.012269  ]]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(input_folder_path, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(input_folder_path, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(input_folder_path, \"test.csv\"))\n",
    "\n",
    "x_train = train['x']\n",
    "y_train = train['y']\n",
    "x_val = val['x']\n",
    "y_val = val['y']\n",
    "x_test = test['x']\n",
    "y_test = test['y']\n",
    "\n",
    "x_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_x.npy\"))\n",
    "y_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_y.npy\"))\n",
    "x_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_x.npy\"))\n",
    "y_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_y.npy\"))\n",
    "x_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_x.npy\"))\n",
    "y_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_y.npy\"))\n",
    "\n",
    "w2v_embedding_vectors = np.load(os.path.join(processed_folder_path, \"embedding_matrix.npy\"))\n",
    "print(w2v_embedding_vectors)\n",
    "\n",
    "%store -r embedding_vocab_size\n",
    "%store -r EMBEDDING_DIM\n",
    "%store -r max_seq_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(score):\n",
    "    acc =  score[1]\n",
    "    loss = score[0]\n",
    "\n",
    "    print(f\"Accuracy: {acc:.2%}\")\n",
    "    print(f\"Loss: {loss:.2f}\")\n",
    "    \n",
    "    return acc, loss\n",
    "\n",
    "def calculate_classification_report(y, y_pred, labels):\n",
    "    report = classification_report(y, y_pred, labels=labels)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
    "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=labels)\n",
    "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, model_name, x_encoded, y_encoded, y=None, only_metrics=True):    \n",
    "    y_pred_prob = model.predict(x_encoded)\n",
    "\n",
    "    print(f\"*{model_name}\")\n",
    "    \n",
    "    score = model.evaluate(x_encoded, y_encoded, verbose=0)\n",
    "    calculate_metrics(score)\n",
    "    \n",
    "    senti_labels = ['negative', 'neutral', 'positive'] #TODO: to constants\n",
    "    \n",
    "    if not only_metrics:\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1) + 1\n",
    "        calculate_classification_report(y, y_pred, labels=senti_labels)\n",
    "        plot_confusion_matrix(y, y_pred, labels=senti_labels)\n",
    "    \n",
    "    print()\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded\n",
    "\n",
    "def plot_roc_curve(prob_test_vec, y_test, labels):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels\n",
    "    colors = cycle(['limegreen', 'dodgerblue', 'red'])\n",
    "    for senti, color in zip(range(NUM_of_CLASSES), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_test[:, senti],\n",
    "            prob_test_vec[:, senti],\n",
    "            name=f\"ROC curve for {labels[senti]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )\n",
    "    \n",
    "def calculate_OvR_roc_auc_score(model, x, y, x_test, y_test, labels): #average??\n",
    "    #y = one_hot_encode(y)\n",
    "    #y_test = one_hot_encode(y_test)\n",
    "\n",
    "    ovr_model = OneVsRestClassifier(model).fit(x, y)\n",
    "    prob_test_vec = ovr_model.predict_proba(x_test)\n",
    "    \n",
    "    fpr, tpr, thresholds, auc_score = [], [], [], []\n",
    "    for _ in range(NUM_of_CLASSES):\n",
    "        fpr.append(0)\n",
    "        tpr.append(0)\n",
    "        thresholds.append(0)\n",
    "        auc_score.append(0)\n",
    "    \n",
    "    for i in range(NUM_of_CLASSES):\n",
    "        fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], prob_test_vec[:, i])\n",
    "        auc_score[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    print(f\"AUC score: {auc_score}\")\n",
    "    averaged_auc_score = (sum(auc_score) / NUM_of_CLASSES)\n",
    "    print(f\"Averaged AUC score: {averaged_auc_score:.2f}\")\n",
    "    \n",
    "    plot_roc_curve(prob_test_vec, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_development(history):\n",
    "    acc =  history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "    \n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_classes = 3\n",
    "batch_size= 16\n",
    "epochs=30\n",
    "\n",
    "num_gru_layersA = Integer(low=1, high=3, name='num_gru_layersA')\n",
    "num_gru_layersB = Integer(low=1, high=3, name='num_gru_layersB')\n",
    "num_gru_unitsA = Categorical([16, 32, 64, 128], name='num_gru_unitsA') \n",
    "num_gru_unitsB = Categorical([16, 32, 64, 128], name='num_gru_unitsB') \n",
    "learning_rate = Categorical([1e-4, 1e-3, 1e-2], name='learning_rate')\n",
    "adam_decay = Categorical([1e-6, 1e-4, 1e-2], name=\"adam_decay\")\n",
    "\n",
    "search_space = [\n",
    "            num_gru_layersA,\n",
    "            num_gru_layersB,\n",
    "            num_gru_unitsA,\n",
    "            num_gru_unitsB,\n",
    "            learning_rate,\n",
    "            adam_decay\n",
    "            ]\n",
    "\n",
    "# Specify one or more initial points for the search of optimal parameter\n",
    "default_params = [1, \n",
    "                  1, \n",
    "                  16,\n",
    "                  16, \n",
    "                  1e-4,\n",
    "                  1e-6 \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_multi_channel_gru_model(num_gru_layersA, num_gru_layersB, num_gru_unitsA, num_gru_unitsB, learning_rate, adam_decay):\n",
    "    # Vocabulary-based embedding layer\n",
    "    inputsA = Input(shape=(max_seq_length,), name=\"input regular embeddings\")\n",
    "    # Word2Vec embedding layer\n",
    "    inputsB = Input(shape=(max_seq_length,), name=\"input word2vec embeddings\")\n",
    "    \n",
    "    # Define an embedding layer for each input\n",
    "    embeddingsA = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, name=\"embeddingsA\")(inputsA)\n",
    "    embeddingsB = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, weights=[w2v_embedding_vectors], trainable=False, name=\"embeddingsB\")(inputsB)\n",
    "    \n",
    "    # Pass both embeddings through their own LSTM layers\n",
    "    gru_layersA = embeddingsA\n",
    "    for i in range(num_gru_layersA):\n",
    "        nameA = 'layer_gruA_{0}'.format(i+1)\n",
    "        if i < num_gru_layersA-1:\n",
    "            gru_layersA = LSTM(num_gru_unitsA, return_sequences=True, name=nameA)(gru_layersA)\n",
    "        else:\n",
    "            gru_layersA = LSTM(num_gru_unitsA, return_sequences=False, name=nameA)(gru_layersA)\n",
    "        \n",
    "    gru_layersB = embeddingsB\n",
    "    for i in range(num_gru_layersB):\n",
    "        nameA = 'layer_gruB_{0}'.format(i+1)\n",
    "        if i < num_gru_layersB-1:\n",
    "            gru_layersB = LSTM(num_gru_unitsB, return_sequences=True, name=nameA)(gru_layersB)\n",
    "        else:\n",
    "            gru_layersB = LSTM(num_gru_unitsB, return_sequences=False, name=nameA)(gru_layersB)\n",
    "        \n",
    "\n",
    "    # Concatenate the two inputs\n",
    "    merged = concatenate([gru_layersA, gru_layersB])\n",
    "\n",
    "    # Dense layer for the merged inputs & output Layer\n",
    "    outputs = Dense(num_output_classes, activation='softmax', name=\"output\")(merged)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[inputsA, inputsB], outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    adam = Adam(learning_rate=learning_rate, decay=adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=search_space)\n",
    "def multi_objective_function(num_gru_layersA, num_gru_layersB, num_gru_unitsA, num_gru_unitsB, learning_rate, adam_decay):\n",
    "\n",
    "    model = define_multi_channel_gru_model(num_gru_layersA=num_gru_layersA,\n",
    "                                            num_gru_layersB=num_gru_layersB,\n",
    "                                            num_gru_unitsA=num_gru_unitsA,\n",
    "                                            num_gru_unitsB=num_gru_unitsB,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            adam_decay=adam_decay\n",
    "                                            )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "    history = model.fit([x_train_encoded, x_train_encoded],\n",
    "                        y_train_encoded,\n",
    "                        validation_data=([x_val_encoded, x_val_encoded], y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=2\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Loss: {loss:.2}\\n\")\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 01:51:33.597641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 36469 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:e3:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input regular embeddings (Inpu  [(None, 659)]       0           []                               \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input word2vec embeddings (Inp  [(None, 659)]       0           []                               \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " embeddingsA (Embedding)        (None, 659, 100)     2480700     ['input regular embeddings[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " embeddingsB (Embedding)        (None, 659, 100)     2480700     ['input word2vec embeddings[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " layer_gruA_1 (LSTM)            (None, 16)           7488        ['embeddingsA[0][0]']            \n",
      "                                                                                                  \n",
      " layer_gruB_1 (LSTM)            (None, 16)           7488        ['embeddingsB[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32)           0           ['layer_gruA_1[0][0]',           \n",
      "                                                                  'layer_gruB_1[0][0]']           \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 3)            99          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,976,475\n",
      "Trainable params: 2,495,775\n",
      "Non-trainable params: 2,480,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 01:51:40.328513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-06-16 01:51:40.555947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "gbrt_result = gbrt_minimize(func=multi_objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
    "print(\"Best Hyperparameters:\", gbrt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_gru_model(gbrt_best_params['num_gru_layersA'],\n",
    "                                        gbrt_best_params['num_gru_layersB'],\n",
    "                                        gbrt_best_params['num_gru_unitsA'], \n",
    "                                        gbrt_best_params['num_gru_unitsB'],\n",
    "                                        gbrt_best_params['learning_rate'], \n",
    "                                        gbrt_best_params['adam_decay'],\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit([x_train_encoded, x_train_encoded],\n",
    "                    y_train_encoded,\n",
    "                    validation_data=([x_val_encoded, x_val_encoded], y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=2\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([x_train_encoded, x_train_encoded], y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate([x_val_encoded, x_val_encoded], y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate([x_test_encoded, x_test_encoded], y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_result = gp_minimize(func=multi_objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gp_best_params = {param.name: value for param, value in zip(gp_result.space, gp_result.x)}\n",
    "print(\"Best Hyperparameters:\", gp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_gru_model(gp_best_params['num_gru_layersA'],\n",
    "                                        gp_best_params['num_gru_layersB'],\n",
    "                                        gp_best_params['num_gru_unitsA'], \n",
    "                                        gp_best_params['num_gru_unitsB'],\n",
    "                                        gp_best_params['learning_rate'], \n",
    "                                        gp_best_params['adam_decay']\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit([x_train_encoded, x_train_encoded],\n",
    "                    y_train_encoded,\n",
    "                    validation_data=([x_val_encoded, x_val_encoded], y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=2\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([x_train_encoded, x_train_encoded], y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate([x_val_encoded, x_val_encoded], y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate([x_test_encoded, x_test_encoded], y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
