{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "import gensim.downloader as api\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import pad_sequences\n",
    "from joblib import dump\n",
    "\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "NUM_of_CLASSES = 3\n",
    "w2v_pretrained_model = \"glove-twitter-100\"\n",
    "#w2v_pretrained_model = \"glove-wiki-gigaword-100\"\n",
    "w2v_pretrained_model_filename = str(w2v_pretrained_model) + \"-word2vec.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/s3985113/Thesis_Jupyter_Final/src/\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname(os.path.abspath('vectorizer.ipynb'))\n",
    "data_path = os.path.join(script_dir, 'Thesis_Jupyter_Final/src/')\n",
    "os.getcwd()\n",
    "print(data_path)\n",
    "\n",
    "input_folder_path = os.path.join(data_path, 'input')\n",
    "processed_folder_path = os.path.join(data_path, 'input/processed')\n",
    "results_folder_path = \"results\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(results_folder_path):\n",
    "    os.makedirs(results_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, process=True):\n",
    "    # Load data\n",
    "    data_file_path = os.path.join(input_folder_path, filename)\n",
    "    df = pd.read_csv(data_file_path)\n",
    "\n",
    "    x = df['x']\n",
    "    y = df['y']\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def load_vocab():\n",
    "    vocab_data_filename = \"vocab.pkl\"\n",
    "    file_path = os.path.join(processed_folder_path, vocab_data_filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    return vocab, vocab_size\n",
    "    \n",
    "\n",
    "x_train, y_train = load_data(\"train.csv\", process=False)\n",
    "x_val, y_val = load_data(\"val.csv\", process=False)\n",
    "x_test, y_test = load_data(\"test.csv\", process=False)\n",
    "\n",
    "vocab, vocab_size = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_npz(matrix, file_path):\n",
    "    scipy.sparse.save_npz(file_path, matrix)\n",
    "\n",
    "def save_to_npy(arr, file_path):\n",
    "    np.save(file_path, np.array(arr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Shape (doc, vocab_size):\n",
      "* train: (41000, 11905)\n",
      "* validation: (11529, 11905)\n",
      "* test: (11899, 11905)\n",
      "\n",
      "x_train_tfidf:\n",
      "  (0, 7674)\t0.41401673971112746\n",
      "  (0, 3005)\t0.34153855159235935\n",
      "  (0, 1288)\t0.2701998993071273\n",
      "  (0, 1188)\t0.28300945321786775\n",
      "  (0, 1101)\t0.2506496977415895\n",
      "  (0, 1049)\t0.27107612519882957\n",
      "  (0, 991)\t0.2660827031141531\n",
      "  (0, 415)\t0.22753195491195433\n",
      "  (0, 290)\t0.1974816461099669\n",
      "  (0, 277)\t0.21180699919929327\n",
      "  (0, 251)\t0.19520146633712748\n",
      "  (0, 228)\t0.21589497141966651\n",
      "  (0, 216)\t0.22556793582011625\n",
      "  (0, 139)\t0.21066367388175217\n",
      "  (0, 93)\t0.1900667120183632\n",
      "  (1, 1588)\t0.5935472288937115\n",
      "  (1, 586)\t0.44068854266770097\n",
      "  (1, 475)\t0.49109821654100483\n",
      "  (1, 450)\t0.4607795971440773\n",
      "  (2, 2300)\t0.438158529659526\n",
      "  (2, 992)\t0.4115649106107287\n",
      "  (2, 880)\t0.39367520419393415\n",
      "  (2, 681)\t0.40500932727012673\n",
      "  (2, 471)\t0.33518988777153336\n",
      "  (2, 307)\t0.3415349809089223\n",
      "  :\t:\n",
      "  (40997, 28)\t0.32303288191390067\n",
      "  (40998, 1271)\t0.3193570093028152\n",
      "  (40998, 745)\t0.5690133809196177\n",
      "  (40998, 558)\t0.28449706310274575\n",
      "  (40998, 481)\t0.2678001638795765\n",
      "  (40998, 189)\t0.24047688856049013\n",
      "  (40998, 166)\t0.22583050752347397\n",
      "  (40998, 163)\t0.24554675923380392\n",
      "  (40998, 144)\t0.2384437415807572\n",
      "  (40998, 119)\t0.22932461847406876\n",
      "  (40998, 104)\t0.22194175619676\n",
      "  (40998, 70)\t0.21121999835857666\n",
      "  (40998, 42)\t0.22167628163088773\n",
      "  (40999, 7626)\t0.39095943893851726\n",
      "  (40999, 5585)\t0.386960796644919\n",
      "  (40999, 1827)\t0.3194384551826338\n",
      "  (40999, 1412)\t0.3172102861230227\n",
      "  (40999, 1041)\t0.2808441200832876\n",
      "  (40999, 920)\t0.2713609279824924\n",
      "  (40999, 723)\t0.3042463001944778\n",
      "  (40999, 554)\t0.26969681222777636\n",
      "  (40999, 154)\t0.22245614740090622\n",
      "  (40999, 139)\t0.2184918795374275\n",
      "  (40999, 107)\t0.20976768607209423\n",
      "  (40999, 41)\t0.1889669042559358\n"
     ]
    }
   ],
   "source": [
    "def get_tfidf_vectorizer(vocab, max_features, min_df, max_df):\n",
    "    # Convert vocab to a dict in order to use it in TF-IDF vectorizer\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features, # maximum number of features to keep, check unique vocabs and determine based on that, high causes saprse metrics and low value causes loss in important words/vocab\n",
    "        vocabulary=vocab_dict,\n",
    "        lowercase=False,\n",
    "        ngram_range=(1, 3),  # range of n-grams,\n",
    "        max_df=max_df,  # ignore terms that have a document frequency strictly higher than the threshold\n",
    "        min_df=min_df,  # ignore terms that have a document frequency strictly lower than the threshold.\n",
    "        use_idf=True,  # enable IDF weighting\n",
    "        smooth_idf=True,  # smooth IDF weights --> provides stability, reduces run time errors\n",
    "        sublinear_tf=True  # apply sublinear scaling to term frequencies\n",
    "    )\n",
    "\n",
    "    # Save tfidf vectorizer\n",
    "    file_path = os.path.join(processed_folder_path, 'tfidf_vectorizer.joblib')\n",
    "    dump(tfidf_vectorizer, file_path)\n",
    "\n",
    "    return tfidf_vectorizer\n",
    "\n",
    "def transform_to_tfidf(x_train, x_val, x_test):\n",
    "    # Fit and transform the training set\n",
    "    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "    # Transform the validation and testing set\n",
    "    x_val_tfidf = tfidf_vectorizer.transform(x_val)\n",
    "    x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "    # Save data\n",
    "    save_to_npz(x_train_tfidf, os.path.join(processed_folder_path, \"train_tfidf.npz\"))\n",
    "    save_to_npz(x_val_tfidf, os.path.join(processed_folder_path, \"val_tfidf.npz\"))\n",
    "    save_to_npz(x_test_tfidf, os.path.join(processed_folder_path, \"test_tfidf.npz\"))\n",
    "\n",
    "    return x_train_tfidf, x_val_tfidf, x_test_tfidf\n",
    "\n",
    "\n",
    "max_features = 10000\n",
    "max_df = 0.95\n",
    "min_df = 5\n",
    "\n",
    "tfidf_vectorizer = get_tfidf_vectorizer(vocab, max_features, min_df, max_df)\n",
    "x_train_tfidf, x_val_tfidf, x_test_tfidf = transform_to_tfidf(x_train, x_val, x_test)\n",
    "\n",
    "print(\"\\nData Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_tfidf.shape, x_val_tfidf.shape, x_test_tfidf.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: delete\n",
    "def save_tfidf_data(data, filename, feature_names):\n",
    "    # Save the matrix with feature names as a DataFrame\n",
    "    data = pd.DataFrame(data.toarray(), columns=feature_names)\n",
    "    file_path = os.path.join(processed_folder_path, filename)\n",
    "    data.to_csv(file_path, sep=',', index=False) # TODO: if this isn't working, note that you added sep=','\n",
    "\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Save vectorized data\n",
    "#save_tfidf_data(x_train_tfidf, \"train_tfidf.csv\", feature_names)\n",
    "#save_tfidf_data(x_val_tfidf, \"val_tfidf.csv\", feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 430\n"
     ]
    }
   ],
   "source": [
    "def find_max_seq_len(data):\n",
    "    # Find maximum sequence length\n",
    "    max_seq_length = max([len(line.split()) for line in data])\n",
    "    print(f'Maximum review length: {max_seq_length}')\n",
    "\n",
    "    return max_seq_length\n",
    "\n",
    "max_seq_length = find_max_seq_len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Data Shape (doc, vocab_size):\n",
      "* train: (41000, 430)\n",
      "* validation: (11529, 430)\n",
      "* test: (11899, 430)\n",
      "\n",
      "x_train_tfidf:\n",
      "[[  96  561  851 ...    0    0    0]\n",
      " [ 423  240 1166 ...    0    0    0]\n",
      " [1205   62  299 ...    0    0    0]\n",
      " ...\n",
      " [1733  162 1602 ...    0    0    0]\n",
      " [2164  108   97 ...    0    0    0]\n",
      " [1358  118  663 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "def fit_tokenizer(data):\n",
    "    # Fit tokenizer (on training data)\n",
    "    tokenizer = Tokenizer()\n",
    "    # Remove default filters, including punctuation\n",
    "    tokenizer.filters = \"\"  \n",
    "    # Disable lowercase conversion\n",
    "    tokenizer.lower = False  \n",
    "    tokenizer.fit_on_texts(data) \n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def encode_text(lines, tokenizer, max_length, filename):\n",
    "    # Integer encode\n",
    "    encoded_seq = tokenizer.texts_to_sequences(lines)\n",
    "    # Pad the encoded sequences\n",
    "    padded = pad_sequences(encoded_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Save to np.array\n",
    "    save_to_npy(padded, os.path.join(processed_folder_path, filename))\n",
    "\n",
    "    return padded\n",
    "    \n",
    "    \n",
    "tokenizer = fit_tokenizer(x_train)\n",
    "\n",
    "# Encode Data\n",
    "x_train_encoded = encode_text(x_train, tokenizer, max_seq_length, \"x_train_encoded.npy\")\n",
    "x_val_encoded = encode_text(x_val, tokenizer, max_seq_length, \"x_val_encoded.npy\")\n",
    "x_test_encoded = encode_text(x_test, tokenizer, max_seq_length, \"x_test_encoded.npy\")\n",
    "\n",
    "print(\"\\nEncoded Data Shape (doc, vocab_size):\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_encoded.shape, x_val_encoded.shape, x_test_encoded.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y-Encoded Data Shape:\n",
      "* train: (41000, 3)\n",
      "* validation: (11529, 3)\n",
      "* test: (11899, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: can't remember if this is used somewhere else, if not save data inside function\n",
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded\n",
    "\n",
    "# Convert sentiment labels to one-hot encoding\n",
    "y_train_encoded = one_hot_encode(y_train)\n",
    "y_val_encoded = one_hot_encode(y_val)\n",
    "y_test_encoded = one_hot_encode(y_test)\n",
    "\n",
    "# Save y-encoded sets\n",
    "save_to_npy(y_train_encoded, os.path.join(processed_folder_path, \"y_train_encoded.npy\"))\n",
    "save_to_npy(y_val_encoded, os.path.join(processed_folder_path, \"y_val_encoded.npy\"))\n",
    "save_to_npy(y_test_encoded, os.path.join(processed_folder_path, \"y_test_encoded.npy\"))\n",
    "   \n",
    "print(\"\\ny-Encoded Data Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_vocab_size:  11354\n",
      "Words in tokenizer but not in vocab:  0\n"
     ]
    }
   ],
   "source": [
    "# Total vocabulary size plus 0 for unknown words\n",
    "embedding_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"embedding_vocab_size: \", embedding_vocab_size)\n",
    "\n",
    "# Check if there are any words identified via the tokenizer that are not in vocab\n",
    "tokenizer_vocab = set(tokenizer.word_index.keys())\n",
    "vocab_set = set(vocab)\n",
    "tokenizer_only_words = tokenizer_vocab.difference(vocab_set)\n",
    "print(\"Words in tokenizer but not in vocab: \", len(tokenizer_only_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading w2v model...\n",
      "821/11353 (7.23%) are not defined in the pretrained W2V model and will receive vectors with all 0.\n",
      "W2V Embedding Matrix shape: (11354, 100)\n",
      "Embedding Matrix:\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 1.19679999 -0.028458   -0.29611     0.49471     0.15605     0.53438997]\n",
      " [ 0.66938001 -0.1402      0.080513    0.10082    -0.56133002  0.67628998]\n",
      " [-0.18062     0.28406999 -0.16242    -0.0034944  -0.41459     0.80851001]\n",
      " [-0.059789    0.076035   -0.0072208  -0.044774   -0.60459    -0.41768   ]\n",
      " [ 0.11942    -0.18155999 -0.041091    0.047532   -0.14318     0.64025003]\n",
      " [-0.25685    -0.23058    -0.017128    0.46162999 -0.35681999  0.47191   ]\n",
      " [-0.57674998 -0.42304999  0.27188    -0.31986001  0.18842     0.71320999]\n",
      " [-0.22439    -0.25909001 -0.29517999 -0.56308001 -0.10016    -0.32510999]\n",
      " [ 0.23627    -0.12958001  0.087473   -0.018755    0.33734     0.66074997]]\n"
     ]
    }
   ],
   "source": [
    "def load_embedding():\n",
    "    w2v_pretrained_file_path = os.path.join(processed_folder_path, w2v_pretrained_model_filename)\n",
    "    if not os.path.exists(w2v_pretrained_file_path):\n",
    "        # Check if the pre-trained Word2Vec model is already downloaded. If not, download it.\n",
    "        print(\"\\nW2v model doesn't exist\")\n",
    "        model = api.load(w2v_pretrained_model)\n",
    "        model.save_word2vec_format(w2v_pretrained_file_path, binary=False)\n",
    "        # 5186/12465 (41.60%) are not defined with twitter-glove\n",
    "        # 5177/12465 (41.53%) are not defined with wiki\n",
    "\n",
    "    # Load embedding into memory, skip first line\n",
    "    print(\"Loading w2v model...\")\n",
    "    file = open(w2v_pretrained_file_path, 'r', encoding='utf8')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "\n",
    "    # Create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # Set key as string word, value as numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def get_emb_matrix(loaded_embedding, tokenizer, embedding_dim):\n",
    "    # Create a weight matrix for the Embedding layer from a loaded/pretrained embedding\n",
    "\n",
    "    # Define weight matrix dimensions (vocab_size + 1 for unknown words) with all 0 \n",
    "    emb_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "\n",
    "    count_all_words = 0\n",
    "    count_na_words = 0\n",
    "    zero_vector_words = []\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # Map loaded vectors to terms in vocab\n",
    "        if word in loaded_embedding.keys():\n",
    "            emb_matrix[i] = loaded_embedding.get(word)\n",
    "        else:\n",
    "            # Some terms such as emojis or neg-tagged words are not found in the loaded w2v model, hence they will have vectors with all 0\n",
    "            zero_vector_words.append(word)\n",
    "            count_na_words += 1\n",
    "        count_all_words += 1\n",
    "    print(f'{count_na_words}/{count_all_words} ({((count_na_words/count_all_words)*100):.2f}%) are not defined in the pretrained W2V model and will receive vectors with all 0.')\n",
    "    print(f\"W2V Embedding Matrix shape: {emb_matrix.shape}\")\n",
    "    print(f\"Embedding Matrix:\\n{emb_matrix[:10, :6]}\")\n",
    "\n",
    "    # Save unrecognized words that are not present in the GloVe model\n",
    "    file_path = os.path.join(processed_folder_path, \"out_of_glove_words.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(zero_vector_words))\n",
    "\n",
    "    # Save embeddings\n",
    "    # TODO: delete\n",
    "    file_path = os.path.join(processed_folder_path, \"embedding_matrix.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(' '.join(str(x) for x in row) for row in emb_matrix))\n",
    "\n",
    "    # TODO: keep\n",
    "    save_to_npy(emb_matrix, (os.path.join(processed_folder_path, \"embedding_matrix.npy\")))\n",
    "    \n",
    "    return emb_matrix\n",
    "\n",
    "pretrained_embedding = load_embedding()\n",
    "embedding_dim = 100\n",
    "w2v_embedding_vectors = get_emb_matrix(pretrained_embedding, tokenizer, embedding_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, -2.0], [-3.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0], [0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, -3.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]]\n",
      "[[-3  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [-3  0  0 ...  0  0  0]\n",
      " [ 0  2  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "(41000, 430)\n"
     ]
    }
   ],
   "source": [
    "afinn = Afinn(language='en')\n",
    "\n",
    "def compute_scores(text):\n",
    "    words = text.split() \n",
    "    scores = [afinn.score(word) for word in words]  # compute the AFINN score for each word\n",
    "    return scores\n",
    "\n",
    "# Compute AFINN scores\n",
    "x_train_scores = [compute_scores(text) for text in x_train]\n",
    "x_val_scores = [compute_scores(text) for text in x_val]\n",
    "x_test_scores = [compute_scores(text) for text in x_test]\n",
    "print(x_train_scores[:5])\n",
    "\n",
    "# Pad the sequences with zeros\n",
    "x_train_scores_padded = pad_sequences(x_train_scores, maxlen=max_seq_length, padding='post')\n",
    "x_val_scores_padded = pad_sequences(x_val_scores, maxlen=max_seq_length, padding='post')\n",
    "x_test_scores_padded = pad_sequences(x_test_scores, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "print(x_train_scores_padded[:5])\n",
    "print(x_train_scores_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data zero percentage: 87.03%\n",
      "Validation data zero percentage: 86.83%\n",
      "Test data zero percentage: 86.53%\n"
     ]
    }
   ],
   "source": [
    "def calculate_zero_percentage(score_list):\n",
    "    total_scores = sum([len(scores) for scores in score_list])\n",
    "    total_zeros = sum([scores.count(0) for scores in score_list])\n",
    "    return total_zeros / total_scores * 100\n",
    "\n",
    "# Calculate the percentage of exact zeros\n",
    "train_zero_percentage = calculate_zero_percentage(x_train_scores)\n",
    "val_zero_percentage = calculate_zero_percentage(x_val_scores)\n",
    "test_zero_percentage = calculate_zero_percentage(x_test_scores)\n",
    "\n",
    "print(f'Training data zero percentage: {train_zero_percentage:.2f}%')\n",
    "print(f'Validation data zero percentage: {val_zero_percentage:.2f}%')\n",
    "print(f'Test data zero percentage: {test_zero_percentage:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.875 -0.5    0.    ...  0.     0.     0.   ]\n",
      " [ 0.     0.     0.    ...  0.     0.     0.   ]\n",
      " [-0.625  0.125  0.375 ...  0.     0.     0.   ]\n",
      " [ 0.     0.25   0.    ...  0.     0.     0.   ]\n",
      " [-0.125 -0.25   0.    ...  0.     0.     0.   ]]\n",
      "(41000, 430)\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(word):\n",
    "    synsets = wn.synsets(word) # get set of synonyms\n",
    "    if not synsets:\n",
    "        return 0  # return 0 if the word is not in WordNet\n",
    "    synset = synsets[0] # The first synset is the most common sense (which are orderd by freq)\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "    return swn_synset.pos_score() - swn_synset.neg_score() # Return the overall sentiment polarity\n",
    "\n",
    "def compute_scores(text):\n",
    "    words = text.split()\n",
    "    scores = [get_sentiment(word) for word in words]\n",
    "    return scores\n",
    "\n",
    "def pad_scores(score_list, max_len):\n",
    "    # Pad the sequences to max sequence length\n",
    "    return [scores + [0] * (max_len - len(scores)) for scores in score_list]\n",
    "\n",
    "# Compute sentiment scores\n",
    "x_train_scores = [compute_scores(text) for text in x_train]\n",
    "x_val_scores = [compute_scores(text) for text in x_val]\n",
    "x_test_scores = [compute_scores(text) for text in x_test]\n",
    "\n",
    "# Pad the scores\n",
    "x_train_scores_padded = np.array(pad_scores(x_train_scores, max_seq_length))\n",
    "x_val_scores_padded = np.array(pad_scores(x_val_scores, max_seq_length))\n",
    "x_test_scores_padded = np.array(pad_scores(x_test_scores, max_seq_length))\n",
    "\n",
    "print(x_train_scores_padded[:5])\n",
    "print(x_train_scores_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data zero percentage: 72.14%\n",
      "Validation data zero percentage: 72.26%\n",
      "Test data zero percentage: 72.25%\n"
     ]
    }
   ],
   "source": [
    "def zero_percentage(score_list):\n",
    "    total_scores = sum([len(scores) for scores in score_list])\n",
    "    total_zeros = sum([scores.count(0) for scores in score_list])\n",
    "    return total_zeros / total_scores * 100\n",
    "\n",
    "# Calculate the percentage of exact zeros\n",
    "train_zero_percentage = zero_percentage(x_train_scores)\n",
    "val_zero_percentage = zero_percentage(x_val_scores)\n",
    "test_zero_percentage = zero_percentage(x_test_scores)\n",
    "\n",
    "print(f'Training data zero percentage: {train_zero_percentage:.2f}%')\n",
    "print(f'Validation data zero percentage: {val_zero_percentage:.2f}%')\n",
    "print(f'Test data zero percentage: {test_zero_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the padded scores\n",
    "save_to_npy(x_train_scores_padded, os.path.join(processed_folder_path, 'x_train_scores_padded.npy'))\n",
    "save_to_npy(x_val_scores_padded, os.path.join(processed_folder_path, 'x_val_scores_padded.npy'))\n",
    "save_to_npy(x_test_scores_padded, os.path.join(processed_folder_path, 'x_test_scores_padded.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(scores, file_path):\n",
    "    np.savetxt(file_path, scores, fmt='%f')\n",
    "\n",
    "# Save padded scores to text files\n",
    "save_to_txt(x_train_scores_padded, os.path.join(processed_folder_path, 'x_train_scores_padded.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
