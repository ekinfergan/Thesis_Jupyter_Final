{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 02:17:57.771233: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from numpy import asarray\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, RocCurveDisplay, confusion_matrix, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.legacy import Adam, SGD, RMSprop, Adagrad\n",
    "\n",
    "import skopt\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer  \n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "# DATASET\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "# Define a dictionary to map sentiment values to category names\n",
    "senti_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_categories = list(senti_labels.values())\n",
    "NUM_of_CLASSES = 3\n",
    "\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/\"\n",
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.66859001 -0.34494999  0.68182999 ...  1.27419996  0.56130999\n",
      "   0.30846   ]\n",
      " [ 0.31551999  0.81511003  0.067789   ... -0.37785     0.42570001\n",
      "  -0.19459   ]]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(input_folder_path, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(input_folder_path, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(input_folder_path, \"test.csv\"))\n",
    "\n",
    "x_train = train['x']\n",
    "y_train = train['y']\n",
    "x_val = val['x']\n",
    "y_val = val['y']\n",
    "x_test = test['x']\n",
    "y_test = test['y']\n",
    "\n",
    "x_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_x.npy\"))\n",
    "y_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_y.npy\"))\n",
    "x_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_x.npy\"))\n",
    "y_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_y.npy\"))\n",
    "x_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_x.npy\"))\n",
    "y_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_y.npy\"))\n",
    "\n",
    "w2v_embedding_vectors = np.load(os.path.join(processed_folder_path, \"embedding_matrix.npy\"))\n",
    "print(w2v_embedding_vectors)\n",
    "\n",
    "%store -r embedding_vocab_size\n",
    "%store -r EMBEDDING_DIM\n",
    "%store -r max_seq_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(score):\n",
    "    acc =  score[1]\n",
    "    loss = score[0]\n",
    "\n",
    "    print(f\"Accuracy: {acc:.2%}\")\n",
    "    print(f\"Loss: {loss:.2f}\")\n",
    "    \n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_report(y, y_pred, labels):\n",
    "    report = classification_report(y, y_pred, labels=labels)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
    "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=labels)\n",
    "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, x_encoded, y_encoded, y=None, only_metrics=True):    \n",
    "    y_pred_prob = model.predict(x_encoded)\n",
    "\n",
    "    print(f\"*{model_name}\")\n",
    "    \n",
    "    score = model.evaluate(x_encoded, y_encoded, verbose=0)\n",
    "    calculate_metrics(score)\n",
    "    \n",
    "    senti_labels = ['negative', 'neutral', 'positive'] #TODO: to constants\n",
    "    \n",
    "    if not only_metrics:\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1) + 1\n",
    "        calculate_classification_report(y, y_pred, labels=senti_labels)\n",
    "        plot_confusion_matrix(y, y_pred, labels=senti_labels)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(prob_test_vec, y_test, labels):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels\n",
    "    colors = cycle(['limegreen', 'dodgerblue', 'red'])\n",
    "    for senti, color in zip(range(NUM_of_CLASSES), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_test[:, senti],\n",
    "            prob_test_vec[:, senti],\n",
    "            name=f\"ROC curve for {labels[senti]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_OvR_roc_auc_score(model, x, y, x_test, y_test, labels): #average??\n",
    "    #y = one_hot_encode(y)\n",
    "    #y_test = one_hot_encode(y_test)\n",
    "\n",
    "    ovr_model = OneVsRestClassifier(model).fit(x, y)\n",
    "    prob_test_vec = ovr_model.predict_proba(x_test)\n",
    "    \n",
    "    fpr, tpr, thresholds, auc_score = [], [], [], []\n",
    "    for _ in range(NUM_of_CLASSES):\n",
    "        fpr.append(0)\n",
    "        tpr.append(0)\n",
    "        thresholds.append(0)\n",
    "        auc_score.append(0)\n",
    "    \n",
    "    for i in range(NUM_of_CLASSES):\n",
    "        fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], prob_test_vec[:, i])\n",
    "        auc_score[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    print(f\"AUC score: {auc_score}\")\n",
    "    averaged_auc_score = (sum(auc_score) / NUM_of_CLASSES)\n",
    "    print(f\"Averaged AUC score: {averaged_auc_score:.2f}\")\n",
    "    \n",
    "    plot_roc_curve(prob_test_vec, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_development(history):\n",
    "    acc =  history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "    \n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a basic neural net to see the baseline for accuracy with minimum tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_classes = 3\n",
    "batch_size= 64\n",
    "epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# input layer is sequence of integers (words)\n",
    "model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, name=\"embedding_layer\")) # part of input layer as it transforms integers into dense vectors, input shape = (None, max_seq_length)\n",
    "model.add(LSTM(64, name='hidden_layer')) # hidden layer\n",
    "model.add(Dense(num_output_classes, activation='softmax', name=\"output_layer\"))\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "basic_history = model.fit(x_train_encoded, y_train_encoded, validation_data=(x_val_encoded, y_val_encoded), batch_size=batch_size, epochs=10)\n",
    "\n",
    "accuracy = model.evaluate(x_test_encoded, y_test_encoded)[1]\n",
    "print(f\"Naive model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "del model\n",
    "\n",
    "backend.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our naive model, we get an accuracy of x%. # TODO: x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm_layers = Integer(low=1, high=5, name='num_lstm_layers')\n",
    "num_lstm_units = Integer(low=32, high=256, name='num_lstm_units') # TODO: step Keras Tuner\n",
    "learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "adam_decay = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n",
    "#batch_size = Integer(low=1, high=128, name='batch_size')\n",
    "\n",
    "search_space = [\n",
    "            num_lstm_layers,\n",
    "            num_lstm_units,\n",
    "            learning_rate,\n",
    "            adam_decay\n",
    "            ]\n",
    "\n",
    "# Specify one or more initial points for the search of optimal parameter\n",
    "default_params = [1,\n",
    "                  32,\n",
    "                  1e-3, \n",
    "                  1e-3,\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_lstm_model(num_lstm_layers, num_lstm_units, learning_rate, adam_decay):\n",
    "    # Start the model making process and create our first layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length))\n",
    "\n",
    "    # Create a loop making a new LSTM layer for the amount passed to this model.\n",
    "    # Naming the layers helps avoid tensorflow error deep in the stack trace.\n",
    "    for i in range(num_lstm_layers):\n",
    "        name = 'layer_lstm_{0}'.format(i+1)\n",
    "        if i < num_lstm_layers-1:\n",
    "            model.add(LSTM(num_lstm_units, return_sequences=True, name=name))\n",
    "        else:\n",
    "            model.add(LSTM(num_lstm_units, return_sequences=False, name=name))\n",
    "\n",
    "    # Add our classification layer.\n",
    "    model.add(Dense(num_output_classes, activation='softmax'))\n",
    "\n",
    "    # Setup our optimizer and compile\n",
    "    adam = Adam(learning_rate=learning_rate, decay=adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=search_space)\n",
    "def objective_function(num_lstm_layers, num_lstm_units, learning_rate, adam_decay):\n",
    "\n",
    "    model = define_lstm_model(num_lstm_layers=num_lstm_layers,\n",
    "                         num_lstm_units=num_lstm_units,\n",
    "                         learning_rate=learning_rate,\n",
    "                         adam_decay=adam_decay\n",
    "                         )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    loss = history.history['val_loss'][-1]\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Loss: {loss:.2}\\n\")\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 32)                17024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,317,423\n",
      "Trainable params: 2,317,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 02:19:42.536434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-06-15 02:19:42.679040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 17s 26ms/step - loss: 1.0798 - accuracy: 0.4276 - val_loss: 0.8946 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 13s 24ms/step - loss: 1.0794 - accuracy: 0.4287 - val_loss: 0.8621 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 13s 24ms/step - loss: 1.0792 - accuracy: 0.4287 - val_loss: 0.8751 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 13s 25ms/step - loss: 1.0792 - accuracy: 0.4287 - val_loss: 0.8774 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 13s 25ms/step - loss: 1.0791 - accuracy: 0.4287 - val_loss: 0.8820 - val_accuracy: 0.9023\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 13s 25ms/step - loss: 1.0790 - accuracy: 0.4287 - val_loss: 0.8919 - val_accuracy: 0.9023\n",
      "Epoch 7/30\n",
      "547/547 [==============================] - 13s 25ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.9070 - val_accuracy: 0.9023\n",
      "Accuracy: 90.23%\n",
      "Loss: 0.91\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 35)                19040     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 108       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,319,448\n",
      "Trainable params: 2,319,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "547/547 [==============================] - 18s 29ms/step - loss: 1.0799 - accuracy: 0.4282 - val_loss: 0.8873 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0791 - accuracy: 0.4287 - val_loss: 0.8824 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0790 - accuracy: 0.4287 - val_loss: 0.8794 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0790 - accuracy: 0.4287 - val_loss: 0.8868 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0790 - accuracy: 0.4287 - val_loss: 0.8921 - val_accuracy: 0.9023\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8860 - val_accuracy: 0.9023\n",
      "Epoch 7/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.9011 - val_accuracy: 0.9023\n",
      "Epoch 8/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8786 - val_accuracy: 0.9023\n",
      "Epoch 9/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8884 - val_accuracy: 0.9023\n",
      "Epoch 10/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8834 - val_accuracy: 0.9023\n",
      "Epoch 11/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8903 - val_accuracy: 0.9023\n",
      "Epoch 12/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8843 - val_accuracy: 0.9023\n",
      "Epoch 13/30\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8865 - val_accuracy: 0.9023\n",
      "Accuracy: 90.23%\n",
      "Loss: 0.89\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 561, 178)          198648    \n",
      "                                                                 \n",
      " layer_lstm_2 (LSTM)         (None, 561, 178)          254184    \n",
      "                                                                 \n",
      " layer_lstm_3 (LSTM)         (None, 561, 178)          254184    \n",
      "                                                                 \n",
      " layer_lstm_4 (LSTM)         (None, 561, 178)          254184    \n",
      "                                                                 \n",
      " layer_lstm_5 (LSTM)         (None, 178)               254184    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 537       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,516,221\n",
      "Trainable params: 3,516,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "547/547 [==============================] - 84s 141ms/step - loss: 1.0887 - accuracy: 0.4246 - val_loss: 0.8980 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 75s 138ms/step - loss: 1.0793 - accuracy: 0.4286 - val_loss: 0.8775 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 75s 137ms/step - loss: 1.0793 - accuracy: 0.4286 - val_loss: 0.8818 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 75s 137ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8978 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 75s 138ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8730 - val_accuracy: 0.9023\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 75s 138ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.8838 - val_accuracy: 0.9023\n",
      "Epoch 7/30\n",
      "547/547 [==============================] - 75s 138ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8750 - val_accuracy: 0.9023\n",
      "Epoch 8/30\n",
      "547/547 [==============================] - 75s 138ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8912 - val_accuracy: 0.9023\n",
      "Epoch 9/30\n",
      "547/547 [==============================] - 75s 137ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8835 - val_accuracy: 0.9023\n",
      "Epoch 10/30\n",
      "547/547 [==============================] - 75s 138ms/step - loss: 1.0790 - accuracy: 0.4286 - val_loss: 0.8958 - val_accuracy: 0.9023\n",
      "Accuracy: 90.23%\n",
      "Loss: 0.9\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 561, 139)          133440    \n",
      "                                                                 \n",
      " layer_lstm_2 (LSTM)         (None, 561, 139)          155124    \n",
      "                                                                 \n",
      " layer_lstm_3 (LSTM)         (None, 561, 139)          155124    \n",
      "                                                                 \n",
      " layer_lstm_4 (LSTM)         (None, 139)               155124    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 420       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,899,532\n",
      "Trainable params: 2,899,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "547/547 [==============================] - 65s 110ms/step - loss: 1.0801 - accuracy: 0.4282 - val_loss: 0.8914 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 58s 107ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.8695 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 59s 107ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.8833 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 59s 107ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8985 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 59s 108ms/step - loss: 1.0790 - accuracy: 0.4286 - val_loss: 0.8897 - val_accuracy: 0.9023\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 58s 106ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8850 - val_accuracy: 0.9023\n",
      "Epoch 7/30\n",
      "547/547 [==============================] - 58s 107ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8912 - val_accuracy: 0.9023\n",
      "Accuracy: 90.23%\n",
      "Loss: 0.89\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 561, 186)          213528    \n",
      "                                                                 \n",
      " layer_lstm_2 (LSTM)         (None, 561, 186)          277512    \n",
      "                                                                 \n",
      " layer_lstm_3 (LSTM)         (None, 186)               277512    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 561       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,069,413\n",
      "Trainable params: 3,069,413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "547/547 [==============================] - 56s 94ms/step - loss: 1.0800 - accuracy: 0.4286 - val_loss: 0.9095 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 50s 92ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.8862 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 50s 92ms/step - loss: 1.0793 - accuracy: 0.4286 - val_loss: 0.8764 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 50s 92ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.8821 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 50s 91ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8857 - val_accuracy: 0.9023\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 50s 91ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8876 - val_accuracy: 0.9023\n",
      "Epoch 7/30\n",
      "547/547 [==============================] - 50s 91ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8825 - val_accuracy: 0.9023\n",
      "Epoch 8/30\n",
      "547/547 [==============================] - 50s 91ms/step - loss: 1.0790 - accuracy: 0.4286 - val_loss: 0.8824 - val_accuracy: 0.9023\n",
      "Accuracy: 90.23%\n",
      "Loss: 0.88\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 561, 133)          124488    \n",
      "                                                                 \n",
      " layer_lstm_2 (LSTM)         (None, 561, 133)          142044    \n",
      "                                                                 \n",
      " layer_lstm_3 (LSTM)         (None, 561, 133)          142044    \n",
      "                                                                 \n",
      " layer_lstm_4 (LSTM)         (None, 561, 133)          142044    \n",
      "                                                                 \n",
      " layer_lstm_5 (LSTM)         (None, 133)               142044    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 402       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,993,366\n",
      "Trainable params: 2,993,366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "547/547 [==============================] - 80s 135ms/step - loss: 1.0825 - accuracy: 0.4262 - val_loss: 0.8470 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 73s 134ms/step - loss: 1.0770 - accuracy: 0.4280 - val_loss: 0.8569 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 73s 134ms/step - loss: 1.0764 - accuracy: 0.4286 - val_loss: 0.8728 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 74s 135ms/step - loss: 1.0763 - accuracy: 0.4286 - val_loss: 0.8771 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 73s 134ms/step - loss: 1.0761 - accuracy: 0.4286 - val_loss: 0.8562 - val_accuracy: 0.9023\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 73s 134ms/step - loss: 1.0762 - accuracy: 0.4286 - val_loss: 0.8908 - val_accuracy: 0.9023\n",
      "Accuracy: 90.23%\n",
      "Loss: 0.89\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 561, 94)           73320     \n",
      "                                                                 \n",
      " layer_lstm_2 (LSTM)         (None, 561, 94)           71064     \n",
      "                                                                 \n",
      " layer_lstm_3 (LSTM)         (None, 561, 94)           71064     \n",
      "                                                                 \n",
      " layer_lstm_4 (LSTM)         (None, 94)                71064     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 285       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,587,097\n",
      "Trainable params: 2,587,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "547/547 [==============================] - 65s 109ms/step - loss: 1.0804 - accuracy: 0.4278 - val_loss: 0.8889 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 57s 105ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.8923 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 58s 105ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8533 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 57s 105ms/step - loss: 1.0790 - accuracy: 0.4287 - val_loss: 0.8837 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 57s 105ms/step - loss: 1.0790 - accuracy: 0.4287 - val_loss: 0.8846 - val_accuracy: 0.9023\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 58s 105ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8986 - val_accuracy: 0.9023\n",
      "Epoch 7/30\n",
      "547/547 [==============================] - 58s 106ms/step - loss: 1.0790 - accuracy: 0.4287 - val_loss: 0.8811 - val_accuracy: 0.9023\n",
      "Epoch 8/30\n",
      "547/547 [==============================] - 58s 106ms/step - loss: 1.0789 - accuracy: 0.4287 - val_loss: 0.8861 - val_accuracy: 0.9023\n",
      "Accuracy: 90.23%\n",
      "Loss: 0.89\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 561, 109)          91560     \n",
      "                                                                 \n",
      " layer_lstm_2 (LSTM)         (None, 561, 109)          95484     \n",
      "                                                                 \n",
      " layer_lstm_3 (LSTM)         (None, 561, 109)          95484     \n",
      "                                                                 \n",
      " layer_lstm_4 (LSTM)         (None, 109)               95484     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,678,642\n",
      "Trainable params: 2,678,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "547/547 [==============================] - 66s 110ms/step - loss: 1.0799 - accuracy: 0.4283 - val_loss: 0.8788 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 58s 107ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.8980 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 59s 108ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.8683 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 58s 107ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.8815 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 59s 107ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8898 - val_accuracy: 0.9023\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 59s 107ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8862 - val_accuracy: 0.9023\n",
      "Epoch 7/30\n",
      "547/547 [==============================] - 58s 107ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8899 - val_accuracy: 0.9023\n",
      "Epoch 8/30\n",
      "547/547 [==============================] - 59s 107ms/step - loss: 1.0790 - accuracy: 0.4286 - val_loss: 0.8851 - val_accuracy: 0.9023\n",
      "Accuracy: 90.23%\n",
      "Loss: 0.89\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 561, 111)          94128     \n",
      "                                                                 \n",
      " layer_lstm_2 (LSTM)         (None, 111)               99012     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 336       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,493,776\n",
      "Trainable params: 2,493,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "547/547 [==============================] - 36s 59ms/step - loss: 1.0798 - accuracy: 0.4281 - val_loss: 0.9194 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 31s 57ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8750 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 32s 58ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8989 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 31s 57ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8877 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 31s 58ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8947 - val_accuracy: 0.9023\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 31s 57ms/step - loss: 1.0790 - accuracy: 0.4286 - val_loss: 0.8871 - val_accuracy: 0.9023\n",
      "Epoch 7/30\n",
      "547/547 [==============================] - 31s 57ms/step - loss: 1.0791 - accuracy: 0.4286 - val_loss: 0.8897 - val_accuracy: 0.9023\n",
      "Accuracy: 90.23%\n",
      "Loss: 0.89\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_lstm_1 (LSTM)         (None, 185)               211640    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 558       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,512,498\n",
      "Trainable params: 2,512,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "547/547 [==============================] - 23s 38ms/step - loss: 1.0797 - accuracy: 0.4279 - val_loss: 0.8912 - val_accuracy: 0.9023\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 20s 36ms/step - loss: 1.0792 - accuracy: 0.4286 - val_loss: 0.9013 - val_accuracy: 0.9023\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 20s 36ms/step - loss: 1.0791 - accuracy: 0.4287 - val_loss: 0.8948 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "298/547 [===============>..............] - ETA: 7s - loss: 1.0811 - accuracy: 0.4238"
     ]
    }
   ],
   "source": [
    "gp_result = gp_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gp_best_params = {param.name: value for param, value in zip(gp_result.space, gp_result.x)}\n",
    "print(\"Best Hyperparameters:\", gp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_lstm_model(gp_best_params['num_lstm_layers'], \n",
    "                          gp_best_params['num_lstm_units'], \n",
    "                          gp_best_params['learning_rate'], \n",
    "                          gp_best_params['adam_decay']\n",
    "                          )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                    y_train_encoded,\n",
    "                    validation_data=(x_val_encoded, y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train single-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val single-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test single-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_result = gbrt_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
    "print(\"Best Hyperparameters:\", gbrt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_lstm_model(gbrt_best_params['num_lstm_layers'], \n",
    "                          gbrt_best_params['num_lstm_units'], \n",
    "                          gbrt_best_params['learning_rate'], \n",
    "                          gbrt_best_params['adam_decay']\n",
    "                          )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                    y_train_encoded,\n",
    "                    validation_data=(x_val_encoded, y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train single-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val single-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test single-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm_layersA = Integer(low=1, high=5, name='num_lstm_layersA')\n",
    "num_lstm_layersB = Integer(low=1, high=5, name='num_lstm_layersB')\n",
    "num_lstm_unitsA = Integer(low=32, high=256, name='num_lstm_unitsA')\n",
    "num_lstm_unitsB = Integer(low=32, high=256, name='num_lstm_unitsB')\n",
    "\n",
    "search_space = [\n",
    "            num_lstm_layersA,\n",
    "            num_lstm_layersB,\n",
    "            num_lstm_unitsA,\n",
    "            num_lstm_unitsB,\n",
    "            learning_rate,\n",
    "            adam_decay\n",
    "            ]\n",
    "\n",
    "# Specify one or more initial points for the search of optimal parameter\n",
    "default_params = [1, \n",
    "                  1, \n",
    "                  32,\n",
    "                  32, \n",
    "                  1e-3,\n",
    "                  1e-3 \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_multi_channel_lstm_model(num_lstm_layersA, num_lstm_layersB, num_lstm_unitsA, num_lstm_unitsB, learning_rate, adam_decay):\n",
    "    # Vocabulary-based embedding layer\n",
    "    inputsA = Input(shape=(max_seq_length,), name=\"input regular embeddings\")\n",
    "    # Word2Vec embedding layer\n",
    "    inputsB = Input(shape=(max_seq_length,), name=\"input word2vec embeddings\")\n",
    "    \n",
    "    # Define an embedding layer for each input\n",
    "    embeddingsA = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, name=\"embeddingsA\")(inputsA)\n",
    "    embeddingsB = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, weights=[w2v_embedding_vectors], trainable=False, name=\"embeddingsB\")(inputsB)\n",
    "    \n",
    "    # Pass both embeddings through their own LSTM layers\n",
    "    lstm_layersA = embeddingsA\n",
    "    for i in range(num_lstm_layersA):\n",
    "        nameA = 'layer_lstmA_{0}'.format(i+1)\n",
    "        if i < num_lstm_layers-1:\n",
    "            lstm_layersA = LSTM(num_lstm_unitsA, return_sequences=True, name=nameA)(lstm_layersA)\n",
    "        else:\n",
    "            lstm_layersA = LSTM(num_lstm_unitsA, return_sequences=False, name=nameA)(lstm_layersA)\n",
    "        \n",
    "    lstm_layersB = embeddingsB\n",
    "    for i in range(num_lstm_layersB):\n",
    "        nameA = 'layer_lstmB_{0}'.format(i+1)\n",
    "        if i < num_lstm_layers-1:\n",
    "            lstm_layersA = LSTM(num_lstm_unitsB, return_sequences=True, name=nameA)(lstm_layersB)\n",
    "        else:\n",
    "            lstm_layersA = LSTM(num_lstm_unitsB, return_sequences=False, name=nameA)(lstm_layersB)\n",
    "        \n",
    "\n",
    "    # Concatenate the two inputs\n",
    "    merged = concatenate([lstm_layersA, lstm_layersB])\n",
    "\n",
    "    # Dense layer for the merged inputs & output Layer\n",
    "    outputs = Dense(num_output_classes, activation='softmax', name=\"output\")(merged)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[inputsA, inputsB], outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    adam = Adam(learning_rate=learning_rate, decay=adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=search_space)\n",
    "def multi_objective_function(num_lstm_layersA, num_lstm_layersB, num_lstm_unitsA, num_lstm_unitsB, learning_rate, adam_decay, batch_size):\n",
    "\n",
    "    model = define_multi_channel_lstm_model(num_lstm_layersA=num_lstm_layersA,\n",
    "                                            num_lstm_layersB=num_lstm_layersB,\n",
    "                                            num_lstm_unitsA=num_lstm_unitsA,\n",
    "                                            num_lstm_unitsB=num_lstm_unitsB,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            adam_decay=adam_decay\n",
    "                                            )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Loss: {loss:.2}\\n\")\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_result = gp_minimize(func=multi_objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gp_best_params = {param.name: value for param, value in zip(gp_result.space, gp_result.x)}\n",
    "print(\"Best Hyperparameters:\", gp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_lstm_model(gp_best_params['num_lstm_layersA'],\n",
    "                                        gp_best_params['num_lstm_layersB'],\n",
    "                                        gp_best_params['num_lstm_unitsA'], \n",
    "                                        gp_best_params['num_lstm_unitsB'],\n",
    "                                        gp_best_params['learning_rate'], \n",
    "                                        gp_best_params['adam_decay']\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_result = gbrt_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
    "print(\"Best Hyperparameters:\", gbrt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_lstm_model(gbrt_best_params['num_lstm_layersA'],\n",
    "                                        gbrt_best_params['num_lstm_layersB'],\n",
    "                                        gbrt_best_params['num_lstm_unitsA'], \n",
    "                                        gbrt_best_params['num_lstm_unitsB'],\n",
    "                                        gbrt_best_params['learning_rate'], \n",
    "                                        gbrt_best_params['adam_decay'],\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=20, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-LSTM\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-LSTM\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-LSTM\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
