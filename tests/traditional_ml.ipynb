{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.sparse import load_npz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, RocCurveDisplay, confusion_matrix, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "senti_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_categories = list(senti_labels.values())\n",
    "NUM_of_CLASSES = 3\n",
    "\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/\"\n",
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(lowercase=False, max_df=0.95, max_features=10000, min_df=5,\n",
      "                sublinear_tf=True,\n",
      "                vocabulary={'.': 3751, '..': 769, '.._NEG': 1272, ':(': 2451,\n",
      "                            ':(_NEG': 3380, ':)': 1093, ':)_NEG': 1714,\n",
      "                            ':-)': 2622, ':-)_NEG': 2967, ':d': 283, ';)': 3912,\n",
      "                            ';)_NEG': 518, ';-)': 1364, '<3': 2613, '=)': 3464,\n",
      "                            'ability': 154, 'ability_NEG': 2319, 'able': 159,\n",
      "                            'able_NEG': 2175, 'ableton': 2610, 'absolute': 1723,\n",
      "                            'absolute_NEG': 2293, 'absolutely': 959,\n",
      "                            'absolutely_NEG': 2644, 'absorb': 3292,\n",
      "                            'abstract': 1383, 'abstract_NEG': 3675,\n",
      "                            'academic': 1831, 'academic_NEG': 1840,\n",
      "                            'accent': 2827, ...})\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(input_folder_path, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(input_folder_path, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(input_folder_path, \"test.csv\"))\n",
    "\n",
    "#x_train = pd.read_csv(os.path.join(processed_folder_path, \"train_tfidf.csv\"))\n",
    "y_train = train['y']\n",
    "#x_val = pd.read_csv(os.path.join(processed_folder_path, \"val_tfidf.csv\"))\n",
    "y_val = val['y']\n",
    "#x_test = pd.read_csv(os.path.join(processed_folder_path, \"test_tfidf.csv\"))\n",
    "y_test = test['y']\n",
    "\n",
    "x_train = load_npz(os.path.join(processed_folder_path, \"train_tfidf.npz\"))\n",
    "x_val = load_npz(os.path.join(processed_folder_path, \"val_tfidf.npz\"))\n",
    "x_test = load_npz(os.path.join(processed_folder_path, \"test_tfidf.npz\"))\n",
    "\n",
    "\n",
    "\n",
    "%store -r tfidf_vectorizer\n",
    "print(tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 253)\t0.20750229352294255\n",
      "  (0, 252)\t0.24986203781929486\n",
      "  (0, 251)\t0.22904360321584438\n",
      "  (0, 250)\t0.27874104803728433\n",
      "  (0, 249)\t0.291209380103886\n",
      "  (0, 248)\t0.22918675777101033\n",
      "  (0, 247)\t0.22549423746376174\n",
      "  (0, 246)\t0.30639145816470104\n",
      "  (0, 245)\t0.21384111716539875\n",
      "  (0, 244)\t0.28537423072697504\n",
      "  (0, 243)\t0.27929905289854423\n",
      "  (0, 242)\t0.25010598083411073\n",
      "  (0, 241)\t0.21269510187158405\n",
      "  (0, 111)\t0.21185708047425877\n",
      "  (0, 47)\t0.3109097526879989\n",
      "  (0, 37)\t0.1666926083077077\n",
      "  (1, 289)\t0.3715426905809993\n",
      "  (1, 288)\t0.32315625447829366\n",
      "  (1, 287)\t0.4841052026343734\n",
      "  (1, 286)\t0.27289199553722276\n",
      "  (1, 285)\t0.4242784682901177\n",
      "  (1, 284)\t0.4380211438201473\n",
      "  (1, 71)\t0.2771704073910455\n",
      "  (2, 361)\t0.32269871638662884\n",
      "  (2, 360)\t0.37822739558263596\n",
      "  :\t:\n",
      "  (48, 135)\t0.1292547530301328\n",
      "  (48, 130)\t0.19354553333194058\n",
      "  (48, 73)\t0.09640381757745732\n",
      "  (48, 51)\t0.10367974228120816\n",
      "  (49, 2668)\t0.25947011372353246\n",
      "  (49, 2667)\t0.25064393203798324\n",
      "  (49, 2666)\t0.25501770954527797\n",
      "  (49, 2641)\t0.28429898013687005\n",
      "  (49, 2319)\t0.2532036266028734\n",
      "  (49, 2210)\t0.23234591174920458\n",
      "  (49, 1655)\t0.165544948331637\n",
      "  (49, 1430)\t0.21142861665675092\n",
      "  (49, 1121)\t0.2037676931377664\n",
      "  (49, 1015)\t0.20057681781318404\n",
      "  (49, 585)\t0.2523299129844212\n",
      "  (49, 580)\t0.1919382765904741\n",
      "  (49, 564)\t0.2362574830700885\n",
      "  (49, 501)\t0.17376841558848014\n",
      "  (49, 495)\t0.18111738333420632\n",
      "  (49, 394)\t0.17319527564215928\n",
      "  (49, 388)\t0.23161236087670542\n",
      "  (49, 196)\t0.18041499842905753\n",
      "  (49, 137)\t0.22520273477458455\n",
      "  (49, 134)\t0.1718469088055691\n",
      "  (49, 129)\t0.19045043612701398\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(x_train[:50])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    # TODO: not anymore?? Handle the zero-division error when there are no predicted samples for a label\n",
    "    # only interested in labels that were predicted at least once\n",
    "    precision = precision_score(y, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    \n",
    "    print(f\"Accuracy: {(accuracy * 100):.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"f1-score: {f1:.2f}\")\n",
    "    \n",
    "def calculate_classification_report(y, y_pred, labels):\n",
    "    report = classification_report(y, y_pred, labels=labels)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
    "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=labels)\n",
    "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# TODO: can I use for svm?\n",
    "def plot_feature_imp(model):\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    feature_importances.nlargest(20).plot.bar(ax=ax)\n",
    "    ax.set_title(\"Top 20 Most Predictive Features\")\n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Importance')\n",
    "    fig.tight_layout()\n",
    "\n",
    "def evaluate_model(model, model_name, x, y, params, only_metrics):    \n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    print(f\"*{model_name}\")\n",
    "\n",
    "    calculate_metrics(y, y_pred)\n",
    "    print(f\"Params: {params}\\n\")\n",
    "    \n",
    "    senti_labels = ['negative', 'neutral', 'positive'] #TODO: to constants\n",
    "    \n",
    "    if not only_metrics:\n",
    "        calculate_classification_report(y, y_pred, labels=senti_labels)\n",
    "        plot_confusion_matrix(y, y_pred, labels=senti_labels)\n",
    "        plot_feature_imp(model) #TODO: for especially RF\n",
    "        \n",
    "def plot_roc_curve(prob_test_vec, y_test, labels):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels\n",
    "    colors = cycle(['limegreen', 'dodgerblue', 'red'])\n",
    "    for senti, color in zip(range(NUM_of_CLASSES), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_test[:, senti],\n",
    "            prob_test_vec[:, senti],\n",
    "            name=f\"ROC curve for {labels[senti]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "def calculate_OvR_roc_auc_score(model, x, y, x_test, y_test, labels): #average??\n",
    "    y = one_hot_encode(y)\n",
    "    y_test = one_hot_encode(y_test)\n",
    "\n",
    "    ovr_model = OneVsRestClassifier(model).fit(x, y)\n",
    "    prob_test_vec = ovr_model.predict_proba(x_test)\n",
    "    \n",
    "    fpr, tpr, thresholds, auc_score = [], [], [], []\n",
    "    for _ in range(NUM_of_CLASSES):\n",
    "        fpr.append(0)\n",
    "        tpr.append(0)\n",
    "        thresholds.append(0)\n",
    "        auc_score.append(0)\n",
    "    \n",
    "    for i in range(NUM_of_CLASSES):\n",
    "        fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], prob_test_vec[:, i])\n",
    "        auc_score[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    print(f\"AUC score: {auc_score}\")\n",
    "    averaged_auc_score = (sum(auc_score) / NUM_of_CLASSES)\n",
    "    print(f\"Averaged AUC score: {averaged_auc_score:.2f}\")\n",
    "    \n",
    "    plot_roc_curve(prob_test_vec, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top3_models(top3_models):    \n",
    "    # Print the sorted list of mean test scores and standard deviation of test scores\n",
    "    print(\"\\nTop 3 parameter combinations ranked by performance (from best to worst):\")\n",
    "    for index, row in top3_models.iterrows():\n",
    "        mean_score = row['mean_test_score']\n",
    "        std_score = row['std_test_score']\n",
    "        params = row['params']\n",
    "        print(f\"Mean Test Score: {mean_score:.4f} (±{std_score:.4f}) for {params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Random Forest model\n",
    "rf_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [20, 50, 100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10, 30, 50],\n",
    "    'min_samples_leaf': [1, 5, 20, 50],  # Minimum number of samples required to be at a leaf node\n",
    "    #'max_features': ['auto', 'sqrt'],  # Number of features to consider when looking for the best split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Perform grid search\u001b[39;00m\n\u001b[1;32m      2\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mrf_classifier, param_grid\u001b[39m=\u001b[39mrf_param_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Get the mean test scores and standard deviations of test scores for all parameter combinations\u001b[39;00m\n\u001b[1;32m      6\u001b[0m results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(grid_search\u001b[39m.\u001b[39mcv_results_)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m )(\n\u001b[1;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    479\u001b[0m         t,\n\u001b[1;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    481\u001b[0m         X,\n\u001b[1;32m    482\u001b[0m         y,\n\u001b[1;32m    483\u001b[0m         sample_weight,\n\u001b[1;32m    484\u001b[0m         i,\n\u001b[1;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=rf_param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "top3_models = sorted_results[:3]\n",
    "print_top3_models(top3_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_params = top3_models['params'].values\n",
    "res_filename = \"rf_results\"\n",
    "\n",
    "# Evaluate the top 3 models on the validation set\n",
    "rf_cand_0 = RandomForestClassifier(**top3_params[0])\n",
    "rf_cand_0.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_0, \"Training-RF-0\", x_train, y_train, top3_params[0], only_metrics=True)\n",
    "evaluate_model(rf_cand_0, \"Validation-RF-0\", x_val, y_val, top3_params[0], only_metrics=True)\n",
    "\n",
    "rf_cand_1 = RandomForestClassifier(**top3_params[1])\n",
    "rf_cand_1.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_1, \"Training-RF-1\", x_train, y_train, top3_params[1], only_metrics=True)\n",
    "evaluate_model(rf_cand_1, \"Validation-RF-1\", x_val, y_val, top3_params[1], only_metrics=True)\n",
    "\n",
    "rf_cand_2 = RandomForestClassifier(**top3_params[2])\n",
    "rf_cand_2.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_2, \"Training-RF-2\", x_train, y_train, top3_params[2], only_metrics=True)\n",
    "evaluate_model(rf_cand_2, \"Validation-RF-2\", x_val, y_val, top3_params[2], only_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "rf_best = rf_cand_0\n",
    "y_pred = rf_best.predict(x_test)\n",
    "evaluate_model(rf_best, \"RF-best\", x_test, y_test, rf_best.get_params(), only_metrics=False)\n",
    "# Calculate OvR AUC ROC score\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "calculate_OvR_roc_auc_score(rf_best, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Naive Bayes model & fit on training data\n",
    "nb_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "nb_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict on the training data\n",
    "y_train_pred = nb_model.predict(x_train)\n",
    "\n",
    "# Calculate the training accuracy score\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "print(nb_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "nb_param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0],  # Smoothing parameter for MultinomialNB\n",
    "    'fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=nb_model, param_grid=nb_param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "top3_models = sorted_results[:3]\n",
    "print_top3_models(top3_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_params = top3_models['params'].values\n",
    "\n",
    "# Evaluate the top 3 models on the validation set\n",
    "nb_cand_0 = MultinomialNB(**top3_params[0])\n",
    "nb_cand_0.fit(x_train, y_train)\n",
    "evaluate_model(nb_cand_0, \"Training-NB-0\", x_train, y_train, top3_params[0], only_metrics=True)\n",
    "evaluate_model(nb_cand_0, \"Validation-NB-0\", x_val, y_val, top3_params[0], only_metrics=True)\n",
    "\n",
    "nb_cand_1 = MultinomialNB(**top3_params[1])\n",
    "nb_cand_1.fit(x_train, y_train)\n",
    "evaluate_model(nb_cand_1, \"Training-NB-1\", x_train, y_train, top3_params[1], only_metrics=True)\n",
    "evaluate_model(nb_cand_1, \"Validation-NB-1\", x_val, y_val, top3_params[1], only_metrics=True)\n",
    "\n",
    "nb_cand_2 = MultinomialNB(**top3_params[2])\n",
    "nb_cand_2.fit(x_train, y_train)\n",
    "evaluate_model(nb_cand_2, \"Training-NB-2\", x_train, y_train, top3_params[2], only_metrics=True)\n",
    "evaluate_model(nb_cand_2, \"Validation-NB-2\", x_val, y_val, top3_params[2], only_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "nb_best = nb_cand_0\n",
    "y_pred = nb_best.predict(x_test)\n",
    "evaluate_model(rf_best, \"NB-best\", x_test, y_test, nb_best.get_params(), only_metrics=False)\n",
    "calculate_OvR_roc_auc_score(nb_best, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training &  Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the SVM model\n",
    "svm_model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': [0.1, 1, 'scale']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(svm_model, param_grid=svm_param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "top3_models = sorted_results[:3]\n",
    "print_top3_models(top3_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_params = top3_models['params'].values\n",
    "\n",
    "# Evaluate the top 3 models on the validation set\n",
    "svm_cand_0 = SVC(**top3_params[0])\n",
    "svm_cand_0.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_0, \"Training-SVM-0\", x_train, y_train, top3_params[0], only_metrics=True)\n",
    "evaluate_model(rf_cand_0, \"Validation-SVM-0\", x_val, y_val, top3_params[0], only_metrics=True)\n",
    "\n",
    "svm_cand_1 = SVC(**top3_params[1])\n",
    "svm_cand_1.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_1, \"Training-SVM-1\", x_train, y_train, top3_params[1], only_metrics=True)\n",
    "evaluate_model(rf_cand_1, \"Validation-SVM-1\", x_val, y_val, top3_params[1], only_metrics=True)\n",
    "\n",
    "\n",
    "svm_cand_2 = SVC(**top3_params[2])\n",
    "svm_cand_2.fit(x_train, y_train)\n",
    "evaluate_model(rf_cand_2, \"Training-SVM-2\", x_train, y_train, top3_params[2], only_metrics=True)\n",
    "evaluate_model(rf_cand_2, \"Validation-SVM-2\", x_val, y_val, top3_params[2], only_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "svm_best = svm_cand_0\n",
    "y_pred = svm_best.predict(x_test)\n",
    "evaluate_model(rf_best, \"SVM-best\", x_test, y_test, svm_best.get_params(), only_metrics=False)\n",
    "calculate_OvR_roc_auc_score(svm_best, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
