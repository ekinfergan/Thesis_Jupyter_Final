{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from numpy import asarray\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, RocCurveDisplay, confusion_matrix, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate, GRU, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.legacy import Adam, SGD, RMSprop, Adagrad\n",
    "\n",
    "import skopt\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer  \n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "# DATASET\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "# Define a dictionary to map sentiment values to category names\n",
    "senti_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_categories = list(senti_labels.values())\n",
    "NUM_of_CLASSES = 3\n",
    "\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/\"\n",
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/src/input/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.66859001 -0.34494999  0.68182999 ...  1.27419996  0.56130999\n",
      "   0.30846   ]\n",
      " [ 0.31551999  0.81511003  0.067789   ... -0.37785     0.42570001\n",
      "  -0.19459   ]]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(input_folder_path, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(input_folder_path, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(input_folder_path, \"test.csv\"))\n",
    "\n",
    "x_train = train['x']\n",
    "y_train = train['y']\n",
    "x_val = val['x']\n",
    "y_val = val['y']\n",
    "x_test = test['x']\n",
    "y_test = test['y']\n",
    "\n",
    "x_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_x.npy\"))\n",
    "y_train_encoded = np.load(os.path.join(processed_folder_path, \"train_encoded_y.npy\"))\n",
    "x_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_x.npy\"))\n",
    "y_val_encoded = np.load(os.path.join(processed_folder_path, \"val_encoded_y.npy\"))\n",
    "x_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_x.npy\"))\n",
    "y_test_encoded = np.load(os.path.join(processed_folder_path, \"test_encoded_y.npy\"))\n",
    "\n",
    "w2v_embedding_vectors = np.load(os.path.join(processed_folder_path, \"embedding_matrix.npy\"))\n",
    "print(w2v_embedding_vectors)\n",
    "\n",
    "%store -r embedding_vocab_size\n",
    "%store -r EMBEDDING_DIM\n",
    "%store -r max_seq_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(score):\n",
    "    acc =  score[1]\n",
    "    loss = score[0]\n",
    "\n",
    "    print(f\"Accuracy: {acc:.2%}\")\n",
    "    print(f\"Loss: {loss:.2f}\")\n",
    "    \n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_report(y, y_pred, labels):\n",
    "    report = classification_report(y, y_pred, labels=labels)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cnf_mat = confusion_matrix(y_true, y_pred)\n",
    "    mat_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_mat, display_labels=labels)\n",
    "    mat_disp = mat_disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, x_encoded, y_encoded, y=None, only_metrics=True):    \n",
    "    y_pred_prob = model.predict(x_encoded)\n",
    "\n",
    "    print(f\"*{model_name}\")\n",
    "    \n",
    "    score = model.evaluate(x_encoded, y_encoded, verbose=0)\n",
    "    calculate_metrics(score)\n",
    "    \n",
    "    senti_labels = ['negative', 'neutral', 'positive'] #TODO: to constants\n",
    "    \n",
    "    if not only_metrics:\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1) + 1\n",
    "        calculate_classification_report(y, y_pred, labels=senti_labels)\n",
    "        plot_confusion_matrix(y, y_pred, labels=senti_labels)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(prob_test_vec, y_test, labels):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels\n",
    "    colors = cycle(['limegreen', 'dodgerblue', 'red'])\n",
    "    for senti, color in zip(range(NUM_of_CLASSES), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_test[:, senti],\n",
    "            prob_test_vec[:, senti],\n",
    "            name=f\"ROC curve for {labels[senti]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_OvR_roc_auc_score(model, x, y, x_test, y_test, labels): #average??\n",
    "    #y = one_hot_encode(y)\n",
    "    #y_test = one_hot_encode(y_test)\n",
    "\n",
    "    ovr_model = OneVsRestClassifier(model).fit(x, y)\n",
    "    prob_test_vec = ovr_model.predict_proba(x_test)\n",
    "    \n",
    "    fpr, tpr, thresholds, auc_score = [], [], [], []\n",
    "    for _ in range(NUM_of_CLASSES):\n",
    "        fpr.append(0)\n",
    "        tpr.append(0)\n",
    "        thresholds.append(0)\n",
    "        auc_score.append(0)\n",
    "    \n",
    "    for i in range(NUM_of_CLASSES):\n",
    "        fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], prob_test_vec[:, i])\n",
    "        auc_score[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    print(f\"AUC score: {auc_score}\")\n",
    "    averaged_auc_score = (sum(auc_score) / NUM_of_CLASSES)\n",
    "    print(f\"Averaged AUC score: {averaged_auc_score:.2f}\")\n",
    "    \n",
    "    plot_roc_curve(prob_test_vec, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_development(history):\n",
    "    acc =  history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "    \n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a basic neural net to see the baseline for accuracy with minimum tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_classes = 3\n",
    "batch_size= 16\n",
    "epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_layer (Embedding)  (None, 561, 100)         2300300   \n",
      "                                                                 \n",
      " hidden_layer (GRU)          (None, 64)                31872     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,332,367\n",
      "Trainable params: 2,332,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 02:39:19.574345: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-06-15 02:39:19.574396: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1554 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/hidden_layer/PartitionedCall]] [Op:__inference_train_function_23067]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m----> 9\u001b[0m basic_history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train_encoded, y_train_encoded, validation_data\u001b[39m=\u001b[39;49m(x_val_encoded, y_val_encoded), batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(x_test_encoded, y_test_encoded)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNaive model Accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/hidden_layer/PartitionedCall]] [Op:__inference_train_function_23067]"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# input layer is sequence of integers (words)\n",
    "model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, name=\"embedding_layer\")) # part of input layer as it transforms integers into dense vectors, input shape = (None, max_seq_length)\n",
    "model.add(GRU(64, name='hidden_layer')) # hidden layer\n",
    "model.add(Dense(num_output_classes, activation='softmax', name=\"output_layer\"))\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "basic_history = model.fit(x_train_encoded, y_train_encoded, validation_data=(x_val_encoded, y_val_encoded), batch_size=batch_size, epochs=10)\n",
    "\n",
    "accuracy = model.evaluate(x_test_encoded, y_test_encoded)[1]\n",
    "print(f\"Naive model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "del model\n",
    "\n",
    "backend.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our naive model, we get an accuracy of x%. # TODO: x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gru_layers = Integer(low=1, high=5, name='num_gru_layers')\n",
    "num_gru_units = Integer(low=32, high=256, name='num_gru_units') # TODO: step Keras Tuner\n",
    "learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "adam_decay = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n",
    "#batch_size = Integer(low=1, high=128, name='batch_size')\n",
    "\n",
    "search_space = [\n",
    "            num_gru_layers,\n",
    "            num_gru_units,\n",
    "            learning_rate,\n",
    "            adam_decay\n",
    "            ]\n",
    "\n",
    "# Specify one or more initial points for the search of optimal parameter\n",
    "default_params = [1,\n",
    "                  32,\n",
    "                  1e-3, \n",
    "                  1e-3,\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gru_model(num_gru_layers, num_gru_units, learning_rate, adam_decay):\n",
    "    # Start the model making process and create our first layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length))\n",
    "\n",
    "    # Create a loop making a new GRU layer for the amount passed to this model.\n",
    "    # Naming the layers helps avoid tensorflow error deep in the stack trace.\n",
    "    for i in range(num_gru_layers):\n",
    "        name = 'layer_gru_{0}'.format(i+1)\n",
    "        if i < num_gru_layers-1:\n",
    "            model.add(GRU(num_gru_units, return_sequences=True, name=name))\n",
    "        else:\n",
    "            model.add(GRU(num_gru_units, return_sequences=False, name=name))\n",
    "\n",
    "    # Add our classification layer.\n",
    "    model.add(Dense(num_output_classes, activation='softmax'))\n",
    "\n",
    "    # Setup our optimizer and compile\n",
    "    adam = Adam(learning_rate=learning_rate, decay=adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=search_space)\n",
    "def objective_function(num_gru_layers, num_gru_units, learning_rate, adam_decay):\n",
    "\n",
    "    model = define_gru_model(num_gru_layers=num_gru_layers,\n",
    "                         num_gru_units=num_gru_units,\n",
    "                         learning_rate=learning_rate,\n",
    "                         adam_decay=adam_decay\n",
    "                         )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    loss = history.history['val_loss'][-1]\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Loss: {loss:.2}\\n\")\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 561, 100)          2300300   \n",
      "                                                                 \n",
      " layer_gru_1 (GRU)           (None, 32)                12864     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,313,263\n",
      "Trainable params: 2,313,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 02:29:31.892331: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-06-15 02:29:31.892374: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1554 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/layer_gru_1/PartitionedCall]] [Op:__inference_train_function_6453]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gp_result \u001b[39m=\u001b[39m gp_minimize(func\u001b[39m=\u001b[39;49mobjective_function,\n\u001b[1;32m      2\u001b[0m                             dimensions\u001b[39m=\u001b[39;49msearch_space,\n\u001b[1;32m      3\u001b[0m                             n_calls\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m                             noise\u001b[39m=\u001b[39;49m \u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m                             n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m                             kappa \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m                             x0\u001b[39m=\u001b[39;49mdefault_params)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/skopt/optimizer/gp.py:259\u001b[0m, in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m base_estimator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     base_estimator \u001b[39m=\u001b[39m cook_estimator(\n\u001b[1;32m    256\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGP\u001b[39m\u001b[39m\"\u001b[39m, space\u001b[39m=\u001b[39mspace, random_state\u001b[39m=\u001b[39mrng\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax),\n\u001b[1;32m    257\u001b[0m         noise\u001b[39m=\u001b[39mnoise)\n\u001b[0;32m--> 259\u001b[0m \u001b[39mreturn\u001b[39;00m base_minimize(\n\u001b[1;32m    260\u001b[0m     func, space, base_estimator\u001b[39m=\u001b[39;49mbase_estimator,\n\u001b[1;32m    261\u001b[0m     acq_func\u001b[39m=\u001b[39;49macq_func,\n\u001b[1;32m    262\u001b[0m     xi\u001b[39m=\u001b[39;49mxi, kappa\u001b[39m=\u001b[39;49mkappa, acq_optimizer\u001b[39m=\u001b[39;49macq_optimizer, n_calls\u001b[39m=\u001b[39;49mn_calls,\n\u001b[1;32m    263\u001b[0m     n_points\u001b[39m=\u001b[39;49mn_points, n_random_starts\u001b[39m=\u001b[39;49mn_random_starts,\n\u001b[1;32m    264\u001b[0m     n_initial_points\u001b[39m=\u001b[39;49mn_initial_points,\n\u001b[1;32m    265\u001b[0m     initial_point_generator\u001b[39m=\u001b[39;49minitial_point_generator,\n\u001b[1;32m    266\u001b[0m     n_restarts_optimizer\u001b[39m=\u001b[39;49mn_restarts_optimizer,\n\u001b[1;32m    267\u001b[0m     x0\u001b[39m=\u001b[39;49mx0, y0\u001b[39m=\u001b[39;49my0, random_state\u001b[39m=\u001b[39;49mrng, verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    268\u001b[0m     callback\u001b[39m=\u001b[39;49mcallback, n_jobs\u001b[39m=\u001b[39;49mn_jobs, model_queue_size\u001b[39m=\u001b[39;49mmodel_queue_size)\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/skopt/optimizer/base.py:282\u001b[0m, in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m# evaluate y0 if only x0 is provided\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39mif\u001b[39;00m x0 \u001b[39mand\u001b[39;00m y0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     y0 \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(func, x0))\n\u001b[1;32m    283\u001b[0m     n_calls \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(y0)\n\u001b[1;32m    284\u001b[0m \u001b[39m# record through tell function\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/thesis/lib/python3.10/site-packages/skopt/utils.py:789\u001b[0m, in \u001b[0;36muse_named_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    786\u001b[0m arg_dict \u001b[39m=\u001b[39m {dim\u001b[39m.\u001b[39mname: value \u001b[39mfor\u001b[39;00m dim, value \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(dimensions, x)}\n\u001b[1;32m    788\u001b[0m \u001b[39m# Call the wrapped objective function with the named arguments.\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m objective_value \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49marg_dict)\n\u001b[1;32m    791\u001b[0m \u001b[39mreturn\u001b[39;00m objective_value\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mobjective_function\u001b[0;34m(num_gru_layers, num_gru_units, learning_rate, adam_decay)\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m define_gru_model(num_gru_layers\u001b[39m=\u001b[39mnum_gru_layers,\n\u001b[1;32m      5\u001b[0m                      num_gru_units\u001b[39m=\u001b[39mnum_gru_units,\n\u001b[1;32m      6\u001b[0m                      learning_rate\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m      7\u001b[0m                      adam_decay\u001b[39m=\u001b[39madam_decay\n\u001b[1;32m      8\u001b[0m                      )\n\u001b[1;32m     10\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train_encoded,\n\u001b[1;32m     12\u001b[0m                     y_train_encoded,\n\u001b[1;32m     13\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(x_val_encoded, y_val_encoded),\n\u001b[1;32m     14\u001b[0m                     epochs\u001b[39m=\u001b[39;49mepochs, \u001b[39m# TODO\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m                     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     16\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[early_stopping]\n\u001b[1;32m     17\u001b[0m                     )\n\u001b[1;32m     18\u001b[0m \u001b[39m#return the validation accuracy for the last epoch.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m accuracy \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/layer_gru_1/PartitionedCall]] [Op:__inference_train_function_6453]"
     ]
    }
   ],
   "source": [
    "gp_result = gp_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gp_best_params = {param.name: value for param, value in zip(gp_result.space, gp_result.x)}\n",
    "print(\"Best Hyperparameters:\", gp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_gru_model(gp_best_params['num_gru_layers'], \n",
    "                          gp_best_params['num_gru_units'], \n",
    "                          gp_best_params['learning_rate'], \n",
    "                          gp_best_params['adam_decay']\n",
    "                          )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                    y_train_encoded,\n",
    "                    validation_data=(x_val_encoded, y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train single-GRU\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val single-GRU\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test single-GRU\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_result = gbrt_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
    "print(\"Best Hyperparameters:\", gbrt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_gru_model(gbrt_best_params['num_gru_layers'], \n",
    "                          gbrt_best_params['num_gru_units'], \n",
    "                          gbrt_best_params['learning_rate'], \n",
    "                          gbrt_best_params['adam_decay']\n",
    "                          )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                    y_train_encoded,\n",
    "                    validation_data=(x_val_encoded, y_val_encoded),\n",
    "                    epochs=epochs, # TODO\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train single-GRU\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val single-GRU\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test single-GRU\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gru_layersA = Integer(low=1, high=5, name='num_gru_layersA')\n",
    "num_gru_layersB = Integer(low=1, high=5, name='num_gru_layersB')\n",
    "num_gru_unitsA = Integer(low=32, high=256, name='num_gru_unitsA')\n",
    "num_gru_unitsB = Integer(low=32, high=256, name='num_gru_unitsB')\n",
    "\n",
    "search_space = [\n",
    "            num_gru_layersA,\n",
    "            num_gru_layersB,\n",
    "            num_gru_unitsA,\n",
    "            num_gru_unitsB,\n",
    "            learning_rate,\n",
    "            adam_decay\n",
    "            ]\n",
    "\n",
    "# Specify one or more initial points for the search of optimal parameter\n",
    "default_params = [1, \n",
    "                  1, \n",
    "                  32,\n",
    "                  32, \n",
    "                  1e-3,\n",
    "                  1e-3 \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_multi_channel_gru_model(num_gru_layersA, num_gru_layersB, num_gru_unitsA, num_gru_unitsB, learning_rate, adam_decay):\n",
    "    # Vocabulary-based embedding layer\n",
    "    inputsA = Input(shape=(max_seq_length,), name=\"input regular embeddings\")\n",
    "    # Word2Vec embedding layer\n",
    "    inputsB = Input(shape=(max_seq_length,), name=\"input word2vec embeddings\")\n",
    "    \n",
    "    # Define an embedding layer for each input\n",
    "    embeddingsA = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, name=\"embeddingsA\")(inputsA)\n",
    "    embeddingsB = Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length, weights=[w2v_embedding_vectors], trainable=False, name=\"embeddingsB\")(inputsB)\n",
    "    \n",
    "    # Pass both embeddings through their own GRU layers\n",
    "    gru_layersA = embeddingsA\n",
    "    for i in range(num_gru_layersA):\n",
    "        nameA = 'layer_gruA_{0}'.format(i+1)\n",
    "        if i < num_gru_layers-1:\n",
    "            gru_layersA = GRU(num_gru_unitsA, return_sequences=True, name=nameA)(gru_layersA)\n",
    "        else:\n",
    "            gru_layersA = GRU(num_gru_unitsA, return_sequences=False, name=nameA)(gru_layersA)\n",
    "        \n",
    "    gru_layersB = embeddingsB\n",
    "    for i in range(num_gru_layersB):\n",
    "        nameA = 'layer_gruB_{0}'.format(i+1)\n",
    "        if i < num_gru_layers-1:\n",
    "            gru_layersA = GRU(num_gru_unitsB, return_sequences=True, name=nameA)(gru_layersB)\n",
    "        else:\n",
    "            gru_layersA = GRU(num_gru_unitsB, return_sequences=False, name=nameA)(gru_layersB)\n",
    "        \n",
    "\n",
    "    # Concatenate the two inputs\n",
    "    merged = concatenate([gru_layersA, gru_layersB])\n",
    "\n",
    "    # Dense layer for the merged inputs & output Layer\n",
    "    outputs = Dense(num_output_classes, activation='softmax', name=\"output\")(merged)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[inputsA, inputsB], outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    adam = Adam(learning_rate=learning_rate, decay=adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=search_space)\n",
    "def multi_objective_function(num_gru_layersA, num_gru_layersB, num_gru_unitsA, num_gru_unitsB, learning_rate, adam_decay, batch_size):\n",
    "\n",
    "    model = define_multi_channel_gru_model(num_gru_layersA=num_gru_layersA,\n",
    "                                            num_gru_layersB=num_gru_layersB,\n",
    "                                            num_gru_unitsA=num_gru_unitsA,\n",
    "                                            num_gru_unitsB=num_gru_unitsB,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            adam_decay=adam_decay\n",
    "                                            )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Loss: {loss:.2}\\n\")\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return -accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_result = gp_minimize(func=multi_objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gp_best_params = {param.name: value for param, value in zip(gp_result.space, gp_result.x)}\n",
    "print(\"Best Hyperparameters:\", gp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_gru_model(gp_best_params['num_gru_layersA'],\n",
    "                                        gp_best_params['num_gru_layersB'],\n",
    "                                        gp_best_params['num_gru_unitsA'], \n",
    "                                        gp_best_params['num_gru_unitsB'],\n",
    "                                        gp_best_params['learning_rate'], \n",
    "                                        gp_best_params['adam_decay']\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=epochs, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-GRU\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-GRU\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-GRU\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_result = gbrt_minimize(func=objective_function,\n",
    "                            dimensions=search_space,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data frame summarizing parameter search\n",
    "gbrt_best_params = {param.name: value for param, value in zip(gbrt_result.space, gbrt_result.x)}\n",
    "print(\"Best Hyperparameters:\", gbrt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_multi_channel_gru_model(gbrt_best_params['num_gru_layersA'],\n",
    "                                        gbrt_best_params['num_gru_layersB'],\n",
    "                                        gbrt_best_params['num_gru_unitsA'], \n",
    "                                        gbrt_best_params['num_gru_unitsB'],\n",
    "                                        gbrt_best_params['learning_rate'], \n",
    "                                        gbrt_best_params['adam_decay'],\n",
    "                                        )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # TODO: should I, again?\n",
    "history = model.fit(x_train_encoded,\n",
    "                        y_train_encoded,\n",
    "                        validation_data=(x_val_encoded, y_val_encoded),\n",
    "                        epochs=20, # TODO\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "plot_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_encoded, y_train_encoded)\n",
    "evaluate_model(model, \"Train multi-GRU\", x_train_encoded, y_train_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "evaluate_model(model, \"Val multi-GRU\", x_val_encoded, y_val_encoded, only_metrics=True)\n",
    "\n",
    "model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "evaluate_model(model, \"Test multi-GRU\", x_test_encoded, y_test_encoded, y_test, only_metrics=False)\n",
    "senti_labels = ['negative', 'neutral', 'positive'] # TODO\n",
    "#calculate_OvR_roc_auc_score(model, x_train, y_train, x_test, y_test, senti_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
