{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "import gensim.downloader as api\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "senti_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "senti_categories = list(senti_labels.values())\n",
    "NUM_of_CLASSES = 3\n",
    "\n",
    "input_folder_path = \"./pls/Thesis_Jupyter_Final/input/\"\n",
    "processed_folder_path = \"./pls/Thesis_Jupyter_Final/processed\"\n",
    "data_filename = \"reviews_data.csv\"\n",
    "cleaned_data_filename = \"cleaned_data.csv\"\n",
    "vocab_filename = 'vocab.txt'\n",
    "w2v_filename =  'weight_matrix_word2vec.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(input_folder_path, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(input_folder_path, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(input_folder_path, \"test.csv\"))\n",
    "\n",
    "x_train = train['Review']\n",
    "y_train = train['Sentiment']\n",
    "x_val = val['Review']\n",
    "y_val = val['Sentiment']\n",
    "x_test = test['Review']\n",
    "y_test = test['Sentiment']\n",
    "\n",
    "vocab = pd.read_csv(os.path.join(processed_folder_path, vocab_filename))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "MAX_FEATURES = 10000\n",
    "MAX_DF = 0.95\n",
    "MIN_DF = 5\n",
    "\n",
    "# Word2Vec\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer with the filtered vocabulary\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES, # maximum number of features to keep, check unique vocabs and determine based on that, high causes saprse metrics and low value causes loss in important words/vocab\n",
    "    vocabulary=vocab,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1, 1),  # range of n-grams, only unigrams now\n",
    "    max_df=MAX_DF,  # ignore terms that have a document frequency strictly higher than the threshold\n",
    "    min_df=MIN_DF,  # ignore terms that have a document frequency strictly lower than the threshold.\n",
    "    use_idf=True,  # enable IDF weighting\n",
    "    smooth_idf=True,  # smooth IDF weights --> provides stability, reduces run time errors\n",
    "    sublinear_tf=True  # apply sublinear scaling to term frequencies\n",
    ")\n",
    "\n",
    "# Fit and transform the training set\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Transform the validation and testing set\n",
    "x_val_tfidf = tfidf_vectorizer.transform(x_val)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given vocabulary-size : 1541,\n",
      "\n",
      "Data Shape:\n",
      "* train: (6400, 1541)\n",
      "* validation: (1600, 1541)\n",
      "* test: (2000, 1541)\n",
      "\n",
      "x_train_tfidf:\n",
      "  (0, 15)\t0.21488294804730057\n",
      "  (0, 14)\t0.3258705571787054\n",
      "  (0, 13)\t0.2754694528444137\n",
      "  (0, 12)\t0.2274723012657554\n",
      "  (0, 11)\t0.18810094400150487\n",
      "  (0, 10)\t0.28388400052650353\n",
      "  (0, 9)\t0.36035657104380786\n",
      "  (0, 8)\t0.2748752517308079\n",
      "  (0, 7)\t0.2875212896711316\n",
      "  (0, 6)\t0.1775564539318653\n",
      "  (0, 5)\t0.199508163741704\n",
      "  (0, 3)\t0.3097449723672214\n",
      "  (0, 2)\t0.26822893504871026\n",
      "  (0, 1)\t0.22079750360155437\n",
      "  (0, 0)\t0.16728041506367827\n",
      "  (1, 18)\t0.545190792811988\n",
      "  (1, 17)\t0.4024854525010473\n",
      "  (1, 16)\t0.6169315424330162\n",
      "  (1, 6)\t0.40020985983516544\n",
      "  (2, 933)\t0.2663099426984769\n",
      "  (2, 34)\t0.18229784732070112\n",
      "  (2, 33)\t0.26345337034491717\n",
      "  (2, 32)\t0.3049939672889147\n",
      "  (2, 31)\t0.28948020585724704\n",
      "  (2, 30)\t0.15548796677139815\n",
      "  :\t:\n",
      "  (6399, 942)\t0.2297989266474506\n",
      "  (6399, 888)\t0.18581248936403078\n",
      "  (6399, 835)\t0.22246469755266957\n",
      "  (6399, 644)\t0.2114056082225976\n",
      "  (6399, 602)\t0.23192016338798635\n",
      "  (6399, 575)\t0.18372494587279456\n",
      "  (6399, 513)\t0.12500280870555505\n",
      "  (6399, 444)\t0.19807887762567325\n",
      "  (6399, 426)\t0.15690336276857908\n",
      "  (6399, 424)\t0.18804166630605632\n",
      "  (6399, 398)\t0.14219021692860703\n",
      "  (6399, 351)\t0.1785862004707396\n",
      "  (6399, 309)\t0.1832231382353078\n",
      "  (6399, 297)\t0.18036326902136085\n",
      "  (6399, 250)\t0.1658412153581154\n",
      "  (6399, 217)\t0.1416235173414858\n",
      "  (6399, 127)\t0.1892156397374511\n",
      "  (6399, 97)\t0.13596040203583784\n",
      "  (6399, 46)\t0.13828721120672158\n",
      "  (6399, 44)\t0.11365455831557668\n",
      "  (6399, 24)\t0.22415059333522178\n",
      "  (6399, 23)\t0.14533714265552108\n",
      "  (6399, 18)\t0.15290811535594573\n",
      "  (6399, 11)\t0.11891162630891423\n",
      "  (6399, 0)\t0.17904951662199592\n"
     ]
    }
   ],
   "source": [
    "print(\"Given vocabulary-size : {},\".format(vocab_size))\n",
    "print(\"\\nData Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_tfidf.shape, x_val_tfidf.shape, x_test_tfidf.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Types:\n",
      "x_train_tfidf - type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "x_val_tfidf - type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "y-train - type: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nData Types:\\nx_train_tfidf - type: {type(x_train_tfidf)}\\nx_val_tfidf - type: {type(x_val_tfidf)}\\ny-train - type: {type(y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tfidf_data(data, filename, feature_names):\n",
    "    # Save the matrix with feature names as a DataFrame\n",
    "    data = pd.DataFrame(data.toarray(), columns=feature_names)\n",
    "    file_path = os.path.join(processed_folder_path, filename)\n",
    "    data.to_csv(file_path, sep=',', index=False) # TODO: if this isn't working, note that you added sep=','\n",
    "\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Save vectorized data\n",
    "save_tfidf_data(x_train_tfidf, \"train_tfidf.csv\", feature_names)\n",
    "save_tfidf_data(x_val_tfidf, \"val_tfidf.csv\", feature_names)\n",
    "save_tfidf_data(x_test_tfidf, \"test_tfidf.csv\", feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 238\n"
     ]
    }
   ],
   "source": [
    "# Find maximum sequence length\n",
    "max_seq_length = max([len(review.split()) for review in x_train])\n",
    "print(f'Maximum review length: {max_seq_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tokenizer (on training data)\n",
    "tokenizer = Tokenizer()\n",
    "# Remove default filters, including punctuation\n",
    "tokenizer.filters = \"\"  \n",
    "# Disable lowercase conversion\n",
    "tokenizer.lower = False  \n",
    "tokenizer.fit_on_texts(x_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(lines, tokenizer, max_length):\n",
    "    # Integer encode\n",
    "    encoded_seq = tokenizer.texts_to_sequences(lines)\n",
    "    # Pad the encoded sequences\n",
    "    padded = pad_sequences(encoded_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded-data shapes:\n",
      "* train: (6400, 238)\n",
      "* validation: (1600, 238)\n",
      "* test: (2000, 238)\n",
      "\n",
      "x_train_encoded[:3]:\n",
      "[[ 130  167    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   8    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   5   24  109   26    1 1203   29  273    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Encode Data\n",
    "x_train_encoded = encode_text(x_train, tokenizer, max_seq_length)\n",
    "x_val_encoded = encode_text(x_val, tokenizer, max_seq_length)\n",
    "x_test_encoded = encode_text(x_test, tokenizer, max_seq_length)\n",
    "\n",
    "print(\"Encoded-data shapes:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_encoded.shape, x_val_encoded.shape, x_test_encoded.shape))\n",
    "print(f\"x_train_encoded[:3]:\\n{x_val_encoded[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target-data shapes:\n",
      "* train: (6400,)\n",
      "* validation: (1600,)\n",
      "* test: (2000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restructure labels\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "y_test = y_test.values\n",
    "print(\"target-data shapes:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train.shape, y_val.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_encoded_data(data, filename):\n",
    "    # Save the encoded arrays\n",
    "    file_path = os.path.join(processed_folder_path, filename)\n",
    "    np.save(file_path, np.array(data))\n",
    "\n",
    "save_encoded_data(x_train_encoded, \"train_encoded_x.csv\")\n",
    "save_encoded_data(x_val_encoded, \"val_encoded_x.csv\")\n",
    "save_encoded_data(x_test_encoded, \"test_encoded_x.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_vocab_size:  1543\n"
     ]
    }
   ],
   "source": [
    "# Total vocabulary size plus 0 for unknown words\n",
    "embedding_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"embedding_vocab_size: \", embedding_vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking why embedding vocab_size is 2 greater than original vocab size due to 'empty'\n",
    "    #TODO: remove rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in tokenizer but not in vocab:\n",
      "{'<empty>'}\n"
     ]
    }
   ],
   "source": [
    "# Convert the tokenizer word index into a set\n",
    "tokenizer_words = set(tokenizer.word_index.keys())\n",
    "\n",
    "# Convert the manual vocabulary into a set\n",
    "vocab_set = set(vocab)\n",
    "\n",
    "# Find the words in tokenizer but not in vocab\n",
    "tokenizer_only_words = tokenizer_words.difference(vocab_set)\n",
    "\n",
    "print(\"Words in tokenizer but not in vocab:\")\n",
    "print(tokenizer_only_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: gigaword or twitter?\n",
    "def load_embedding():\n",
    "    # Check if the pre-trained Word2Vec model is already downloaded\n",
    "    #w2v_pretrained_model = \"glove-twitter-100\"\n",
    "    w2v_pretrained_model = \"glove-wiki-gigaword-100\"\n",
    "    w2v_pretrained_model_filename = str(w2v_pretrained_model) + \"-word2vec.txt\"\n",
    "    if not os.path.exists(w2v_pretrained_model_filename):\n",
    "        print(\"\\nw2v model doesn't exist\")\n",
    "        # If the model does not exist, download it\n",
    "        model = api.load(\"glove-twitter-100\")\n",
    "        # Save the word2vec embeddings in the appropriate format\n",
    "        model.save_word2vec_format(w2v_pretrained_model_filename, binary=False)\n",
    "\n",
    "    # load embedding into memory, skip first line\n",
    "    print(\"Loading w2v model...\")\n",
    "    file = open(w2v_pretrained_model_filename, 'r', encoding='utf8')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading w2v model...\n"
     ]
    }
   ],
   "source": [
    "raw_embedding = load_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matrix(embedding, tokenizer):\n",
    "    # create a weight matrix for the Embedding layer from a loaded embedding\n",
    "\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((embedding_vocab_size, EMBEDDING_DIM))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    count_all = 0\n",
    "    count_na = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # TODO: important note, pretrained word2vec model removes all neg_ and emojis (also other words) that are\n",
    "        #  not defined in the model it These values should prob? also be removed from the vocab (and update vocab size) to avoid mismatch in the embedding layer\n",
    "        if word in embedding.keys():\n",
    "            # print(embedding.get(word)[:3])\n",
    "            weight_matrix[i] = embedding.get(word)\n",
    "        else:\n",
    "            #print(word)\n",
    "            count_na += 1\n",
    "        count_all += 1\n",
    "    print(f'count_na/count_all: {str(count_na)}/{count_all}')\n",
    "    print(f\"embedding matrix shape: {weight_matrix.shape}\")\n",
    "\n",
    "    # save model in ASCII (word2vec) format\n",
    "    file_path = os.path.join(processed_folder_path, w2v_filename)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(' '.join(str(x) for x in row) for row in weight_matrix))\n",
    "    \n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_na/count_all: 450/1542\n",
      "embedding matrix shape: (1543, 100)\n"
     ]
    }
   ],
   "source": [
    "w2v_embedding_vectors = get_weight_matrix(raw_embedding, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    y_encoded = np.zeros((len(y), NUM_of_CLASSES))\n",
    "    for i, label in enumerate(y):\n",
    "        y_encoded[i, label - 1] = 1\n",
    "\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment labels to one-hot encoding\n",
    "\n",
    "y_train_encoded = one_hot_encode(y_train)\n",
    "y_val_encoded = one_hot_encode(y_val)\n",
    "y_test_encoded = one_hot_encode(y_test)\n",
    "\n",
    "print(\"Check one-hot encoding:\\n\", y_train_encoded[:3])    \n",
    "print(\"\\ny-encoded Data Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape))\n",
    "print(\"\\nx_train_encoded - type:\", type(x_train_encoded))\n",
    "print(\"y_train_encoded - type:\", type(y_train_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_encoded_data(y_train_encoded, \"train_encoded_y.csv\")\n",
    "save_encoded_data(y_val_encoded, \"val_encoded_y.csv\")\n",
    "save_encoded_data(y_test_encoded, \"test_encoded_y.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
