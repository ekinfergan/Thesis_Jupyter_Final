{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b6a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('words')\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "#from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea063ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "DATASET_COLUMNS = ['Id', 'Review', 'Sentiment']\n",
    "# Define a dictionary to map sentiment values to category names\n",
    "sentiment_labels = {1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
    "\n",
    "\n",
    "\n",
    "# PROCESSING\n",
    "MIN_FREQ = 2\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7a0eb01",
   "metadata": {},
   "source": [
    "Goal of project: \n",
    "\n",
    "This notebook includes: (steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5416c0b",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "First, we load and explore the dataset and apply some initial processing such as setting the '*Id*' column as index and removing any empty rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1152d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing(data):\n",
    "    # Remove any rows with missing values and reset the index\n",
    "    data.replace('', np.nan, inplace=True)\n",
    "    data = data.dropna()\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97b6f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Review     1000 non-null   object\n",
      " 1   Sentiment  1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "\n",
      "Dataset shape: (1000, 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good and interesting</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This class is very helpful to me. Currently, I...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like!Prof and TAs are helpful and the discussi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Easy to follow and includes a lot basic and im...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Really nice teacher!I could got the point eazl...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Great course - I recommend it for all, especia...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>One of the most useful course on IT Management!</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I was disappointed because the name is mislead...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Super content. I'll definitely re-do the course</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>One of the excellent courses at Coursera for i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                               good and interesting          3\n",
       "1  This class is very helpful to me. Currently, I...          3\n",
       "2  like!Prof and TAs are helpful and the discussi...          3\n",
       "3  Easy to follow and includes a lot basic and im...          3\n",
       "4  Really nice teacher!I could got the point eazl...          3\n",
       "5  Great course - I recommend it for all, especia...          3\n",
       "6    One of the most useful course on IT Management!          3\n",
       "7  I was disappointed because the name is mislead...          2\n",
       "8    Super content. I'll definitely re-do the course          3\n",
       "9  One of the excellent courses at Coursera for i...          3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "raw_dataset_path = \"input/reviews_data.csv\"\n",
    "df_raw = pd.read_csv(raw_dataset_path)\n",
    "df_raw = df_raw[:1000]\n",
    "\n",
    "# Set ID as index\n",
    "df_raw.set_index('Id', inplace=True, drop=True)\n",
    "\n",
    "# Remove NaN rows, before cleaning text\n",
    "df_raw = drop_missing(df_raw)\n",
    "\n",
    "# Create a copy of the original DataFrame to preserve the original data\n",
    "df = df_raw.copy()\n",
    "\n",
    "print(df_raw.info())\n",
    "print(f'\\nDataset shape: {df_raw.shape}\\n')\n",
    "df_raw.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "622a91fc",
   "metadata": {},
   "source": [
    "### Analysing Data (TODO)\n",
    "We then analyse the dataset by observing the distribution of review per sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb16161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Positive): 892 reviews\n",
      "1 (Negative): 56 reviews\n",
      "2 (Neutral): 52 reviews\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGZCAYAAAAUzjLvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcg0lEQVR4nO3dd3gUVcMF8DO72U02m94T0huEEnrvRRDBgmADkaCfKIqIglhfwBcVUOFFUIqiAgqIIqCCja5IEaQoSE+AAEkIKZBedu/3R2QlJCF97272/HzySGZnZ07KJidz78woQggBIiIislkq2QGIiIhILpYBIiIiG8cyQEREZONYBoiIiGwcywAREZGNYxkgIiKycSwDRERENo5lgIiIyMaxDBAREdk4lgELs3TpUiiKYnpzcHCAn58fevfujRkzZuDy5ctlnjNt2jQoilKt/eTm5mLatGnYvn17tZ5X3r5CQ0MxePDgam2nMitXrsTcuXPLfUxRFEybNq1O91fXtmzZgnbt2kGv10NRFKxfv77c9c6ePVvq661SqeDu7o6+ffvi559/rteM1/e9dOnSet1PQ/LTTz+hf//+CAgIgL29PQICAtCrVy/MnDmzXvd76dIlTJs2DYcOHSrzWE1e/zIsWLCA32uWTJBF+fTTTwUA8emnn4rdu3eLX375RaxZs0ZMmDBBuLq6Cg8PD7Fp06ZSz0lMTBS7d++u1n5SU1MFADF16tRqPa+8fYWEhIhBgwZVazuVGTRokAgJCSn3sd27d4vExMQ63V9dMhqNwsPDQ3Tq1Els3rxZ7N69W6Snp5e7bkJCggAgnnnmGbF7926xc+dOsWTJEhEUFCTUarXYsWNHveXMz88Xu3fvFpcvX663fTQkCxcuFADE0KFDxddffy22bdsmli9fLp588knRtm3bet33vn37TD8XblaT178MzZo1Ez179pQdgypgJ7WJUIWaN2+Odu3amd4fOnQonnvuOXTr1g333nsvTp06BV9fXwBAYGAgAgMD6zVPbm4uHB0dzbKvynTq1Enq/itz6dIlpKenY8iQIejbt2+VnhMcHGz6uLp27YqoqCj07NkTH3/8MXr06FEvOe3t7S3+c2lu17/PyzNjxgz06NEDa9asKbV85MiRMBqN5ohXLkt4TZL14zCBFQkODsbs2bORlZWFxYsXm5aXd5hw69at6NWrFzw9PaHT6RAcHIyhQ4ciNzcXZ8+ehbe3NwDg9ddfNx2ijouLK7W9AwcOYNiwYXB3d0dERESF+7pu3bp1iI2NhYODA8LDwzFv3rxSj18fAjl79myp5du3b4eiKKYhi169emHjxo04d+5cqUPo15U3THDkyBHcfffdcHd3h4ODA1q1aoVly5aVu59Vq1bh1VdfRUBAAFxcXNCvXz+cOHGi4k/8DXbu3Im+ffvC2dkZjo6O6NKlCzZu3Gh6fNq0aaYfzC+++CIURUFoaGiVtn2j60UwJSWl1PLk5GQ88cQTCAwMhFarRVhYGF5//XUUFxcDAIqKiuDj44ORI0eW2WZmZiZ0Oh2ef/55ABUPE5w6dQrDhw+Hj48P7O3tERMTgw8++MD0uBACvr6+ePrpp03LDAYD3N3doVKpSmWeM2cO7OzskJmZCQCIj4/Hgw8+aDrM7uvri759+5Z7+PtGcXFxcHJywtGjR9G3b1/o9Xp4e3tj3LhxyM3NLbWuEAILFixAq1atoNPp4O7ujmHDhiE+Pr7Uer169ULz5s3xyy+/oEuXLnB0dMSjjz5aYYa0tDT4+/uX+5hKVfpHaXUz7Nu3D927d4ejoyPCw8Mxc+ZMU8HYvn072rdvDwAYPXq06fVw/TVwq6G7DRs2oHXr1tDpdIiJicGGDRsAlLwWY2JioNfr0aFDB+zfv7/Mx7R//37cdddd8PDwgIODA1q3bo0vv/yy1DrXX9Pbtm3D2LFj4eXlBU9PT9x77724dOlSqTxHjx7Fjh07TPlr8rqgeiT5yATd5Powwb59+8p9PDs7W6jVatG3b1/TsqlTp4obv5QJCQnCwcFB3HbbbWL9+vVi+/btYsWKFWLkyJEiIyND5Ofnix9//FEAEI899pjYvXu32L17tzh9+nSp7YWEhIgXX3xRbNq0Saxfv77cfQlRMkzQqFEjERwcLD755BPx/fffixEjRggA4p133inzsSUkJJR6/rZt2wQAsW3bNiGEEEePHhVdu3YVfn5+pmw3HgbFTcMbx48fF87OziIiIkIsX75cbNy4UTz00EMCgJg1a1aZ/YSGhooRI0aIjRs3ilWrVong4GARFRUliouLb/m12b59u9BoNKJt27Zi9erVYv369aJ///5CURTxxRdfCCFKDtmuXbu21KH/AwcOVLjN68MEN36ehBDiyJEjpm1cl5SUJIKCgkRISIhYvHix2Lx5s5g+fbqwt7cXcXFxpvWee+45odPpxNWrV0ttc8GCBQKA+PPPP0vt+8ZDz0ePHhWurq6iRYsWYvny5eLnn38WEydOFCqVSkybNs203oMPPiiio6NN7+/Zs0cAEDqdTqxYscK0fODAgaJDhw6m9xs3biwiIyPFZ599Jnbs2CG+/vprMXHiRNPXviKjRo0SWq1WBAcHizfffFP8/PPPYtq0acLOzk4MHjy41LqPP/640Gg0YuLEieLHH38UK1euFE2aNBG+vr4iOTnZtF7Pnj2Fh4eHCAoKEvPnzxfbtm275bBMv379hJ2dnZg6dao4dOjQLb9fqpPB09NTREVFiUWLFolNmzaJp556SgAQy5YtE0IIcfXqVdNr57XXXjO9Hq4PlVX0mgwMDBTNmzcXq1atEt9//73o2LGj0Gg0YsqUKaJr165i7dq1Yt26dSI6Olr4+vqK3Nxc0/O3bt0qtFqt6N69u1i9erX48ccfRVxcXJnvl+u5wsPDxTPPPCN++uknsWTJEuHu7i569+5tWu/AgQMiPDxctG7d2pT/Vq8LMj+WAQtTWRkQQghfX18RExNjev/mHwZr1qwRAMShQ4cq3Mat5gxc396UKVMqfOxGISEhQlGUMvu77bbbhIuLi8jJySn1sVVWBoS49ZyBm3M/+OCDwt7eXpw/f77UegMHDhSOjo4iMzOz1H7uuOOOUut9+eWXAkCl466dOnUSPj4+Iisry7SsuLhYNG/eXAQGBgqj0SiEqPgXfHmurztr1ixRVFQk8vPzxaFDh0Tnzp2Fv79/qc/VE088IZycnMS5c+dKbePdd98VAMTRo0eFEEL8+eefAoD48MMPS63XoUOHUmPb5ZWBAQMGiMDAwDJFYty4ccLBwcE092HJkiUCgOlz/sYbb4gmTZqIu+66S4wePVoIIURhYaHQ6/XilVdeEUIIceXKFQFAzJ07t9LPy81GjRolAIj33nuv1PI333xTABA7d+4UQpTMJwEgZs+eXWq9xMREodPpxOTJk03LevbsKQCILVu2VCnD6dOnRfPmzQUAU/Hp27eveP/990VhYaFpvZpk2Lt3b6l1mzZtKgYMGGB6/1ZzBip6Tep0OnHhwgXTskOHDgkAwt/f3/SaFEKI9evXCwDi22+/NS1r0qSJaN26tSgqKiq13cGDBwt/f39hMBiEEP++pp966qlS67399tsCgEhKSjIt45wBy8ZhAiskhLjl461atYJWq8WYMWOwbNmyMocmq2ro0KFVXrdZs2Zo2bJlqWXDhw/HtWvXcODAgRrtv6q2bt2Kvn37IigoqNTyuLg45ObmYvfu3aWW33XXXaXej42NBQCcO3euwn3k5ORg7969GDZsGJycnEzL1Wo1Ro4ciQsXLlR5qKE8L774IjQajWmI48iRI/juu+9KHUrdsGEDevfujYCAABQXF5veBg4cCADYsWMHAKBFixZo27YtPv30U9Nzjx07ht9///2Wh8Hz8/OxZcsWDBkyBI6OjqX2cccddyA/Px979uwBAPTr1w8AsHnzZgDApk2bcNttt6Ffv37YtGkTAGD37t3Iyckxrevh4YGIiAi88847mDNnDg4ePFjtsfYRI0aUen/48OEAgG3btpk+R4qi4OGHHy6V38/PDy1btixz9oy7uzv69OlTpX1HRETg8OHD2LFjB15//XX069cP+/btw7hx49C5c2fk5+fXKIOfnx86dOhQallsbOwtvx+rolWrVmjUqJHp/ZiYGAAlQxM3zou4vvz6/k6fPo3jx4+bPtc3fx8kJSWV+V6vyWuKLAvLgJXJyclBWloaAgICKlwnIiICmzdvho+PD55++mlEREQgIiIC7733XrX2VdH4aHn8/PwqXJaWllat/VZXRWO51z9HN+/f09Oz1Pv29vYAgLy8vAr3kZGRASFEtfZTHc8++yz27duHnTt34t1330VRURHuvvvuUttMSUnBd999B41GU+qtWbNmAIArV66Y1n300Uexe/duHD9+HADw6aefwt7eHg899FCFGdLS0lBcXIz58+eX2ccdd9xRah8hISGm77Prhet6GbhejDZv3gydTocuXboAKJnrsWXLFgwYMABvv/022rRpA29vb4wfPx5ZWVmVfo7s7OzKfO1u/h5LSUkxzWm4+WPYs2dPqc8RUL3vcaBkbkCPHj0wZcoUfPvtt7h06RIeeOAB/PHHH/jkk09qlOHmjwko+Z681fdjVXh4eJR6X6vV3nL59TJzfc7HpEmTyuR/6qmnAKDSj6EqrymyLDybwMps3LgRBoMBvXr1uuV63bt3R/fu3WEwGLB//37Mnz8fEyZMgK+vLx588MEq7as65y4nJydXuOz6DwoHBwcAQEFBQan1bv7BUl2enp5ISkoqs/z6BCYvL69abR+AaXJcfe0nMDDQNGmwa9eu8PPzw8MPP4ypU6fi/fffN20/NjYWb775ZrnbuLEgPvTQQ3j++eexdOlSvPnmm/jss89wzz33wN3d/ZYf4/UjHTdODrxRWFiY6d99+/bFN998gx07dsBoNKJXr15wdnZGQEAANm3ahM2bN6N79+6mXwxASYn4+OOPAQAnT57El19+iWnTpqGwsBCLFi265eeouLgYaWlppX7x3Pw95uXlBUVR8Ouvv5ba73U3L6vt+fl6vR4vv/wyVq9ejSNHjtQog6W5/n388ssv49577y13ncaNG5szEpkBy4AVOX/+PCZNmgRXV1c88cQTVXqOWq1Gx44d0aRJE6xYsQIHDhzAgw8+WOfN/ejRozh8+HCpoYKVK1fC2dkZbdq0AQDTIe8///yz1A+Tb7/9tsz2qvOXUd++fbFu3TpcunSp1C/E5cuXw9HRsU5On9Pr9ejYsSPWrl2Ld999FzqdDgBgNBrx+eefIzAwENHR0bXez3UjRozAkiVL8NFHH+GFF15ASEgIBg8ejO+//x4RERG3/KUOlPxiv+eee7B8+XJ07twZycnJtxwiAABHR0f07t0bBw8eRGxsrOkvxor069cPH374IebOnYtOnTrB2dkZwL9fj3379uGtt96q8PnR0dF47bXX8PXXX1d5KGnFihUYP3686f2VK1cCgKkcDx48GDNnzsTFixdx//33V2mbVZWUlFTukYRjx44B+LeM1UcGc/6l3bhxY0RFReHw4cO3/PpVV10c7aD6wzJgoY4cOWIap7t8+TJ+/fVXfPrpp1Cr1Vi3bp3p1MDyLFq0CFu3bsWgQYMQHByM/Px80yHM6+O3zs7OCAkJwTfffIO+ffvCw8MDXl5eNT7dJyAgAHfddRemTZsGf39/fP7559i0aRNmzZplGp9s3749GjdujEmTJqG4uBju7u5Yt24ddu7cWWZ7LVq0wNq1a7Fw4UK0bdsWKpWq1HUXbjR16lTTePqUKVPg4eGBFStWYOPGjXj77bfh6upao4/pZjNmzMBtt92G3r17Y9KkSdBqtViwYAGOHDmCVatW1flV4GbNmoWOHTti+vTpWLJkCf773/9i06ZN6NKlC8aPH4/GjRsjPz8fZ8+exffff49FixaVOt/80UcfxerVqzFu3DgEBgaavva38t5776Fbt27o3r07xo4di9DQUGRlZeH06dP47rvvsHXrVtO6ffr0gaIo+Pnnn/H666+blvfr1w+jRo0y/fu6P//8E+PGjcN9992HqKgoaLVabN26FX/++SdeeumlSrNptVrMnj0b2dnZaN++PXbt2oU33ngDAwcORLdu3QCUHFUZM2YMRo8ejf3796NHjx7Q6/VISkrCzp070aJFC4wdO7byT345mjVrhr59+2LgwIGIiIhAfn4+9u7di9mzZ8PX1xePPfZYvWWIiIiATqfDihUrEBMTAycnJwQEBNxyuLA2Fi9ejIEDB2LAgAGIi4tDo0aNkJ6ejmPHjuHAgQP46quvqr3NFi1a4IsvvsDq1asRHh4OBwcHtGjRoh7SU43Inb9IN7s+O/f6m1arFT4+PqJnz57irbfeKvdqcTfPJt69e7cYMmSICAkJEfb29sLT01P07Nmz1GxhIYTYvHmzaN26tbC3txcAxKhRo0ptLzU1tdJ9CfHvFQjXrFkjmjVrJrRarQgNDRVz5swp8/yTJ0+K/v37CxcXF+Ht7S2eeeYZsXHjxjJnE6Snp4thw4YJNzc3oShKqX2inLMg/vrrL3HnnXcKV1dXodVqRcuWLcvMvL5+NsFXX31Vanl5s+or8uuvv4o+ffoIvV4vdDqd6NSpk/juu+/K3V51ziaoaN377rtP2NnZmU77TE1NFePHjxdhYWFCo9EIDw8P0bZtW/Hqq6+K7OzsUs81GAwiKChIABCvvvpqhfu++eNOSEgQjz76qGjUqJHQaDTC29tbdOnSRbzxxhtlttG6dWsBQPz222+mZRcvXhQAhKenp+kMCyGESElJEXFxcaJJkyZCr9cLJycnERsbK/73v/9VelrnqFGjhF6vF3/++afo1auX0Ol0wsPDQ4wdO7bMxy2EEJ988ono2LGj6esUEREhHnnkEbF//37TOj179hTNmjW75X5vtHjxYnHvvfeK8PBw4ejoKLRarYiIiBBPPvlkuVfErE2GUaNGlTmbZtWqVaJJkyZCo9GUeg3c6jV5MwDi6aefLrWsou/Bw4cPi/vvv1/4+PgIjUYj/Pz8RJ8+fcSiRYtM61R09lN5ZwidPXtW9O/fXzg7O5tOXSbLoQhRydR0IiLJ4uLisGbNGmRnZ8uOQtQg8WwCIiIiG8cyQEREZOM4TEBERGTjeGSAiIjIxrEMEBER2TiWASIiIhvHMkBERGTjWAaIiIhsHMsAERGRjWMZICIisnEsA0RERDaOZYCIiMjGsQwQERHZOJYBIiIiG8cyQEREZONYBoiIiGwcywAREZGNYxkgIiKycSwDRERENo5lgIiIyMaxDBAREdk4lgEiIiIbxzJARERk4+xkByCi2hNC4JrhGtKL05FelF76/8XpyCzORIGxAAZhgBFGGIQBxaIYBmGArkjBokePAmo1YGdX+v8aDeDiAnh7V/zm4QGo+HcFkTVjGSCycEZhxPmC8ziddxqXCi+V+iV//d8ZxRkoFsU12r5bgQbYv7/mAVUqwNPz33Lg5QX4+QGRkUDjxiVvoaEsDEQWjGWAyIKkFaXhdN5pnM47jVN5p3A6/zTi8+JRIApkR6uY0Qikppa8VcTeHoiIAKKjgZgYoEULIDa2pCjY8ccQkWyKEELIDkFka/KN+YjPizf9wr9eANKL082exa1Agy1d9pp9vwBKSkJMTEkxiI0FOnYEOnQAtFo5eYhsFCs5kRlkGbKwP2s/9l7bi31Z+3C+4DyMMMqOJV9BAXDoUMnbdY6OQOfOQK9eJW8sB0T1jkcGiOpBkSjCkZwj2HNtD/Zm7cXfOX/DAIPsWOWSemSgKlgOiOodywBRHYnPi8eerD34/drv+CP7D+Qac2VHqhKLLwM30+lKl4OOHVkOiGqJZYCohtKK0rA3ay/2XtuL37N+x+Wiy7Ij1YjVlYGbubgAgwYBQ4cCAweWHEkgomphGSCqhsziTGzO2IyfMn7CweyDELD+l4/Vl4EbOToCt99eUgwGDy4pCkRUKZYBokrkGnKxPXM7fsz4EXuu7bHYsf+aalBl4Aap/e/Bq8+sw92Ngb5hgAOnSxNViC8PonIYhAF7ru3Bd2nf4Zerv1j2ef5Urh9aDsNPZ4CfzgAuWmBAJHBPY6BLEKBSZKcjsiw8MkB0g8T8RHyT9g02pm+02jkA1dUQjwwIBwd0eOsyLqucyzzmqwfujAYebgGEuUsIR2SBeGSAbF6eIQ+bMjfhmyvf4FDOIdlxqA6kdB9YbhEAgJQcYMlB4JNDQJ9Q4NHWQNcgs8YjsjgsA2Sz0orSsOLyCnx95WtkG7Jlx6E69H2bBypdxyiAzQklb029gNGtgLsbA/b8qUg2iMMEZHMSCxLxWcpn2JC2gXMB0PCGCYSjI9q+cRlpKn21n+vtWDJ88HAs4MUzFMmGsAOTzTieexzLUpZhS8aWBndGAP0rqefgGhUBAEjNBf63F1iwH7irMfB/rYEmXnUckMgCsQxQg7cvax+WJi/Fnqw9sqOQGXzb8v5ab6PAAHz1d8lb1yDgsdYl8wsUnoVADRTLADVIQghsu7oNy5KX4UjuEdlxyEyEkxMW+NxRp9v8LbHkLdwNGNMWuK8pYKeq010QSccyQA1KkSjCD+k/YFnyMpwtOCs7DpnZhV534apKVy/bjs8EXtoCLP4DmNgZGBzFIwXUcLAMUIMghMCG9A1YeGkhUopSZMchSda3rPwsgtpKyATG/QAs+gOY3AXoGVLvuySqdzybgKzeXzl/4d3EdzkcUEMN5WwC4eqK5lNTkK2yN+t+OwcCL3YFWvuZdbdEdYpHBshqpRalYv7F+fg+/fsGccMgqp3zve42exEAgN0XgHtWl1zV8KWuQCDvjURWiGWArE6hsRArLq/AJ8mfINeYKzsOWYivW9T/EMGtfHcS+PkM8H9tgKfbAXqt1DhE1cJhArIq2zK3Ye7FubhQcEF2lAajIQwTGD080PQ/ychTNLKjAAB89MCkziVnHvCmSGQNeGSArEJ8XjzevfAu9mZZ9y8tqh9ne95jMUUAAC7nAJM3A8sOA9N7A239ZSciujWWAbJo14qvYVHSIqxJXcOrBlKFvpI8RFCRo6nAsK9KLlo0qTPgwJ+4ZKH4rUkWa+2VtXj/4vu4argqOwpZMKOXFz527yM7RoWMAvjoALA1AXj3NqANjxKQBWIZIIuTVpSG18+9jt+u/SY7ClmBM72HokCx/B9lZzJKjhL8XxtgYifeHZEsCy+qSRZle+Z2PHDsARYBqrIvmtb+XgTmYhAlVzActAo4nCw7DdG/WAbIIuQZ8jD93HRMjJ+IjOIM2XHIShh9fLHMrafsGNV2Kh0Y8iXw9m9AIafCkAVgGSDp/sr5Cw8dfwjr09bLjkJW5mTvYShS1LJj1IhBAB/sBwavAv7iFbRJMpYBksYgDPgw6UM8duIxJBYkyo5DN5kGQLnp7VZX3F0L4DYA3gBcAHQG8NNN62wCEA3AFcAoAIU3PHb1n8fOVyPjiqaWeRZBdZxIA+75EnhnF48SkDwsAyRFYkEiHjv5GBYnLeYpgxasGYCkG97+usW6v6CkDHwP4A8AvQHcCeDgP48bAYwA8CSAXQB+B/DRDc9/8Z/HgquYzeAfgM9culVxbctWbATe3wfc9UXJ6YhE5sb5rGR266+sx+wLs3kpYStgh1sfDbjR3JvefwvANwC+A9AawBUAqQCeAuAA4C4Af/+z7m8A9gP4oBrZjve6D6KB3UP42BVgyGrg9V7AQ81lpyFbwiMDZDaZxZmYdGYSpp+fziJgJU4BCAAQBuBBAPHVeK4RQBYAj3/e9wbgD+BnAHkAfgUQi5KhgrEAFgGozuj/Zw1giKA8BQbgpS0lVzAsKJadhmwFywCZxd85f2P4seHYdnWb7ChURR0BLEfJuP9HAJIBdAGQVsXnzwaQA+D6iX8KgC8BTAfQFCVHCx4FMBNAXwA6AF0BNAbwfiXbLg4KxhdOnar6oVil1UdLrktw4ZrsJGQLOExA9e6H9B8w/dx0FIgC2VGoGgbe8O8WKJkQGAFgGYDnK3nuKpRMQPwGgM8Ny7sB2HfD+ycBfIaSeQU9AEwAcDuA5v+8H1vB9o/2bHhDBOX583LJ2Qbzbgd6hMhOQw0ZjwxQvTEKI+ZdnIfXzr7GItAA6FFSCk5Vst5qAI+h5ChAv1usJwCMQckRBCNKCsEwlJSHngB23OK5y5o0zCGC8mTkA49+K7AikUfVqP6wDFC9yDZk47kzz2FZyjLZUaiOFAA4hpJx/4qsAhAHYCWAQZVs72MAniiZSHj9fJKiG/5f0TkmRaHh+NqpfRUSNxydmv2COamT8FrCaygwslhT3WMZoDqXmJ+IuBNx2Hltp+woVAuTUPLXeQKAvSj5q/0aSq4PAAAvA3jkhvVX/fP+bACdUDLHIBkl1w+42WUAbwCY98/77gBiUHJGwm4AW1AyP6E8f/W0nssP14UOYRdxzK1kYOaHjB8w5uQYXCm6IjkVNTQsA1SnDmcfRtzJOCTkJ8iOQrV0AcBDKJnQdy8ALYA9AK4PXSeh9AWCFgMoBvA0So4eXH97tpxtP4uSstHohmVLAXwBYDCAFwB0qCDXJ9G2UwYivfJwIWBEqWVHco/gkeOP4HjucUmpqCFShBBCdghqGH7O+BnTzk7j/AAr41agwZYue2XHqJKiiChEPnNSdgyz8NAZ4dU6Dunqo+U+7qBywPSQ6ehjwbdvJuvBIwNUJ5YlL8MrCa+wCFC9OtjDNiYOalQC4bHvVlgEACDfmI/JCZOx+vJqMyajhoqnFlKtGIQBsxJn4esrX8uOQjZgSZRtlIFOLTbhmLbyX/ICAm9feBsFogCP+D5S6fpEFWEZoBrLN+bjxfgXOVGQzKKgcVP85Njwr9HbKeIsjrm8XK3nvHfxPeQb8zHGf0w9paKGjmWAaqTAWIDnzzyPvVnWMdZM1u9A94onDmb+MA3Xfnq91DKVsy8CpydX+BxRXICrP/0XOfs/h+FaMuzcAuFy26tw6vQoACDvxCZkrHkahqwUOLa4Bx4PfATFTgsAMOZdRfKc9vB5ajPs3Kt6a6XKNfbJxVn/h2v03MVJi5FvzMf4RuPrLA/ZDpYBqjYWAZJhUcSthwg0fs3g89Tmfxeobn2ngytL74chKwWeD34MO69IGLIvA8aSmwEIoxFpn42AS9+X4NBkAK58OgzZuz+Cc/enAQCZ370Ipy5P1mkR8HI0wBD1GIqVvBpvY1nKMuQb8/FC4AtQbOAKjVR3WAaoWgqMBZgYPxF7svbIjkI2JD+mBbbrmtx6JZUd1C5Vu8di3rEfkX96BwL+Ew+1vuRWSnaeoabHjTlXYMxOhXO3p6BoHKBrfheKUkrusVgQ/xsKE/fDfVh17rF4a1q1QHDsWzivqv2ZEqtTV6PAWIBXg1+FSuEccaoafqdQlRUaCzExfiJ2X9stOwrZmH3dK584WHzlFC5OCcDF/4bhyrIHUXyl4nss5h35Ftrgdsja+jYuTm2ES29GI+ObSTAWlvxVrnLyhtrFH3knfoaxMA8F8b9C4x8LUVyI9K/Gwv2+RVAqOfJQHR1afIfzmvV1tr31aesx5ewUGERF13EkKo1lgKqk0FiISfGTWARIioXhty4D9iEd4TliObyf/AmeD3wEw7VkJL/XBYac8u+xWJwWj4L4nShMOgKvR9fBfchc5B5ag4w1JcMAiqLAM+5LXPtpOpJmNoU2sDWcOj2Ka5tnwiG6LxSNDsnvdcWlNxsj69fK7rF4a52jTuO48+uVr1hNP2T8gJcTXkaRKKp8ZbJ5vOgQVep6Efjt2m+yo1A9sPSLDuXFtkGTR/+o1nOMBTm49EYEXPpMhkvvsvdYvLywPwrif0Wj/yZDpXMFAOQeXosrS4chcFYOVFpdmecUXT6J1A8Hwe+Fg0iZ3wPOPSZAF3M7kmY1h89Tm6ENqOgeixVr6peFtMgBMCj1d32Obi7d8E74O9CqtPW2D7J+PDJAt1RkLMIL8S+wCJA0e7pU//LDKns9tP4tUJxa/j0W1S7+ULs2MhUBAND4xgBCwHD1Qpn1hRBIXz0GbvfMBoQRRRcOwrHVMKidfWAf0RMFp291j8Xy+ToZkBcRV69FAAB2XtuJCWcmIM9Y84mJ1PCxDFCFrhcBXkeAZFoQVv0yIIoLUJRyDGqX8u+xaB/WFYarl2AsyDYtK0o9CSgqqF0Dy6yfs+djqPSecGx+F2D8ZxzeUGT6vzBWb2zewU7Av/kUZKnOVut5NbU3ay8mnJ6AIiOHDKh8LANUriJjESYnTMav136VHYVsWE7rDvjdPqzS9TK+mYT80ztQnJaAgrN7kfrpMBjzr0HfoeQei5nfvYwrn/97hT7HtsOh0nsibeVoFCX/jfwzvyDz2xeg7/homSECQ9ZlXP35DbjfW3KPRZWjO+x8Y5C1Yy4KEnYj/9QW2IdVdI/F8rWNXYMLmh+r9Zza2p+9H9POTQNHhqk8PLWQyigSRXgx4UX8cvUX2VHIxv3WuWqXHzZkXkDa8odgyLkCtZM3tCGd4PfcHth5lNxj0XAtCYaMf++xqLJ3gs/YTcj4+hkkz25X8ld/q/vhescbZbadsfZZuPSZBDu3f++x6Dl8KdJWjELWL/Pg0vsF2IdUdI/Fsro2PooT+plVXr8u/ZjxIwK0AXi60dNS9k+WixMIqYypZ6diQ/oG2THITCx1AqFQFNwz6xwOaYNkR6kzsQGZSA6/HUZF7uH6V4Nfxb1e90rNQJaFwwRUyrLkZSwCZBFy2nZuUEUgwKUY18JGSS8CADDz/EzsurpLdgyyICwDZPJL5i94/1Ltzpkmqiu/VnGIwBo4agS8mr6EbFXZMxVkMMCAlxJewsnc2l/xkBoGlgECAJzKO4VXz74KI4yyoxBBqFR4L/g+2THqTKvYz3BJs012jFJyjDkYf2Y8kgsrvpkT2Q6WAUJ6UTqeO/Mcco25sqMQAQCy2nfDMU35pwVam24xh3DS8T3ZMcqVWpSKZ08/i2xDduUrU4PGMmDjrl9dMKkwSXYUIpPtnRrGEEGrwDSc8BwjO8Ytnc4/jcnxk3nZYhvHMmDj3jj/Bg7nHJYdg8hEqNWYFzhMdoxaC3YrQnroSAjF8m8WtDdrL94896bsGCQRy4ANW5a8DBvTN8qOQVTK1U69cErjIztGrThpBZxjJiJXSZEdpcq+S/8OHyZ9KDsGScIyYKN2ZO7gmQNkkba2r/7lhy2JAoHmsR8hxc767uexOGkxfk7/WXYMkoBlwAadyj2F186+xjMHyOIIOzu8FzhUdoxa6dbsd5zWLZYdo8beOP8GLhRYximQZD4sAzYmvSgdz8XzzAGyTBld+uKsnafsGDXWNvgyjrlb96V+c4w5eCXhFU4otDEsAzbEKIx4KeElnjlAFmtTe+s9iyDMvQApwQ8DivVf4f1o7lEsvLRQdgwyI5YBG/JZymf4I/sP2TGIyiU0GrwXMER2jBpxsRdwiBmPfCVNdpQ6szxlOfZc2yM7BpkJy4CNOJV7CguT2PTJcqV164+LajfZMapNrQjExL6PVPV+2VHqlIDAlLNTkF6ULjsKmQHLgA0oNBbitbOvcQyQLNpPba1ziKBL819xxmGp7Bj1Iq04DVPPTQVvbtvwsQzYgA8ufYDT+adlxyCqkLC3xzz/u2XHqLb2oZdwzO052THq1a5ru/D55c9lx6B6xjLQwO3P2o+Vl1fKjkF0S5e7345ktYvsGNUS6ZWHi42Gy45hFh9c+gDHco/JjkH1iGWgAcs2ZGPquam8ngBZvB/aWNcQgbvOCFXjsShUZcmOYhZFoggvJ7yMXANPSW6oWAYasLcT3+btScniCZ0O7/veKTtGldmpBCJbzEG66i/ZUcwqsSARMxNnyo5B9YRloIHanLGZ9x0gq5DccxBS1U6yY1RZ5xabkWC/SnYMKTamb8TGNP5caYhYBhqg1KJUvHX+LdkxiKpkQyvruRdBp4hzOObykuwYUs1KnMUjjg0Qy0AD9N9z/8VVw1XZMYgqJfR6fOAzSHaMKmnsnYtz/iNlx5Aux5iDdxLfkR2D6hjLQAPzZeqX2HVtl+wYRFVyseedyFA5yo5RKS9HAwzR/4ciJUd2FIuw/ep27MjcITsG1SGWgQbkUsElvHfxPdkxiKrs21aWfxaBRiUQ0mIGMlUnZEexKLMSZ/HsggaEZaABeffCu8g35suOQVQlwtkZH3gNlB2jUh1jN+Ccdp3sGBYnpSgFi5IWyY5BdYRloIHYdW0XdlzlYTuyHom97ka2yl52jFvqHHUax52nyY5hsb64/AVO5PKISUPAMtAAFBmL8G7iu7JjEFXLupaWPUTQ1Dcb8b6PyI5h0Qww4K3zb/HeBQ0Ay0ADsPLySpwrOCc7BlGVCTc3LPbsLztGhXydDMiPHA2DUiA7isU7knsE36V/JzsG1RLLgJVLLUzFkuQlsmMQVcvZXvcgR9HKjlEuBzsB/+ZTcU0VLzuK1Xj/4vvIMfBMC2vGMmDl3rv4HnKNnNFL1mVNC8sdImgX+zUuaH6QHcOqpBWnYUkS/yixZiwDVuxIzhH8mPGj7BhE1WL09MRHHv1kxyhX18bHcFw/Q3YMq7QqdRXO5XO40lqxDFixuRfnQoATd8i6xPe6FwWKnewYZbQIuIpT3qNlx7BaRaIIsy/Mlh2DaohlwEpty9yGg9kHZccgqrYvm1nevQgCnIuRFfYIjEqR7ChW7bdrv2Hn1Z2yY1ANsAxYoWJRjPkX58uOQVRtRm8ffOLeW3aMUhw1Al7NXkK26oLsKA3CgksLZEegGmAZsEJfp37NUwnJKp3qPRRFilp2jFJaxX6OS5ptsmM0GCfyTuCXzF9kx6BqYhmwMtmGbHyY/KHsGEQ1sqqpZZ1F0C3mME46zpUdo8Hh6c7Wh2XAyqy6vAqZxZmyYxBVm8HPH8tdu8uOYdIqMB0nPB+XHaNBOpp7lHdPtTIsA1Yk35iP1amrZccgqpETvYbBoFjGj5wg1yKkhz4MoRhkR2mweN0B62IZr0yqkm/TvkVGcYbsGEQ18nmMZQwROGkFXJpORK6SIjtKg3Y45zD2Ze2THYOqiGXAShiEAZ+nfC47BlGNGBoFYqVLF9kxoECgRcuPkWL3m+woNoFHB6wHy4CV2JKxBRcLL8qOQVQjf/e6D0JRZMdAt2b7cMphoewYNmN/9n4cyj4kOwZVAcuAlVh+ebnsCEQ1tryJ/CGCtsGXccz9KdkxbA7PLLAOLANW4Pdrv+NY7jHZMYhqpDg4FF86d5SaIdS9ECnBDwMKL99tbruv7cbRnKOyY1AlWAaswLKUZbIjENXYX73kXn7YxV5AF/MM8pU0qTlsGY8OWD6WAQt3Mvck9mTtkR2DqMaWRssrA2pFICb2faSq90vLQMAvV3/BidwTsmPQLbAMWDgeFSBrVhQWgfVObaXtv0uznTjjsFTa/ulfHyd/LDsC3QLLgAVLKkjCpoxNsmMQ1djhnvImDrYPvYRj7hOk7Z9K25q5FUkFSbJjUAVYBizY55c/hwG8QhpZr4+j5ZSBSM98XGw0XMq+qXwCAt+mfSs7BlWAZcBCZRZnYn3aetkxiGqsMKoJvneMNft+3XVGqBuPRaEqy+z7plv7Lv07GIVRdgwqB8uAhVp7ZS3yjfmyYxDV2MEe5p84aKcSiGwxB2nqP82+b6pcUmESfs/6XXYMKgfLgIXakLZBdgSiWlkcaf4hgs4ttiDBfpXZ90tVx6ECy8QyYIGO5hzFuYJzsmMQ1VhBTHNs0TU16z47RZzHMZcXzbpPqr7tmdtxrfia7Bh0E5YBC/R9+veyIxDVyr5u5h0iiPbOxTn/h826T6qZAlGAHzN+lB2DbsIyYGGKRTF+zvhZdgyiWlkcYb4hAk9HI0T0GBQpOWbbJ9XO+ivrZUegm7AMWJi91/YivThddgyiGstr3gq/OESbZV8alUBoixnIUPHeHdbkRN4JXpHQwrAMWBgOEZC1+72r+YYIOsV+j3PatWbbH9Wdb9K+kR2BbsAyYEFyDbnYfnW77BhEtfJBuHmGCDpHnsEx5ylm2RfVvR/Tf0ShsVB2DPoHy4AF2Za5jdcWIKuW27Id9tqH1/t+mvpmI95vZL3vh+rPVcNVbM/cLjsG/YNlwIJwiICs3a4u9X9UwEdvQH7kaBiUgnrfF9UvDhVYDpYBC3Gl6Ar2Ze2THYOoVj4Iq9/5AvZqgUYtXsc1VXy97ofM4/es35FcmCw7BoFlwGL8lP4Tb0pEVi27bScc0AbX6z7at1yLRM3Get0HmY8RRuzI3CE7BoFlwGJwiICs3W+d6neIoGv0MRzXv1Wv+yDz++3ab7IjEFgGLEJCXgKO5x2XHYOoxoSiYH7offW2/Rb+V3HKZ3S9bZ/k2Z+1HwVGzv+QjWXAAmzK3CQ7AlGtZLfvir80jepl2wHOxcgKHwWjUlQv2ye5CkQB/sj6Q3YMm8cyYAF4S0+ydjvqaYhAZyfg1fwVZKsS62X7ZBl2XtspO4LNYxmQLM+Qh79y/pIdg6jGhEqF+UHD6mXbbVquxCW7LfWybbIcu67tkh3B5rEMSHYg+wCKRbHsGEQ1dq1TTxzX+NX5drvGHMYJxzl1vl2yPIkFiUjM59EfmVgGJOMQAVm7be3r/toCrQLTcdLz8TrfLlkuDhXIxTIgGcsAWTOhVmNe0NA63WaQaxHSQx+GUHjdDVvCoQK5WAYkyijKwKm8U7JjENVYZpc+OGPnXWfb02sEXJtOQq6SUmfbJOvwR9YfvDeLRCwDEu3L3gcBITsGUY1tbl93ZxEoEIht+QmS7Xi42BYViALsz9ovO4bNYhmQ6PdrHCIg6yU0GsxrNKTOtte16X6c0i2os+2R9eHVCOVhGZCI8wXImqV37Yfzao862Vab4Ms47jG2TrZF1uu3qywDsrAMSHKp4BIuFl6UHYOoxn5uVzdDBKHuhbgc/DCgcMjM1l0svIhz+edkx7BJLAOS7M3aKzsCUY0Je3vM87+n1ttxsRdwjBmPfCWt9qGoQTiQfUB2BJvEMiDJvqx9siMQ1diVbv1xSe1aq22oFIGYlgtwWc3XAv3raM5R2RFsEsuABEIIlgGyaj+0qf0QQddmu3DG/pM6SEMNyd+5f8uOYJNYBiSIz49HenG67BhENSIcHDDf/65abaN9aBKOuY+vo0TUkJzOO83rDUjAMiDBsdxjsiMQ1VhKjztwWeVc4+dHeObjYuCIOkxEDYkBBpzIPSE7hs1hGZDgZN5J2RGIamxj65rfi8DdwQi7xmNRqFytw0TU0BzN5bwBc2MZkIBlgKyVcHTEB76Da/RcO5VAZOz/kKb+s45TUUPDSYTmxzIgwclclgGyTkk9ByNNpa/Rczs334oE+5V1nIgaIg6lmh/LgJklFybjqoGHSMk6fduqZmcRdAw/j2Ouk+s4DTVUFwoucBKhmbEMmBmPCpC1Ek5OeN/7jmo/L9o7F+cDHq6HRNRQGWBAfF687Bg2hWXAzDhfgKzVhV53IUvlUK3neDoaIaLHoEjJqadU1FDx9u7mxTJgZvH5bLtknda3rN4QgUYlENpiBjJUHP+l6mMZMC+WATM7m39WdgSiahOurljodXu1ntMp9gec066tp0TU0LEMmBfLgBkJIXC+4LzsGETVdq7n3chRtFVev3PkGRxz/k89JqKG7nT+adkRbArLgBklFyUjz5gnOwZRta2NrfoQQYxvNuL9RtZjGrIFmcWZyCjKkB3DZrAMmBGHCMgaGT088KHnbVVa10dvQGHkozAoBfWcimxBSlGK7Ag2g2XAjFgGyBol9BqCPEVT6Xr2aoFGLV7HVdUZM6QiW3C56LLsCDaDZcCMWAbIGn3VvGr3ImgfuxaJmo31nIZsSWphquwINoNlwIySCpNkRyCqFqOXFz5x71Ppel2ij+O401tmSES2JLWIZcBcWAbMKLM4U3YEomo503soChS7W67T3P8qTvuMNlMisiUcJjAflgEzyijmzFiyLqua3fosAn/nYmSHj4JRKTRTIrIlPDJgPiwDZsQyQNbE6OOLpa49K3xcZyfg0/xVZKsSzZiKbAnLgPmwDJhJvjGf1xggq3Ky9zAYlIp/RLRpuRIX7TabMRHZmsuFHCYwF5YBM+FRAbI2K5pWPETQtcmfOOE4x4xpyBZdNVxFoZFDUObAMmAmnDxI1sQQ0AifuXQr97GWjdJx0uv/zJyIbBWHCsyDZcBMeFlNsibHeg6DUJQyywNdi5AR+giEYpCQimwRzygwD5YBM+EwAVmTz8sZItBrBNybTkauitfLIPPhhYfMg2XATFgGyFoUBwXjC6dOpZYpEIht+QmS7H6RlIpsFYcJzINlwExYBshaHO11f5khgq5N/8Ap3QJJiciWcZjAPFgGzIRlgKzFsujS9yJoE5SK4x5PSkpDti7LkCU7gk1gGTATnk1A1qAoNBxfO7c3vR/iXojU4IcBRUhMRbbMIDhZ1RxYBsyERwbIGvzV89+jAi72AvqYCchTXZGYiGwdy4B5sAyYCY8MkDX4OLrkLAKVItA0dhEuq/dKTkS2zgCWAXNgGTATtluydIWR0digbwUA6NJsF047LJEbiAj82WkuLANEBAA41L1kiKB9SBKOu4+XnIaoRLEolh3BJrAMmInqFjd8IbIES6IeQLhHPi4GjZAdhciEZcA8+BvKTBSUvbQrkaUoaNwUv3s0hbbJ0yhUrsqOQ2TCYQLzYBkwE5YBsmQHug9DVOx7uKI+JDsKUSmcQGgeLANmwjJAlkoA2DLAA/H2n8uOQlQGhwnMg2XATJRy7gBHZAmu2hdhXfhy2TGIysVhAvNgGTATHhkgIqo+lgHzYBkwE5YBIqLq4zCBebAMmAnLABFR9fHIgHmwDJgJrzNARFR9PJvAPPgbykx4ZICIqPr0Kr3sCDaBZYCIiCyWq52r7Ag2gWXATDhMQERUfS5qF9kRbAJ/Q5mJRtHIjkBEZHVc7FgGzIFlwEzc7dxlRyAisjpudm6yI9gElgEz8dJ4yY5ARGR1OExgHiwDZuJp5yk7AhGR1eEwgXmwDJiJh8ZDdgQiIqvjqubZBObAMmAmnhoeGSAiqi4eGTAPlgEz8bLjnAEioupyU7vJjmATWAbMhEcGiIiqj0cGzINlwExYBoiIqkcFFZzVzrJj2ASWATNxUDnwGttERNXgpHbi1VvNhJ9lM+K1BoiIqo5DBObDMmBGHCogIqo6Xp/FfFgGzIjf2EREVRdoHyg7gs1gGTAjHhkgIqq6YPtg2RFsBsuAGXHOABFR1bEMmA/LgBkFaANkRyAishpBDkGyI9gMlgEzCteFy45ARGQ1eGTAfFgGzCjUPhRqqGXHICKyeJ52ntCreW0Wc2EZMCONSoMgex72IiKqTLADjwqYE8uAmUXoImRHICKyeOEOHFY1J5YBMwtzCJMdgYjI4kXqImVHsCksA2YW4cAjA0RElYnSRcmOYFNYBsyM3+BERJWLdOCRAXNiGTCzEIcQ6FQ62TGIiCyWr8YXzna8dbE5sQyYmUpRIVoXLTsGEZHF4nwB82MZkKCJYxPZEYiILBbLgPmxDEjAMkBEVLEW+hayI9gclgEJYhxjZEcgIrJIChS0cWojO4bNYRmQINwhHPaKvewYREQWJ0oXBVc7V9kxbA7LgARqRc2hAiKicrRzbic7gk1iGZCkg3MH2RGIiCxOW6e2siPYJJYBSTq5dJIdgYjIoqig4nwBSVgGJGmubw4ntZPsGEREFiNKFwUXOxfZMWwSy4Akdood2ju1lx2DiMhicL6APCwDEnV26Sw7AhGRxeB8AXlYBiRiGSAiKsH5AnKxDEgUYB+AIPsg2TGIiKSL1kXz5kQSsQxIxqMDRERAW2cOEcjEMiBZJ2eeYkhExMmDcrEMSNbOuR3sFDvZMYiIpFFDjdZOrWXHsGksA5Lp1XrE6mNlxyAikqaFvgWc1ZwvIBPLgAXgvAEismUDPAbIjmDzWAYsAOcNEJGtUkON29xukx3D5rEMWIAmjk3gbucuOwYRkdl1cOkAdw1//snGMmABVIoK/d37y45BRGR2A9w5RGAJWAYsxJ2ed8qOQERkVlpFi95uvWXHILAMWIwYxxhEOETIjkFEZDZdXbry7q0WgmXAggz2HCw7AhGR2fAsAsvBMmBBBnoMhBpq2TGIiOqdXqVHd9fusmPQP1gGLIi3xhsdXTrKjkFEVO96uvWEg8pBdgz6B8uAhRnswaECImr4eAaVZWEZsDC93HpxQg0RNWiuald0cuHF1iwJy4CFsVfZszETUYPW170vNIpGdgy6AcuABeJQARE1ZLzQkOVhGbBALZ1aItg+WHYMIqI6F2IfgrZObWXHoJuwDFioQR6DZEcgIqpzw32GQ1EU2THoJiwDFmqQxyAo4AuGiBoONzs3XlzNQrEMWCh/e390dOY1B4io4RjqNZTXFrBQLAMWbJTvKNkRiIjqhFbR4gHvB2THoAqwDFiwDi4d0MyxmewYRES1NtBjIDw1nrJjUAVYBizcaL/RsiMQEdXaCJ8RsiPQLbAMWLherr14a2MismqdXTojQsefY5aMZcDCKYqCON842TGIiGrsYZ+HZUegSrAMWIH+Hv3RSNtIdgwiomqLdIjkfQisAMuAFbBT7DDSd6TsGERE1TbCl3MFrAHLgJW4y/MueNpxJi4RWQ9PO08MdB8oOwZVAcuAlbBX2XM2LhFZlQe8H4BGxbsTWgOWASsyzHsYnNXOsmMQEVVKr9JjmPcw2TGoilgGrIhercf93vfLjkFEVKmRviPhaucqOwZVEcuAlXnI5yFe25uILJqnnSdPJ7QyLANWxt3OHUO9hsqOQURUocf9H4dOrZMdg6qBZcAKPer3KOcOEJFFCrYPxhCvIbJjUDWxDFghNzs3/J/f/8mOQURUxlMBT8FOsZMdg6qJZcBKPeDzAILtg2XHICIyaebYDLe53yY7BtUAy4CV0igaPNvoWdkxiIhMng98XnYEqiGWASvWy60X2jm1kx2DiAgD3AeglVMr2TGohlgGrNzEwIlQ8ctIRBI5qBwwvtF42TGoFvhbxMpFO0Zz5i4RSTXKdxT8tH6yY1AtsAw0AOMCxsHdzl12DCKyQX5aPzzi+4jsGFRLLAMNgIudCw/REZEUzwY8y6uiNgAsAw3EnR53oqW+pewYRGRDOjh3QH+P/rJjUB1gGWggFEXBy8EvQw217ChEZAP0Kj2mhEyRHYPqCMtAAxKli8KDPg/KjkFENmBi4ET4a/1lx6A6wjLQwDzh/wRfoERUr7q5dMPdXnfLjkF1iGWggdGr9ZgeOp3DBURUL1zVrngt5DXZMaiOsQw0QK2dWiPOL052DCJqgCYHTYa3xlt2DKpjLAMN1Bj/MWjm2Ex2DCJqQPq59cPtHrfLjkH1QBFCCNkhqH4kFiRi+LHhyDXmyo5CRFbO084Tq5uu5gXOGigeGWjAguyD8ELQC7JjEFED8ErwKywCDRjLQAN3l+dd6OfWT3YMIrJigzwGoZdbL9kxqB5xmMAGXCu+hgePPYiUohTZUYjIyvhqfLE6ZjWc7ZxlR6F6xCMDNsDFzgX/Df0vb3VMRNX2n5D/sAjYAP52sBHtnNthpO9I2TGIyIo84P0AOrt0lh2DzIBlwIaMDRiLGMcY2TGIyAq0dWqL5wKfkx2DzIRzBmzMufxzGH58OPKN+bKjEJGFCtAGYHmT5Tx7wIbwyICNCXEIwctBL8uOQUQWSqfSYU74HBYBG8MyYIMGew5GnG+c7BhEZGEUKHg95HVEOUbJjkJmxjJgo8YFjOP1B4iolMf8HkNf976yY5AELAM2SlEU/Df0v2ju2Fx2FCKyAL1ce+FJ/ydlxyBJOIHQxqUVpWHUiVFIKkySHYWIJIlwiMDSxkvhqHaUHYUk4ZEBG+ep8cTciLnQq/SyoxCRBK5qV8yJmMMiYONYBgiRukjMDJ8JNdSyoxCRGamhxszwmQi0D5QdhSSr02ECg8GAoqKiutocmYFWq4VKVdIJv0r9CjMTZ0pORETmMilwEh7yeUh2DLIAdnWxESEEkpOTkZmZWRebIzNSqVQICwuDVqvFfd734XzBeay8vFJ2LCKqZ3d73s0iQCZ1cmQgKSkJmZmZ8PHxgaOjIxRFqYtsVM+MRiMuXboEjUaD4OBgKIoCozBiYvxE/HL1F9nxiKie9HHrgxlhM2Cn1Mnfg9QA1LoMGAwGnDx5Ej4+PvD09KyrXGQmV69exaVLlxAZGQmNRgMAyDPk4bGTj+FE3gnJ6YiorvVw7YG3w9+GRtHIjkIWpNYTCK/PEXB05ExUa6TVagGUlLrrdGod5kbMhb/WX1YsIqoHnV06Y1bYLBYBKqPOzibg0IB1qujr5qP1wUdRH6GRtpGZExFRfWjv3B6zw2dDq9LKjkIWiKcWUoX87f3xYfSHPO2IyMq1dmqN/0X8D/Yqe9lRyEKxDNAt+Wn98FHURwi2D5YdhYhqoIW+Bd6LeA86lU52FLJg9TqVtO2BtvW5+VL+aPOH2fZVmbNnzyIsLAwHDx5Eq1atKlyvV69eaNWqFebOnWu2bDXho/XBh9Ef4omTT+BcwTnZcYioipo6NsX8yPnQq3mFUbo1mz4yEBcXB0VRoCgKNBoNwsPDMWnSJOTk5NRqu0FBQUhKSkLz5iU3Adq+fTsURSlzHYa1a9di+vTptdqXuXhrvPFh9IcIdwiXHYWIqiBaF40PIj+As9pZdhSyAjZdBgDg9ttvR1JSEuLj4/HGG29gwYIFmDRpUq22qVar4efnBzu7Wx948fDwgLOz9bxQvTReWBy1GBEOEbKjENEtRDhEYEHUArjYuciOQlbC5suAvb09/Pz8EBQUhOHDh2PEiBFYv349CgoKMH78ePj4+MDBwQHdunXDvn37TM/LyMjAiBEj4O3tDZ1Oh6ioKHz66acASoYJFEXBoUOHcPbsWfTu3RsA4O7uDkVREBcXB6BkmGDChAkAgJdffhmdOnUqky82NhZTp041vf/pp58iJiYGDg4OaNKkCRYsWFBPn5nyeWg8sDh6MaJ10WbdLxFVTYh9CBZGLYS7nbvsKGRFePmpm+h0OhQVFWHy5Mn4+uuvsWzZMoSEhODtt9/GgAEDcPr0aXh4eOA///kP/v77b/zwww/w8vLC6dOnkZeXV2Z7QUFB+PrrrzF06FCcOHECLi4u0OnKTuQZMWIEZs6ciTNnziAiouQv76NHj+Kvv/7CmjVrAAAfffQRpk6divfffx+tW7fGwYMH8fjjj0Ov12PUqFH1+4m5gbudOxZFLcLYU2Ot/sJEolgg6cMkpP+QjqK0Imi8NPAc7Am///ODoio57bIorQgX511E1p4sFGcVw7mNMwInB8Ih2KHC7V5ZewVpG9OQfyYfAOAY44iApwOgb/7v2G369+m4+P5FGPOM8LzbE4ET/j1ro+BSAU4/fRpNPmsCtRNvIEVVE+EQgQ8iP4CnhheAo+qx+SMDN/r999+xcuVK9O7dGwsXLsQ777yDgQMHomnTpvjoo4+g0+nw8ccfAwDOnz+P1q1bo127dggNDUW/fv1w5513ltmmWq2Gh4cHAMDHxwd+fn5wdXUts17z5s0RGxuLlSv/vS/AihUr0L59e0RHl/wVPn36dMyePRv33nsvwsLCcO+99+K5557D4sWL6+PTcUuudq5YFLUITR2bmn3fdSl5WTJS16QiaHIQmq5pikbjGyHlsxSkfpEKoOS+G/ET41F4sRDhc8IRszIGWn8tTo89DUOeocLtZv2RBY8BHohaHIXGnzaG1k+L00+fRuHlQgBAcUYxzr1xDo0mNELk+5FI35COq79eNT0/cUYiAp4JYBGgKmvr1BYfR38Mb6237ChkhWy+DGzYsAFOTk5wcHBA586d0aNHDzzzzDMoKipC165dTetpNBp06NABx44dAwCMHTsWX3zxBVq1aoXJkydj165dtc4yYsQIrFixAkDJL6FVq1ZhxIgRAIDU1FQkJibiscceg5OTk+ntjTfewJkzZ2q975pwsXPBgqgFaO7YXMr+60LOnzlw6+UG1+6usA+wh3s/d7h0ckHusVwAQMH5AuT8lYOgl4Ogb6aHQ6gDgl4KgiHPgIwfMyrcbtibYfC+3xuOjR3hEOaA4NeCIYRA1u9ZJdu9WAC1kxoe/T2gb6aHUzsn5CeUHEVI/yEdikaBex8e5qWquc3tNrwf+T6c7axnDhJZFpsvA71798ahQ4dw4sQJ5OfnY+3ataa/3G++Op8QwrRs4MCBOHfuHCZMmIBLly6hb9++tZ54OHz4cJw8eRIHDhzArl27kJiYiAcffBBAyU2FgJKhgkOHDpnejhw5gj179tRqv7XhrHbGouhF6OXaS1qG2nBq5YSs37OQf67kF3HuyVxkH8qGS9eSiVeisOTWHSrtvy8VRa1AsVOQfSi7yvsx5hshigXsXEpG5uyD7WHMNyL3eC6KrxYj9+9c6CJ1KL5ajKRFSQiaHFRXHyI1cCN8RmBG2AxeWZBqxebnDOj1ekRGRpZaFhkZCa1Wi507d2L48OEASu7BsH//ftOEPwDw9vZGXFwc4uLi0L17d7zwwgt49913y+yjvOv/lycwMBA9evTAihUrkJeXh379+sHX1xcA4Ovri0aNGiE+Pt50tMBS6FQ6vBP+DuZdnIfPLn8mO061+Mb5wpBtwN9D/y6pxkYg4KkAeNxeMrTjEOoArb8WF9+/iOBXg6HSqXD588soTitG0ZWiKu/n4vyL0Hpr4dyx5C83Oxc7hE4LxdkpZyEKBDwGecCliwvOvX4O3g94o+BSAc48fwaiWMB/jD/c+/EoAZWmQMGERhPwsO/DsqNQA2DzZaA8er0eY8eOxQsvvAAPDw8EBwfj7bffRm5uLh577DEAwJQpU9C2bVs0a9YMBQUF2LBhA2JiYsrdXkhICBRFwYYNG3DHHXdAp9PBycmp3HVHjBiBadOmobCwEP/73/9KPTZt2jSMHz8eLi4uGDhwIAoKCrB//35kZGTg+eefr9tPQjWpFBUmBE5AiEMIZibORLEolpqnqjJ+zkD6D+kIfTMUunAdck/m4sLsC9B4a+B5pycUjYLwd8Jx7r/n8GfvPwE14NLBxXTkoCqSlyUj46cMRH0YBZX9v0cY3Pq4wa2Pm+n9rP1ZyDudh6DJQTh6z1GEvhUKjacGxx85Dqc2TtB48OYyVMJescfroa/jNvfbZEehBqJey4AlXRWwumbOnAmj0YiRI0ciKysL7dq1w08//QR395K/0LRaLV5++WWcPXsWOp0O3bt3xxdffFHutho1aoTXX38dL730EkaPHo1HHnkES5cuLXfd++67D8888wzUajXuueeeUo/93//9HxwdHfHOO+9g8uTJ0Ov1aNGiRamjFbIN8RqCRvaNMDl+MrIMWbLjVOriexfhF+cHjwElRwJ0UToUJhUi+dNkeN5ZMiPbMcYRMatiYMgywFhshMa95Be0Y9PK79SZsjwFKZ+kIHJhJByjKl7fWGhE4sxEhE4PRf6FfAiDgHPbkqMIDiEOyDmSA7cebrX/gMnqeWu8MSd8DprqrXvyLlkWRQgharOB/Px8JCQkICwsDA4OFZ9qRZapvr5+CfkJeO7Mc0gsSKyzbdaHw30OI2BsALzv+3cGdvInyUj7Lg3N1jUr9zn55/Px99C/ETkvEi6dKz5CkLI8BUlLkhD1QRT0LW59OdhLCy7BmG9E4POByD2ei1NjT6HltpYAgGMPHYP/GH+49Xar/gdIDUoTXRP8L+J/8NH6yI5CDYzNTyCk+hHmEIbPGn+GLi5dZEe5Jdfurkj+JBlXf72KgksFyNyaicsrLpf6xZuxKQNZ+7NQcKEAmdszcfqp03Dr5VaqCJydchYX5180vZ+8LBmXFlxCyNQQaP21KLpShKIrRTDklp03kncmDxk/Z8B/rD+AknkKUIAr66/g6q9XkX82H47NKj8KQQ1bX7e+WNJ4CYsA1QvOGaB642znjPci3sOCSwvwacqnsuOUK2hyEC4tvITEmYkoyii56JDXUC/4Pe5nWqfoShEu/O8CitOKofHSwGOQR6nHAaAwuRC44eSTK19dgSgSSJicUGo9vzF+CHgiwPS+EALn3zyPwOcDodaVXFNA5aBC6LRQJM5KhLHIiKDJQdD6cKa4LXvM7zGM9R9b5gwnorrCYQIbZ66v35aMLZh2bhpyjbn1tg+ihsbNzg1Tg6eih1sP2VGogeMwAZlFX/e+WNp4KYLsef48UVV0dO6IL2K+YBEgs2AZILOJ0EXgs8afYYD7ANlRiCyWnWKHZxs9iw8iP4C3hpcWJvNgGSCzcrZzxlthb2FG6Ay4qsveo4HIlgXbB2Np46V4xPcRzg8gs2IZICn6e/TH6qar0dWla+UrE9mAuzzvwsomKxHjWP7Fy4jqE8sASeOt8ca8yHl4JegV6FRlb+tMZAuc1c6YEToDU0OmQqfm64DkYBmwQtu3b4eiKMjMzJQdpU4M9R6KVTGr0FLfUnYUIrNqqW+JVU1Wob9Hf9lRyMbV63UGQt6rz62Xdu7Z6j8nLi4Oy5Ytw4wZM/DSSy+Zlq9fvx5DhgxBLc+6NDl79izCwsJw8OBBtGrVqk622dAE2QdhSfQSfJbyGRYmLUSRqPpNgIisjRpq/J///+Exv8egVtSy4xDxyICDgwNmzZqFjIyK701vLoWFhbIjSKVSVBjlNwqfNfkM0bpo2XGI6kWIfQg+jP4QY/zHsAiQxbD5MtCvXz/4+flhxowZFa6za9cu9OjRAzqdDkFBQRg/fjxycnJMjyuKgvXr15d6jpubm+lmRGFhYQCA1q1bQ1EU9OrVC0DJkYl77rkHM2bMQEBAAKKjS34Bfv7552jXrh2cnZ3h5+eH4cOH4/Lly3X3QVu4KF0UljdejtG+o6EGf1hSw6BT6fBMwDNYHbMarZxayY5DVIrNlwG1Wo233noL8+fPx4ULF8o8/tdff2HAgAG499578eeff2L16tXYuXMnxo0bV+V9/P777wCAzZs3IykpCWvXrjU9tmXLFhw7dgybNm3Chg0bAJQcIZg+fToOHz6M9evXIyEhAXFxcbX7QK2MRqXBuEbjsCR6CSIcImTHIaqV/u79sbbpWsT5xUGj4q2oyfLw3gQAhgwZglatWmHq1Kn4+OOPSz32zjvvYPjw4abbBEdFRWHevHno2bMnFi5cWKVL+Hp7l1w4xNPTE35+pa9pr9frsWTJEmi1/157/tFHHzX9Ozw8HPPmzUOHDh2QnZ0NJyenmn6YVinWKRarYlZh/ZX1WJS0COnF6bIjEVVZhEMEJgdNRjvndrKjEN2SzR8ZuG7WrFlYtmwZ/v7771LL//jjDyxduhROTk6mtwEDBsBoNCIhIaGCrVVdixYtShUBADh48CDuvvtuhISEwNnZ2TSscP78+VrvzxqpFTWGeg/FumbrMMp3FLQKb9pDlk2v0mNi4ESsjFnJIkBWgWXgHz169MCAAQPwyiuvlFpuNBrxxBNP4NChQ6a3w4cP49SpU4iIKDl8rShKmTMPioqqNhtery99n/ucnBz0798fTk5O+Pzzz7Fv3z6sW7cOACcYOqmdML7ReHzd9Gvc5nab7DhEZShQMMhjENY1W4fhPsNhp/DgK1kHfqfeYMaMGWjdurVpIh8AtGnTBkePHkVkZGSFz/P29kZSUpLp/VOnTiE399+7813/y99gKHsv+5sdP34cV65cwcyZMxEUVHJTn/3791f7Y2nIAuwDMDN8Jh7MfhBzLszB0dyjsiMRobGuMV4MehEtnXi9DLI+PDJwg9jYWIwYMQLz5883LXvxxRexe/duPP300zh06BBOnTqFb7/9Fs8884xpnT59+uD999/HgQMHsH//fjz55JPQaP6dJOTj4wOdTocff/wRKSkpuHr1aoUZgoODodVqMX/+fMTHx+Pbb7/F9OnT6+cDtnKtnFphWeNlmB46Hb4aX9lxyEa5qF3wUtBL+LzJ5ywCZLVYBm4yffr0Uof8Y2NjsWPHDpw6dQrdu3dH69at8Z///Af+/v6mdWbPno2goCD06NEDw4cPx6RJk+Do6Gh63M7ODvPmzcPixYsREBCAu+++u8L9e3t7Y+nSpfjqq6/QtGlTzJw5E++++279fLANgKIouMPjDqxtthZP+j/JyxqT2bioXTDGbwy+afYN7vO+DyqFP07JeimilpfZy8/PR0JCAsLCwqo0s54sS0P7+qUWpWLhpYXYkLYBBlQ+LENUXZ52nhjhMwLDvIdBr9ZX/gQiK8AyYOMa6tcvqSAJK1NXYv2V9cg15lb+BKJK+Gp88YjvI7jH6x44qBrOa4UIYBmweQ3965dVnIU1V9bgi9QvcKXoiuw4ZIUC7QMR5xuHwR6DecEgarB4NgE1aM52zhjtNxoP+zyMHzJ+wOcpn+NM/hnZscgKRDhEYLTfaPR37897CFCDxzJANkGj0uAuz7twp8ed+O3ab1ieshx/ZP8hOxZZoBjHGDzq9yh6u/aGoiiy4xCZBcsA2RRFUdDNtRu6uXbDsdxjWJ6yHFsytnCyoY1TQYUuLl3wgM8D6OLSRXYcIrNjGSCbFeMYgxlhM3Ap4BJWXl6Jb9K+4WRDG+On9cPdnnfjLs+74Kf1q/wJRA0UJxDaOH79/pVnzMO2jG3YkL4B+7L2wQij7EhUD9RQo4drDwzxGoLOLp15fQAi8MgAkYlOpcMdnnfgDs87kFKYgu/Tv8fG9I1IyK/9DalIvsa6xrjD4w7c7nE7vDResuMQWRSWAaJy+Gp9MdpvNEb7jcbRnKP4If0HbMncgstFl2VHo2rw0/rhdvfbMchjEMJ14bLjEFkslgELFxoaigkTJmDChAmyo9isZvpmaKZvhomBE3Eo5xA2ZWzClswtvG6BhfLWeKObSzcM9BiINk5teEYAURXU72CZopjvrQbi4uKgKApmzpxZavn69evN/gNk6dKlcHNzK7N83759GDNmjFmzUPkURUFrp9aYHDQZPzT/AR9GfYj7vO7jTZIk0ygatHNqh/EB4/FFky/wY4sf8VrIa2jr3JZFgKiKbP7IgIODA2bNmoUnnngC7u7usuOU4e3tLTsClUOlqNDWuS3aOrfFS3gJifmJ2Je9D/uy9mF/1n6kF6fLjtigNdI2QheXLuji0gXtnNvBUe1Y+ZOIqEI2P422X79+8PPzw4wZMypcZ9euXejRowd0Oh2CgoIwfvx45OTkmB5PSkrCoEGDoNPpEBYWhpUrVyI0NBRz5841rTNnzhy0aNECer0eQUFBeOqpp5CdnQ0A2L59O0aPHo2rV69CURQoioJp06YBQKntPPTQQ3jwwQdLZSsqKoKXlxc+/fRTAIAQAm+//TbCw8Oh0+nQsmVLrFmzpg4+U3QrQQ5BuNfrXswIm4FNsZuwOmY1JgVOQi/XXnBRu8iOZ/UcVA7o6tIVLwS+gHVN1+Hb5t/ipeCX0MOtB4sAUR2w+SMDarUab731FoYPH47x48cjMDCw1ON//fUXBgwYgOnTp+Pjjz9Gamoqxo0bh3Hjxpl+AT/yyCO4cuUKtm/fDo1Gg+effx6XL5eeaKZSqTBv3jyEhoYiISEBTz31FCZPnowFCxagS5cumDt3LqZMmYITJ04AAJycnMpkHTFiBO6//35kZ2ebHv/pp5+Qk5ODoUOHAgBee+01rF27FgsXLkRUVBR++eUXPPzww/D29kbPnj3r/PNH5YvURSJSF4mHfB6CURhxIu8E9mWVHDk4lH2I1zOohF6lR5QuCs31zdHZpTPaOLWBVqWVHYuowbL5MgAAQ4YMQatWrTB16lR8/PHHpR575513MHz4cNMEvqioKMybNw89e/bEwoULcfbsWWzevBn79u1Du3btAABLlixBVFRUqe3cOAEwLCwM06dPx9ixY7FgwQJotVq4urpCURT4+VV84ZMBAwZAr9dj3bp1GDlyJABg5cqVuPPOO+Hi4oKcnBzMmTMHW7duRefOnQEA4eHh2LlzJxYvXswyIIlKUSHGMQYxjjF4xPcRFItiHM05ikPZh3Am/wzi8+NxNv8s8ox5sqNK4a3xRmNdYzR2bIxoXTQa6xoj0D6Q4/1EZsQy8I9Zs2ahT58+mDhxYqnlf/zxB06fPo0VK1aYlgkhYDQakZCQgJMnT8LOzg5t2rQxPR4ZGVlm/sG2bdvw1ltv4e+//8a1a9dQXFyM/Px85OTkQK+v2j3RNRoN7rvvPqxYsQIjR45ETk4OvvnmG6xcuRIA8PfffyM/Px+33XZbqecVFhaidevW1fp8UP2xU+zQ0qklWjq1NC0TQuBS4SXE58cjIT8BZ/LOICE/AQn5CQ3mKIIaagQ7BCNaF40mjk1Mv/jdNZY3V4fI1rAM/KNHjx4YMGAAXnnlFcTFxZmWG41GPPHEExg/fnyZ5wQHB5sO69/sxgs7njt3DnfccQeefPJJTJ8+HR4eHti5cycee+wxFBUVVSvniBEj0LNnT1y+fBmbNm2Cg4MDBg4caMoKABs3bkSjRo1KPc/e3r5a+yHzUhQFjewboZF9I3R37W5aLoRAcmEy4vPjcSa/pCDE58UjtSgVmcWZKBAFElOXZq/Yw0vjVebNW+ONMIcwROoi4aCy7atcElkqloEbzJgxA61bt0Z0dLRpWZs2bXD06FFERkaW+5wmTZqguLgYBw8eRNu2bQEAp0+fRmZmpmmd/fv3o7i4GLNnz4ZKVTJn88svvyy1Ha1WC4Oh8pvldOnSBUFBQVi9ejV++OEH3HfffdBqS8ZSmzZtCnt7e5w/f55DAg2Eoijwt/eHv70/urp2LfN4njEPV4uvIrM4E5nFmaX/bfj339ffsgxZMMJoKqvixv+EgJ1iB41KA42igVbRwk6xg1alhUbRQK/Ww8uu9C/5G//vYseJkkTWimXgBrGxsRgxYgTmz59vWvbiiy+iU6dOePrpp/H4449Dr9fj2LFj2LRpE+bPn48mTZqgX79+GDNmDBYuXAiNRoOJEydCp9OZxjwjIiJQXFyM+fPn484778Rvv/2GRYsWldp3aGgosrOzsWXLFrRs2RKOjo5wdCw7S1pRFAwfPhyLFi3CyZMnsW3bNtNjzs7OmDRpEp577jkYjUZ069YN165dw65du+Dk5IRRo0bV02eOZNGpdNBpdbzJDhHVis2fWniz6dOnlzrEHxsbix07duDUqVPo3r07Wrdujf/85z/w9/c3rbN8+XL4+vqiR48eGDJkCB5//HE4OzubbvzTqlUrzJkzB7NmzULz5s2xYsWKMqcydunSBU8++SQeeOABeHt74+23364w44gRI/D333+jUaNG6Nq19F+L06dPx5QpUzBjxgzExMRgwIAB+O677xAWFlYXnx4iImqAeNfCenDhwgUEBQVh8+bN6Nu3r+w4t8SvHxERcZigDmzduhXZ2dlo0aIFkpKSMHnyZISGhqJHjx6yoxEREVWKZaAOFBUV4ZVXXkF8fDycnZ3RpUsXrFixAhqNRnY0IiKiSrEM1IEBAwZgwIABsmMQERHVCCcQEhER2bg6KwO1nIdIkvDrRkREtS4D18fFc3MbxiVTbU1hYSGAkhs2ERGRbar1nAG1Wg03NzfTXfocHR15gxErYTQakZqaCkdHR9jZcfoIEZGtqpPfANfvtHfzbXvJ8qlUKgQHB7PAERHZsFpfdOhGBoOh2jfeIbm0Wq3pfglERGSb6rQMEBERkfXhn4REREQ2jmWAiIjIxrEMEBER2TiWASIiIhvHMkBERGTjWAaIiIhsHMsAERGRjWMZICIisnEsA0RERDaOZYCIiMjGsQwQERHZOJYBIiIiG8cyQEREZONYBoiIiGzc/wOx0dIj4IH8CQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the number of reviews per sentiment\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "\n",
    "# Print the counts for each category\n",
    "for sentiment_value, count in sentiment_counts.items():\n",
    "    sentiment_name = sentiment_labels[sentiment_value]\n",
    "    print(f\"{sentiment_value} ({sentiment_name}): {count} reviews\")\n",
    "\n",
    "# Define labels and colors for the pie chart\n",
    "labels = ['Positive', 'Neutral', 'Negative']\n",
    "colors = ['limegreen', 'dodgerblue', 'red']\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.pie(sentiment_counts, colors=colors, autopct='%1.1f%%',  pctdistance=0.8, textprops={'fontsize': 10, 'color': 'black'}, startangle=90)\n",
    "plt.axis('equal')  # pie as a circle\n",
    "plt.legend(labels=labels, loc='lower left')\n",
    "plt.title('Distribution of Reviews per Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc1bd547",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "675d3d0f",
   "metadata": {},
   "source": [
    "Next, we clean the data applying the following techniques (TODO: add info):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3edfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "#spell_checker = SpellChecker()\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "emojis = [\n",
    "        #HAPPY\n",
    "        \":-)\",\n",
    "        \":)\",\n",
    "        \";)\",\n",
    "        \":o)\",\n",
    "        \":]\",\n",
    "        \":3\",\n",
    "        \":c)\",\n",
    "        \":>\",\n",
    "        \"=]\",\n",
    "        \"8)\",\n",
    "        \"=)\",\n",
    "        \":}\",\n",
    "        \":^)\",\n",
    "        \":-D\",\n",
    "        \":D\",\n",
    "        \"8-D\",\n",
    "        \"8D\",\n",
    "        \"x-D\",\n",
    "        \"xD\",\n",
    "        \"X-D\",\n",
    "        \"XD\",\n",
    "        \"=-D\",\n",
    "        \"=D\",\n",
    "        \"=-3\",\n",
    "        \"=3\",\n",
    "        \":-))\",\n",
    "        \":'-)\",\n",
    "        \":')\",\n",
    "        \":*\",\n",
    "        \":^*\",\n",
    "        \">:P\",\n",
    "        \":-P\",\n",
    "        \":P\",\n",
    "        \"X-P\",\n",
    "        \"x-p\",\n",
    "        \"xp\",\n",
    "        \"XP\",\n",
    "        \":-p\",\n",
    "        \":p\",\n",
    "        \"=p\",\n",
    "        \":-b\",\n",
    "        \":b\",\n",
    "        \">:)\",\n",
    "        \">;)\",\n",
    "        \">:-)\",\n",
    "        \"<3\",\n",
    "        # SAD\n",
    "        \":L\",\n",
    "        \":-/\",\n",
    "        \">:/\",\n",
    "        \":S\",\n",
    "        \">:[\",\n",
    "        \":@\",\n",
    "        \":-(\",\n",
    "        \":[\",\n",
    "        \":-||\",\n",
    "        \"=L\",\n",
    "        \":<\",\n",
    "        \":-[\",\n",
    "        \":-<\",\n",
    "        \"=\\\\\",\n",
    "        \"=/\",\n",
    "        \">:(\",\n",
    "        \":(\",\n",
    "        \">.<\",\n",
    "        \":'-(\",\n",
    "        \":'(\",\n",
    "        \":\\\\\",\n",
    "        \":-c\",\n",
    "        \":c\",\n",
    "        \":{\",\n",
    "        \">:\\\\\",\n",
    "        \";(\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1f5731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Sentiment\n",
      "0                                 good and interesting          3\n",
      "1    this class is very helpful to me. currently, i...          3\n",
      "2    like!prof and tas are helpful and the discussi...          3\n",
      "3    easy to follow and includes a lot basic and im...          3\n",
      "4    really nice teacher!i could got the point eazl...          3\n",
      "..                                                 ...        ...\n",
      "995  very good course on algorithms, a lot of terms...          3\n",
      "996     well structured, well explained. great course!          3\n",
      "997                                very nice course !!          3\n",
      "998  this is a great course for who doesn't have a ...          3\n",
      "999  this course has some great quiz questions. i'd...          3\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1) Lowercase\n",
    "df['Review'] = df['Review'].str.lower()\n",
    "#pd.set_option('display.max_rows', df.shape[0]+1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4e83c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Sentiment\n",
      "0                                 good and interesting          3\n",
      "1    this class is very helpful to me. currently, i...          3\n",
      "2    like!prof and tas are helpful and the discussi...          3\n",
      "3    easy to follow and includes a lot basic and im...          3\n",
      "4    really nice teacher!i could got the point eazl...          3\n",
      "..                                                 ...        ...\n",
      "995  very good course on algorithms, a lot of terms...          3\n",
      "996     well structured, well explained. great course!          3\n",
      "997                                very nice course !!          3\n",
      "998  this is a great course for who does not have a...          3\n",
      "999  this course has some great quiz questions. i'd...          3\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2) Replace contractions with their standard full forms\n",
    "contraction_mapping = {\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        }\n",
    "\n",
    "for contraction, standard in contraction_mapping.items():\n",
    "    df['Review'] = df['Review'].str.replace(contraction, standard)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8492481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Sentiment\n",
      "0                                 good and interesting          3\n",
      "1    this class is very helpful to me. currently, i...          3\n",
      "2    like prof and tas are helpful and the discussi...          3\n",
      "3    easy to follow and includes a lot basic and im...          3\n",
      "4    really nice teacher i could got the point eazl...          3\n",
      "..                                                 ...        ...\n",
      "995  very good course on algorithms, a lot of terms...          3\n",
      "996     well structured, well explained. great course!          3\n",
      "997                                very nice course !!          3\n",
      "998  this is a great course for who does not have a...          3\n",
      "999  this course has some great quiz questions. i d...          3\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferga\\AppData\\Local\\Temp\\ipykernel_18512\\603766481.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Review'] = df['Review'].str.replace(pattern, ' ')\n"
     ]
    }
   ],
   "source": [
    "# 3) Remove punctuation in between words e.g. \"course.sometimes\" \n",
    "# and replace with space\n",
    "pattern = r'(?<=\\w)[^\\w\\s]+(?=\\w)'\n",
    "df['Review'] = df['Review'].str.replace(pattern, ' ')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdd5d78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[good, and, interesting]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[this, class, is, very, helpful, to, me, ., cu...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[like, prof, and, tas, are, helpful, and, the,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[easy, to, follow, and, includes, a, lot, basi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[really, nice, teacher, i, could, got, the, po...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           [good, and, interesting]          3\n",
       "1  [this, class, is, very, helpful, to, me, ., cu...          3\n",
       "2  [like, prof, and, tas, are, helpful, and, the,...          3\n",
       "3  [easy, to, follow, and, includes, a, lot, basi...          3\n",
       "4  [really, nice, teacher, i, could, got, the, po...          3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Tokenize text into individual words (removes all extra spaces \\s)\n",
    "tokenizer = TweetTokenizer()\n",
    "df['Review'] = df['Review'].apply(tokenizer.tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05647ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Sentiment\n",
      "0                             [good, and, interesting]          3\n",
      "1    [this, class, is, very, helpful, to, me, ., cu...          3\n",
      "2    [like, prof, and, tas, are, helpful, and, the,...          3\n",
      "3    [easy, to, follow, and, includes, a, lot, basi...          3\n",
      "4    [really, nice, teacher, i, could, got, the, po...          3\n",
      "..                                                 ...        ...\n",
      "995  [very, good, course, on, algorithms, a, lot, o...          3\n",
      "996  [well, structured, well, explained, ., great, ...          3\n",
      "997                               [very, nice, course]          3\n",
      "998  [this, is, a, great, course, for, who, does, n...          3\n",
      "999  [this, course, has, some, great, quiz, questio...          3\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: three dots i.e. ... not removed\n",
    "# 5) Remove punctuation first in between words (typo),\n",
    "# and then all punctuation and numerals except for tokenized emojis\n",
    "pattern = r\"[^\\w\\s\" + \"\".join(re.escape(e) for e in emojis + list(emoji.EMOJI_DATA.keys())) + \"]|[\\d]+\" # match non-emoji special characters\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [token for token in tokens if not re.match(pattern, token)])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ca98a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Sentiment\n",
      "0                             [good, and, interesting]          3\n",
      "1    [this, class, is, very, helpful, to, me, curre...          3\n",
      "2    [like, prof, and, tas, are, helpful, and, the,...          3\n",
      "3    [easy, to, follow, and, includes, lot, basic, ...          3\n",
      "4    [really, nice, teacher, could, got, the, point...          3\n",
      "..                                                 ...        ...\n",
      "995  [very, good, course, on, algorithms, lot, of, ...          3\n",
      "996  [well, structured, well, explained, great, cou...          3\n",
      "997                               [very, nice, course]          3\n",
      "998  [this, is, great, course, for, who, does, not,...          3\n",
      "999  [this, course, has, some, great, quiz, questio...          3\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 6) Remove single characters\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [word for word in tokens if len(word) > 1])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e249e600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 7) Correct Spelling\\ncorrected_tokens = []\\nfor token in filtered_tokens:\\n    if token in emojis or token in emoji.EMOJI_DATA.keys():\\n        corrected_tokens.append(token)  # If token is an emoji, add it to the corrected tokens\\n    else:\\n        corrected_token = spell_checker.correction(token)\\n        if corrected_token is not None:\\n            corrected_tokens.append(corrected_token)\\n#print('spell-check: '+str(corrected_tokens))\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: package not loading\n",
    "'''\n",
    "# 7) Correct Spelling\n",
    "corrected_tokens = []\n",
    "for token in filtered_tokens:\n",
    "    if token in emojis or token in emoji.EMOJI_DATA.keys():\n",
    "        corrected_tokens.append(token)  # If token is an emoji, add it to the corrected tokens\n",
    "    else:\n",
    "        corrected_token = spell_checker.correction(token)\n",
    "        if corrected_token is not None:\n",
    "            corrected_tokens.append(corrected_token)\n",
    "#print('spell-check: '+str(corrected_tokens))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db373fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: note to self (to be added to word-doc): If you check token by token, it also removes english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab8a4a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Sentiment\n",
      "0                             [good, and, interesting]          3\n",
      "1    [this, class, is, very, helpful, to, me, curre...          3\n",
      "2    [like, prof, and, tas, are, helpful, and, the,...          3\n",
      "3    [easy, to, follow, and, includes, lot, basic, ...          3\n",
      "4    [really, nice, teacher, could, got, the, point...          3\n",
      "..                                                 ...        ...\n",
      "995  [very, good, course, on, algorithms, lot, of, ...          3\n",
      "996  [well, structured, well, explained, great, cou...          3\n",
      "997                               [very, nice, course]          3\n",
      "998  [this, is, great, course, for, who, does, not,...          3\n",
      "999  [this, course, has, some, great, quiz, questio...          3\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 8) Perform negation tagging\n",
    "df['Review'] = df['Review'].apply(mark_negation)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02fce956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Sentiment\n",
      "0                                  [good, interesting]          3\n",
      "1    [class, helpful, currently, still, learning, c...          3\n",
      "2    [like, prof, tas, helpful, discussion, among, ...          3\n",
      "3    [easy, follow, includes, lot, basic, important...          3\n",
      "4    [really, nice, teacher, could, got, point, eaz...          3\n",
      "..                                                 ...        ...\n",
      "995  [good, course, algorithms, lot, terms, explain...          3\n",
      "996  [well, structured, well, explained, great, cou...          3\n",
      "997                                     [nice, course]          3\n",
      "998  [great, course, computer_NEG, science_NEG, bac...          3\n",
      "999  [course, great, quiz, questions, recommend, pe...          3\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 9) Remove stopwords --> also removes words like 'not'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [token for token in tokens if token.split('_')[0] not in stop_words])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71dbcde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Sentiment\n",
      "0                                  [good, interesting]          3\n",
      "1    [class, helpful, currently, still, learning, c...          3\n",
      "2    [like, prof, ta, helpful, discussion, among, s...          3\n",
      "3    [easy, follow, includes, lot, basic, important...          3\n",
      "4    [really, nice, teacher, could, got, point, eaz...          3\n",
      "..                                                 ...        ...\n",
      "995  [good, course, algorithm, lot, term, explained...          3\n",
      "996  [well, structured, well, explained, great, cou...          3\n",
      "997                                     [nice, course]          3\n",
      "998  [great, course, computer_NEG, science_NEG, bac...          3\n",
      "999  [course, great, quiz, question, recommend, peo...          3\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: also lemmatize word removing _NEG\n",
    "# 10) Lemmatize words using WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Review'] = df['Review'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e25a06a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good interesting</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class helpful currently still learning class m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like prof ta helpful discussion among student ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>easy follow includes lot basic important techn...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>really nice teacher could got point eazliy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                                   good interesting          3\n",
       "1  class helpful currently still learning class m...          3\n",
       "2  like prof ta helpful discussion among student ...          3\n",
       "3  easy follow includes lot basic important techn...          3\n",
       "4         really nice teacher could got point eazliy          3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert preprocessed tokens back to string\n",
    "df['Review'] = df['Review'].apply(' '.join)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25434a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (1000, 2)\n",
      "Shape after preprocessing, before removing empty rows: (1000, 2)\n",
      "Shape after preprocessing, after removing empty rows: (1000, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape before: {df_raw.shape}')\n",
    "print(f'Shape after preprocessing, before removing empty rows: {df.shape}')\n",
    "\n",
    "# Remove NaN rows, after cleaning text\n",
    "df = drop_missing(df) \n",
    "print(f'Shape after preprocessing, after removing empty rows: {df.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b120f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset_path = \"cleaned_input/cleaned_data.csv\"\n",
    "df.to_csv(cleaned_dataset_path, sep=',', index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eecf627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create word clouds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f18328f",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5de5ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import gensim.downloader as api\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f65f114",
   "metadata": {},
   "source": [
    "### Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73f09fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Distribution:\n",
      "* train: 640\n",
      "* validation: 160\n",
      "* test: 200\n",
      "\n",
      "x_train: 390    simply introduction definitely help learn prog...\n",
      "847    detailed rigorous exciting teacher presentatio...\n",
      "93                                     good introduction\n",
      "236         excellent material instruction hard valuable\n",
      "858                                         advanced_NEG\n",
      "Name: Review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Review'], df['Sentiment'],\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "# Split the training dataset further into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "print(\"Data Distribution:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(len(x_train), len(x_val), len(x_test)))\n",
    "\n",
    "print(\"x_train: {}\".format(x_train.head()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceb8e8d3",
   "metadata": {},
   "source": [
    "### Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b30607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('course', 433), ('great', 144), ('good', 91), ('really', 73), ('excellent', 69), ('lot', 59), ('course_NEG', 55), ('much', 50), ('thank', 49), ('interesting', 46), ('material', 43), ('well', 41), ('printing', 39), ('class', 39), ('professor', 38), ('ableton', 38), ('useful', 38), ('introduction', 37), ('video', 37), ('easy', 37), ('like', 37), ('would', 35), ('one', 35), ('learned', 33), ('thanks', 32), ('learn', 31), ('way', 31), ('understand', 31), ('accounting', 31), ('content', 30)]\n"
     ]
    }
   ],
   "source": [
    "# Count words to create vocabulary\n",
    "word_counter = Counter()\n",
    "for review in x_train:\n",
    "    word_counter.update(review.split())\n",
    "\n",
    "print(word_counter.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0aa7a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of 2765 reduced to 1106.\n",
      "\n",
      "Vocabulary (first 50 tokens):\n",
      "['simply', 'introduction', 'definitely', 'help', 'learn', 'program', 'least', 'level', 'basic', 'well', 'way', 'complex', 'arrangement', 'detailed', 'exciting', 'teacher', 'presentation', 'video', 'material', 'interesting', 'good', 'excellent', 'instruction', 'hard', 'valuable', 'advanced_NEG', 'easy', 'understand', 'course', 'example', 'would', 'love', 'similar', 'great', 'provided', 'increased', 'really', 'looking', 'forward', 'increase', 'skill', 'complete', 'extensive', 'week', 'actually', 'take', 'okay_NEG', 'comment', 'fast', 'informative']\n"
     ]
    }
   ],
   "source": [
    "# Filter vocabulary by removing words with frequency less than a set minimum frequency\n",
    "vocab = [word for word, count in word_counter.items() if count >= MIN_FREQ]\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size of {} reduced to {}.\\n\".format(len(word_counter), vocab_size))\n",
    "print(\"Vocabulary (first 50 tokens):\\n{}\".format(vocab[:50]))\n",
    "\n",
    "# Save filtered vocabulary to a txt file TODO: pickle?\n",
    "vocab_file = 'processed/vocab.txt'\n",
    "with open(vocab_file, 'w') as f:\n",
    "    f.write('\\n'.join(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83fc4550",
   "metadata": {},
   "source": [
    "### Filter data with vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31b1e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_filter_dataset(docs, filename, vocab):\n",
    "    filtered_dataset = []\n",
    "    for doc in docs:\n",
    "        filtered_text = ' '.join([word for word in doc.split() if word in vocab])\n",
    "        filtered_dataset.append(filtered_text)\n",
    "\n",
    "    # Save filtered dataset to a txt file\n",
    "    filtered_filename = f'processed/filtered_{str(filename)}.txt'\n",
    "    with open(filtered_filename, 'w') as f:\n",
    "        f.write('\\n'.join(filtered_dataset))\n",
    "\n",
    "    # Convert the processed documents back to pandas.Series\n",
    "    filtered_dataset = pd.Series(filtered_dataset, index=docs.index)\n",
    "\n",
    "    # Convert empty rows to '<empty>'\n",
    "    placeholder = \"<empty>\"\n",
    "    filtered_dataset.replace('', placeholder, inplace=True)\n",
    "    \n",
    "    # Count the number of rows with '<empty>'\n",
    "    num_empty_rows = filtered_dataset.str.count('<empty>').sum()\n",
    "    print(f'Number of rows with <empty> for {filename}: {num_empty_rows}')\n",
    "\n",
    "    # Save filled dataset to a txt file\n",
    "    filled_filename = f'processed/filled_{str(filename)}.txt'\n",
    "    with open(filled_filename, 'w') as f:\n",
    "        f.write('\\n'.join(filtered_dataset))\n",
    "    \n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e136714f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with <empty> for x_train: 1\n",
      "Number of rows with <empty> for x_val: 3\n",
      "Number of rows with <empty> for x_test: 4\n",
      "\n",
      "Data Distribution:\n",
      "* train: 640\n",
      "* validation: 160\n",
      "* test: 200\n",
      "\n",
      "x_train - updated:\n",
      "390    simply introduction definitely help learn prog...\n",
      "847    detailed exciting teacher presentation video m...\n",
      "93                                     good introduction\n",
      "236         excellent material instruction hard valuable\n",
      "858                                         advanced_NEG\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset based on vocabulary\n",
    "x_train = freq_filter_dataset(x_train, \"x_train\", vocab)\n",
    "x_val = freq_filter_dataset(x_val, \"x_val\", vocab)\n",
    "x_test = freq_filter_dataset(x_test, \"x_test\", vocab)\n",
    "\n",
    "print(\"\\nData Distribution:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(len(x_train), len(x_val), len(x_test)))\n",
    "print(\"x_train - updated:\")\n",
    "print(x_train.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8c6d509",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b739f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS \n",
    "\n",
    "# TF-IDF\n",
    "MAX_FEATURES = 10000\n",
    "MAX_DF = 0.95\n",
    "MIN_DF = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ccee997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer with the filtered vocabulary\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES, # maximum number of features to keep, check unique vocabs and determine based on that, high causes saprse metrics and low value causes loss in important words/vocab\n",
    "    vocabulary=vocab,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1, 1),  # range of n-grams, only unigrams now\n",
    "    max_df=MAX_DF,  # ignore terms that have a document frequency strictly higher than the threshold\n",
    "    min_df=MIN_DF,  # ignore terms that have a document frequency strictly lower than the threshold.\n",
    "    use_idf=True,  # enable IDF weighting\n",
    "    smooth_idf=True,  # smooth IDF weights --> provides stability, reduces run time errors\n",
    "    sublinear_tf=True  # apply sublinear scaling to term frequencies\n",
    ")\n",
    "\n",
    "# Fit and transform the training set\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Transform the validation and testing set\n",
    "x_val_tfidf = tfidf_vectorizer.transform(x_val)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3afdbb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tfidf_data(data, data_name, feature_names):\n",
    "    # Save the matrix with feature names as a DataFrame\n",
    "    data = pd.DataFrame(data.toarray(), columns=feature_names)\n",
    "    data.to_csv(f'processed/{data_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40d62af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Save vectorized data\n",
    "save_tfidf_data(x_train_tfidf, \"train_tfidf\", feature_names)\n",
    "save_tfidf_data(x_train_tfidf, \"val_tfidf\", feature_names)\n",
    "save_tfidf_data(x_test_tfidf, \"test_tfidf\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daad3bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given vocabulary-size : 1106,\n",
      "\n",
      "Data Shape:\n",
      "* train: (640, 1106)\n",
      "* validation: (160, 1106)\n",
      "* test: (200, 1106)\n",
      "\n",
      "x_train_tfidf:\n",
      "  (0, 12)\t0.33919258043595374\n",
      "  (0, 11)\t0.33919258043595374\n",
      "  (0, 10)\t0.22489399599358414\n",
      "  (0, 9)\t0.21352946324472982\n",
      "  (0, 8)\t0.24663322884546274\n",
      "  (0, 7)\t0.25842812785300573\n",
      "  (0, 6)\t0.33919258043595374\n",
      "  (0, 5)\t0.3005023336028307\n",
      "  (0, 4)\t0.2267242657749761\n",
      "  (0, 3)\t0.2522197100778529\n",
      "  (0, 2)\t0.2827268108748124\n",
      "  (0, 1)\t0.21352946324472982\n",
      "  (0, 0)\t0.3165602368949313\n",
      "  (1, 19)\t0.27802113682873\n",
      "  (1, 18)\t0.28335792159281337\n",
      "  (1, 17)\t0.2911158088575771\n",
      "  (1, 16)\t0.3899905072326257\n",
      "  (1, 15)\t0.3270450264901535\n",
      "  (1, 14)\t0.4285951563811605\n",
      "  (1, 13)\t0.4809784012085009\n",
      "  (1, 9)\t0.28910040815307975\n",
      "  (2, 20)\t0.6138182338892179\n",
      "  (2, 1)\t0.7894473863058394\n",
      "  (3, 24)\t0.5270656955989282\n",
      "  (3, 23)\t0.4929915498464934\n",
      "  :\t:\n",
      "  (636, 462)\t0.3152694014494952\n",
      "  (636, 386)\t0.20848579897303376\n",
      "  (636, 322)\t0.19791010307855386\n",
      "  (636, 312)\t0.1862031872180371\n",
      "  (636, 311)\t0.21518825812604261\n",
      "  (636, 75)\t0.1426175886215295\n",
      "  (636, 74)\t0.22339140516054856\n",
      "  (636, 28)\t0.05879667680806362\n",
      "  (637, 964)\t0.42670136584751844\n",
      "  (637, 331)\t0.34661730795777923\n",
      "  (637, 250)\t0.37617395093814293\n",
      "  (637, 149)\t0.34661730795777923\n",
      "  (637, 103)\t0.2559939072832295\n",
      "  (637, 102)\t0.2913853155407505\n",
      "  (637, 81)\t0.36288350237913775\n",
      "  (637, 5)\t0.3924401453595015\n",
      "  (638, 698)\t0.5089439509829856\n",
      "  (638, 291)\t0.4857606643924692\n",
      "  (638, 225)\t0.476041353620985\n",
      "  (638, 154)\t0.432684396720733\n",
      "  (638, 21)\t0.30196270326636526\n",
      "  (639, 373)\t0.5750519554918363\n",
      "  (639, 190)\t0.458196043137778\n",
      "  (639, 33)\t0.2989355599761042\n",
      "  (639, 22)\t0.6082837869939257\n"
     ]
    }
   ],
   "source": [
    "print(\"Given vocabulary-size : {},\".format(vocab_size))\n",
    "print(\"\\nData Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(x_train_tfidf.shape, x_val_tfidf.shape, x_test_tfidf.shape))\n",
    "print(\"x_train_tfidf:\\n{}\".format(x_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "199b0d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Types:\n",
      "x_train_tfidf - type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "x_val_tfidf - type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "y-train - type: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nData Types:\\nx_train_tfidf - type: {type(x_train_tfidf)}\\nx_val_tfidf - type: {type(x_val_tfidf)}\\ny-train - type: {type(y_train)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ca1595c",
   "metadata": {},
   "source": [
    "# Classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9cc3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08c72e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # TODO: not anymore?? Handle the zero-division error when there are no predicted samples for a label\n",
    "    # only interested in labels that were predicted at least once\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Print results\n",
    "    print(f'{model_name} Accuracy: {accuracy}')\n",
    "    print(f'{model_name} Precision: {precision}')\n",
    "    print(f'{model_name} Recall: {recall}')\n",
    "    \n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efa6a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_report(y_true, y_pred):\n",
    "    # Calculate classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30335352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, model_name):\n",
    "    save_dir = f'results/{model_name}_confusion_matrix.png'\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cnf_matrix, display_labels=labels)\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    plt.savefig(save)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3811bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, save_dir, model_name):\n",
    "    accuracy_plot = f'{save_dir}/{model_name}_plot.png'\n",
    "    loss_plot = f'{save_dir}/{model_name}_loss_plot.png'\n",
    "    \n",
    "    accuracy = history.history['accuracy']\n",
    "    #val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(accuracy))\n",
    "    plt.plot(epochs, accuracy, 'r', label='Training acc')\n",
    "    #plt.plot(epochs, val_accuracy, 'b', label='Validation acc')\n",
    "\n",
    "    plt.title(f'{model_name} Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(save_dir)\n",
    "    plt.close()\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    #val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "    plt.plot(epochs, loss, 'r', label='Training acc')\n",
    "    #plt.plot(epochs, val_loss, 'b', label='Validation acc')\n",
    "\n",
    "    plt.title(f'{model_name} Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(save_dir)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5391bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(accuracy, precision, recall, f1_score, report, model_name):\n",
    "        # Save results\n",
    "    save_dir = f'results/{model_name}_results.txt'\n",
    "    with open(save_dir, 'w') as file:\n",
    "        file.write(f'{model_name} Accuracy: {accuracy}\\n')\n",
    "        file.write(f'{model_name} Precision: {precision}\\n')\n",
    "        file.write(f'{model_name} Recall: {recall}\\n')\n",
    "        file.write(f'{model_name} F1 Score: {f1_score}\\n')\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7aefe79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred, model_name):\n",
    "    print(f'{model_name} Testing complete!\\n')\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    accuracy, precision, recall = calculate_metrics(y_test, y_pred, model_name)\n",
    "    \n",
    "    # Calculate f1_score\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    print(f'{model_name} F1 Score: {f1}')\n",
    "    \n",
    "    # Calculate classification report\n",
    "    report = calculate_classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Save results\n",
    "    save_results(accuracy, precision, recall, f1, report, model_name)\n",
    "\n",
    "    # Plot accuracy # TODO\n",
    "    #plot(history, model_name)\n",
    "\n",
    "    # Plot confusion matrix # TODO\n",
    "    senti_labels = ['negative', 'neutral', 'positive']\n",
    "    #plot_confusion_matrix(y_test, y_pred, senti_labels, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10848f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top3_models(top3_models):    \n",
    "    # Print the sorted list of mean test scores and standard deviation of test scores\n",
    "    print(\"\\nTop 3 parameter combinations ranked by performance (from best to worst):\")\n",
    "    for index, row in top3_models.iterrows():\n",
    "        mean_score = row['mean_test_score']\n",
    "        std_score = row['std_test_score']\n",
    "        params = row['params']\n",
    "        print(f\"Mean Test Score: {mean_score:.4f} (±{std_score:.4f}) for {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf3dba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(x_val, model, model_name, params):\n",
    "    y_pred = model.predict(x_val)\n",
    "    \n",
    "    calculate_metrics(y_val, y_pred, model_name)\n",
    "    print(\"Params: {}\\n\".format(params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b616c18",
   "metadata": {},
   "source": [
    "## 1. Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6e69ba1",
   "metadata": {},
   "source": [
    "### Training & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "681b15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Random Forest model\n",
    "rf_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be257e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    #'min_samples_split': [2, 5, 10],\n",
    "    #'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    #'max_features': ['auto', 'sqrt'],  # Number of features to consider when looking for the best split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "740e75e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'max_depth': None, 'n_estimators': 50}\n",
      "Best Score:  0.89375\n",
      "\n",
      "Top 3 parameter combinations ranked by performance (from best to worst):\n",
      "Mean Test Score: 0.8938 (±0.0038) for {'max_depth': None, 'n_estimators': 50}\n",
      "Mean Test Score: 0.8922 (±0.0031) for {'max_depth': None, 'n_estimators': 100}\n",
      "Mean Test Score: 0.8922 (±0.0031) for {'max_depth': None, 'n_estimators': 200}\n",
      "Mean Test Score: 0.8922 (±0.0031) for {'max_depth': None, 'n_estimators': 300}\n",
      "Mean Test Score: 0.8922 (±0.0031) for {'max_depth': 5, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=rf_param_grid, cv=5)\n",
    "grid_search.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Get the best parameters and best score from grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "top3_models = sorted_results[:5] # TODO: update 10 to 3\n",
    "print_top3_models(top3_models)\n",
    "top3_models = sorted_results[:3] # TODO: and delete this\n",
    "top3_params = top3_models['params'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "159c13c0",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26126294",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17d1f15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF-1 Accuracy: 0.89375\n",
      "RF-1 Precision: 0.9014130523564486\n",
      "RF-1 Recall: 0.89375\n",
      "Params: {'max_depth': None, 'n_estimators': 50}\n",
      "\n",
      "RF-2 Accuracy: 0.89375\n",
      "RF-2 Precision: 0.8800562494830011\n",
      "RF-2 Recall: 0.89375\n",
      "Params: {'max_depth': None, 'n_estimators': 100}\n",
      "\n",
      "RF-3 Accuracy: 0.89375\n",
      "RF-3 Precision: 0.9014130523564486\n",
      "RF-3 Recall: 0.89375\n",
      "Params: {'max_depth': None, 'n_estimators': 200}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the top 3 models on the validation set\n",
    "# TODO: remove f1_score\n",
    "rf_cand_1 = RandomForestClassifier(**top3_params[0])\n",
    "rf_cand_1.fit(x_train_tfidf, y_train)\n",
    "\n",
    "rf_cand_2 = RandomForestClassifier(**top3_params[1])\n",
    "rf_cand_2.fit(x_train_tfidf, y_train)\n",
    "\n",
    "rf_cand_3 = RandomForestClassifier(**top3_params[2])\n",
    "rf_cand_3.fit(x_train_tfidf, y_train)\n",
    "\n",
    "predict_data(x_val_tfidf, rf_cand_1, \"RF-1\", top3_params[0])\n",
    "predict_data(x_val_tfidf, rf_cand_2, \"RF-2\", top3_params[1])\n",
    "predict_data(x_val_tfidf, rf_cand_3, \"RF-3\", top3_params[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdaaf668",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f7d1211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Testing complete!\n",
      "\n",
      "RF Accuracy: 0.895\n",
      "RF Precision: 0.895\n",
      "RF Recall: 0.895\n",
      "RF F1 Score: 0.9445910290237467\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.90      1.00      0.94       179\n",
      "\n",
      "    accuracy                           0.90       200\n",
      "   macro avg       0.30      0.33      0.31       200\n",
      "weighted avg       0.80      0.90      0.85       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferga\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ferga\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ferga\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "rf_best = rf_cand_1\n",
    "y_pred = rf_best.predict(x_test_tfidf)\n",
    "evaluate_model(y_test, y_pred, \"RF\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1790120",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "214966ae",
   "metadata": {},
   "source": [
    "### Training & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f6e4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Naive Bayes model & fit on training data\n",
    "nb_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a7cbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with TF-IDF vectorizer and multinomial Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vectorizer),  # Replace tfidf_vectorizer with your existing TF-IDF vectorizer\n",
    "    ('nb_clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "nb_param_grid = {\n",
    "    'tfidf__max_features': [1000, 5000, 10000],  # Maximum number of features\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Range of n-grams\n",
    "    'nb_clf__alpha': [0.1, 0.5, 1.0],  # Smoothing parameter for MultinomialNB\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfb8d9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"type of x_train_tfidf: \", type(x_train_tfidf))\\nprint(\"type of x_train_tfidf: \", type(y_train))\\nprint(\"shape of x_train_tfidf: \", x_train_tfidf.shape)\\nprint(\"shape of x_train_tfidf: \", y_train.shape)\\n\\n# Perform grid search\\ngrid_search = GridSearchCV(pipeline, param_grid=nb_param_grid, cv=5, error_score=\\'raise\\')\\ngrid_search.fit(x_train_tfidf, y_train)\\n\\n# Get the best parameters and best score from grid search\\nbest_params = grid_search.best_params_\\nbest_score = grid_search.best_score_\\nprint(\"Best Parameters: \", best_params)\\nprint(\"Best Score: \", best_score)\\n\\n# Get the mean test scores and standard deviations of test scores for all parameter combinations\\nresults_df = pd.DataFrame(grid_search.cv_results_)\\nsorted_results = results_df.sort_values(by=\\'mean_test_score\\', ascending=False)\\ntop3_models = sorted_results[:5] # TODO: update 10 to 3\\nprint_top3_models(top3_models)\\ntop3_models = sorted_results[:3] # TODO: and delete this\\ntop3_params = top3_models[\\'params\\'].values\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"type of x_train_tfidf: \", type(x_train_tfidf))\n",
    "print(\"type of x_train_tfidf: \", type(y_train))\n",
    "print(\"shape of x_train_tfidf: \", x_train_tfidf.shape)\n",
    "print(\"shape of x_train_tfidf: \", y_train.shape)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid=nb_param_grid, cv=5, error_score='raise')\n",
    "grid_search.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Get the best parameters and best score from grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "top3_models = sorted_results[:5] # TODO: update 10 to 3\n",
    "print_top3_models(top3_models)\n",
    "top3_models = sorted_results[:3] # TODO: and delete this\n",
    "top3_params = top3_models['params'].values\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fae5d8b7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dd270a0",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d223208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d7f0cd9",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bad5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3287aee",
   "metadata": {},
   "source": [
    "## 3. SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f34d76f",
   "metadata": {},
   "source": [
    "### Training &  Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ccbbc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the SVM model\n",
    "svm_model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ade86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': [0.1, 1, 'scale']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecf8968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Best Score:  0.8984375\n",
      "\n",
      "Top 3 parameter combinations ranked by performance (from best to worst):\n",
      "Mean Test Score: 0.8984 (±0.0131) for {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Mean Test Score: 0.8984 (±0.0131) for {'C': 10, 'gamma': 1, 'kernel': 'linear'}\n",
      "Mean Test Score: 0.8984 (±0.0131) for {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Mean Test Score: 0.8938 (±0.0038) for {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Mean Test Score: 0.8938 (±0.0038) for {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(svm_model, param_grid=svm_param_grid, cv=5)\n",
    "grid_search.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Get the best parameters and best score from grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "top3_models = sorted_results[:5] # TODO: update 10 to 3\n",
    "print_top3_models(top3_models)\n",
    "top3_models = sorted_results[:3] # TODO: and delete this\n",
    "top3_params = top3_models['params'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceb654f8",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "525c68dd",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94f4da28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM-1 Accuracy: 0.88125\n",
      "SVM-1 Precision: 0.8574583333333333\n",
      "SVM-1 Recall: 0.88125\n",
      "Params: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\n",
      "SVM-2 Accuracy: 0.88125\n",
      "SVM-2 Precision: 0.8574583333333333\n",
      "SVM-2 Recall: 0.88125\n",
      "Params: {'C': 10, 'gamma': 1, 'kernel': 'linear'}\n",
      "\n",
      "SVM-3 Accuracy: 0.88125\n",
      "SVM-3 Precision: 0.8574583333333333\n",
      "SVM-3 Recall: 0.88125\n",
      "Params: {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the top 3 models on the validation set\n",
    "# TODO: remove f1_score\n",
    "svm_cand_1 = SVC(**top3_params[0])\n",
    "svm_cand_1.fit(x_train_tfidf, y_train)\n",
    "\n",
    "svm_cand_2 = SVC(**top3_params[1])\n",
    "svm_cand_2.fit(x_train_tfidf, y_train)\n",
    "\n",
    "svm_cand_3 = SVC(**top3_params[2])\n",
    "svm_cand_3.fit(x_train_tfidf, y_train)\n",
    "\n",
    "predict_data(x_val_tfidf, svm_cand_1, \"SVM-1\", top3_params[0])\n",
    "predict_data(x_val_tfidf, svm_cand_2, \"SVM-2\", top3_params[1])\n",
    "predict_data(x_val_tfidf, svm_cand_3, \"SVM-3\", top3_params[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0071c488",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2f8eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Testing complete!\n",
      "\n",
      "SVM Accuracy: 0.9\n",
      "SVM Precision: 0.8744166666666666\n",
      "SVM Recall: 0.9\n",
      "SVM F1 Score: 0.8782820675439987\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.22      0.33         9\n",
      "           2       0.40      0.17      0.24        12\n",
      "           3       0.92      0.98      0.95       179\n",
      "\n",
      "    accuracy                           0.90       200\n",
      "   macro avg       0.66      0.46      0.51       200\n",
      "weighted avg       0.87      0.90      0.88       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "svm_best = svm_cand_1\n",
    "y_pred = svm_best.predict(x_test_tfidf)\n",
    "evaluate_model(y_test, y_pred, \"SVM\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9982ae27",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5d99acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum review length: 223\n"
     ]
    }
   ],
   "source": [
    "# Find maximum sequence length\n",
    "max_seq_length = max([len(doc.split()) for doc in x_train])\n",
    "print(f'\\nMaximum review length: {max_seq_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b12065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tokenizer (on training data)\n",
    "tokenizer = Tokenizer()\n",
    "# Remove default filters, including punctuation\n",
    "tokenizer.filters = \"\"  \n",
    "# Disable lowercase conversion\n",
    "tokenizer.lower = False  \n",
    "tokenizer.fit_on_texts(x_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbb76b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(lines, tokenizer, max_length):\n",
    "    # Integer encode\n",
    "    encoded_seq = tokenizer.texts_to_sequences(lines)\n",
    "    # Pad the encoded sequences\n",
    "    padded = pad_sequences(encoded_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Save padded sequences to a text file\n",
    "    # TODO: note, only encodes final input sent\n",
    "    encoded_file = \"processed/encoded.txt\"\n",
    "    with open(encoded_file, 'w') as file:\n",
    "        for sequence in padded:\n",
    "            line = \" \".join(str(num) for num in sequence)\n",
    "            file.write(line + \"\\n\")\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7655e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_encoded - shape: (640, 223)\n",
      "x_val_encoded - shape: (160, 223)\n",
      "x_test_encoded - shape: (200, 223)\n",
      "[[  4   3   1 215  17  45   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [526   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 42   1 325 242   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Encode Data\n",
    "x_train_encoded = encode_text(x_train, tokenizer, max_seq_length)\n",
    "x_val_encoded = encode_text(x_val, tokenizer, max_seq_length)\n",
    "x_test_encoded = encode_text(x_test, tokenizer, max_seq_length)\n",
    "\n",
    "print(\"x_train_encoded - shape:\", x_train_encoded.shape)\n",
    "print(\"x_val_encoded - shape:\", x_val_encoded.shape)\n",
    "print(\"x_test_encoded - shape:\", x_test_encoded.shape)\n",
    "print(x_val_encoded[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e193568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train - shape: (640,)\n",
      "y_val - shape: (160,)\n",
      "y_test - shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Restructure labels\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "y_test = y_test.values\n",
    "print(\"y_train - shape:\", y_train.shape)\n",
    "print(\"y_val - shape:\", y_val.shape)\n",
    "print(\"y_test - shape:\", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "527d5290",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80ad4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_vocab_size:  1108\n"
     ]
    }
   ],
   "source": [
    "# Total vocabulary size plus 0 for unknown words\n",
    "embedding_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"embedding_vocab_size: \", embedding_vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a3248ef",
   "metadata": {},
   "source": [
    "Checking why embedding vocab_size is 2 greater than original vocab size due to <empty> \n",
    "    #TODO: remove rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16e71425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in tokenizer but not in vocab:\n",
      "{'<empty>'}\n"
     ]
    }
   ],
   "source": [
    "# Convert the tokenizer word index into a set\n",
    "tokenizer_words = set(tokenizer.word_index.keys())\n",
    "\n",
    "# Convert the manual vocabulary into a set\n",
    "vocab_set = set(vocab)\n",
    "\n",
    "# Find the words in tokenizer but not in vocab\n",
    "tokenizer_only_words = tokenizer_words.difference(vocab_set)\n",
    "\n",
    "print(\"Words in tokenizer but not in vocab:\")\n",
    "print(tokenizer_only_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b891123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: gigaword or twitter?\n",
    "def load_embedding():\n",
    "    # Check if the pre-trained Word2Vec model is already downloaded\n",
    "    #w2v_pretrained_model = \"glove-twitter-100\"\n",
    "    w2v_pretrained_model = \"glove-wiki-gigaword-100\"\n",
    "    w2v_pretrained_model_filename = str(w2v_pretrained_model) + \"-word2vec.txt\"\n",
    "    if not os.path.exists(w2v_pretrained_model_filename):\n",
    "        print(\"\\nw2v model doesn't exist\")\n",
    "        # If the model does not exist, download it\n",
    "        model = api.load(\"glove-twitter-100\")\n",
    "        # Save the word2vec embeddings in the appropriate format\n",
    "        model.save_word2vec_format(w2v_pretrained_model_filename, binary=False)\n",
    "\n",
    "    # load embedding into memory, skip first line\n",
    "    print(\"Loading w2v model...\")\n",
    "    file = open(w2v_pretrained_model_filename, 'r', encoding='utf8')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2010e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading w2v model...\n"
     ]
    }
   ],
   "source": [
    "raw_embedding = load_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f8aec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matrix(embedding, tokenizer):\n",
    "    # create a weight matrix for the Embedding layer from a loaded embedding\n",
    "\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((embedding_vocab_size, EMBEDDING_DIM))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    count_all = 0\n",
    "    count_na = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # TODO: important note, pretrained word2vec model removes all neg_ and emojis (also other words) that are\n",
    "        #  not defined in the model it These values should prob? also be removed from the vocab (and update vocab size) to avoid mismatch in the embedding layer\n",
    "        if word in embedding.keys():\n",
    "            # print(embedding.get(word)[:3])\n",
    "            weight_matrix[i] = embedding.get(word)\n",
    "        else:\n",
    "            #print(word)\n",
    "            count_na += 1\n",
    "        count_all += 1\n",
    "    print(f'count_na/count_all: {str(count_na)}/{count_all}')\n",
    "    print(f\"embedding matrix shape: {weight_matrix.shape}\")\n",
    "\n",
    "    # save model in ASCII (word2vec) format\n",
    "    weight_matrix_filename =  'processed/weight_matrix_word2vec.txt'\n",
    "    with open(weight_matrix_filename, 'w') as f:\n",
    "        f.write('\\n'.join(' '.join(str(x) for x in row) for row in weight_matrix))\n",
    "    print(\"Saving weight embedding matrix to a txt file...\")\n",
    "\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3a6844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_na/count_all: 371/1107\n",
      "embedding matrix shape: (1108, 100)\n",
      "Saving weight embedding matrix to a txt file...\n"
     ]
    }
   ],
   "source": [
    "w2v_embedding_vectors = get_weight_matrix(raw_embedding, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fa517b2",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15dc440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate, LSTM, Dense, Conv1D, Dropout, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62ebef08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check one-hot encoding:\n",
      " [[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "y-encoded Data Shape:\n",
      "* train: (640, 3)\n",
      "* validation: (160, 3)\n",
      "* test: (200, 3)\n",
      "\n",
      "\n",
      "x_train_encoded - type: <class 'numpy.ndarray'>\n",
      "y_train_encoded - type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Convert sentiment labels to one-hot encoding\n",
    "num_classes = 3  # Number of sentiment classes [pos, neg, neut]\n",
    "y_train_encoded = np.zeros((len(y_train), num_classes))\n",
    "for i, label in enumerate(y_train):\n",
    "    y_train_encoded[i, label - 1] = 1\n",
    "\n",
    "y_val_encoded = np.zeros((len(y_val), num_classes))\n",
    "for i, label in enumerate(y_val):\n",
    "    y_val_encoded[i, label - 1] = 1\n",
    "\n",
    "y_test_encoded = np.zeros((len(y_test), num_classes))\n",
    "for i, label in enumerate(y_test):\n",
    "    y_test_encoded[i, label - 1] = 1\n",
    "\n",
    "    \n",
    "print(\"Check one-hot encoding:\\n\", y_train_encoded[:3])    \n",
    "print(\"\\ny-encoded Data Shape:\\n* train: {}\\n* validation: {}\\n* test: {}\\n\".format(y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape))\n",
    "print(\"\\nx_train_encoded - type:\", type(x_train_encoded))\n",
    "print(\"y_train_encoded - type:\", type(y_train_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0b1edcd",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4958c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Grid search params\n",
    "# TODO: evaluation from the general functions\n",
    "# TODO: model architecture aspects (dropout) etc.\n",
    "# TODO: add train accuracy where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "69975c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ebeefe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(score, model_name):\n",
    "    acc =  (score[1] * 100)\n",
    "    loss = score[0]\n",
    "    print(\"{} Accuracy: {}\".format(model_name, acc))\n",
    "    print(\"{} Loss: {}\".format(model_name, loss))\n",
    "    \n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7df1bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(accuracy, loss, report, model_name):\n",
    "        # Save results\n",
    "    save_dir = f'results/{model_name}_results.txt'\n",
    "    with open(save_dir, 'w') as file:\n",
    "        file.write(f'{model_name} Accuracy: {accuracy}\\n')\n",
    "        file.write(f'{model_name} Loss: {loss}\\n')\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f0f987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, x_test_encoded, y_test_encoded, y_test_true, params):\n",
    "    print(f'{model_name} Testing complete!\\n')\n",
    "    \n",
    "    score = model.evaluate(x_test_encoded, y_test_encoded, verbose=0)\n",
    "    # Calculate and save metrics\n",
    "    loss, accuracy = calculate_metrics(score, model_name)\n",
    "    \n",
    "    # Predict labels for the validation set\n",
    "    y_pred = model.predict(x_test_encoded)\n",
    "    # Convert one-hot encoded labels back to original format\n",
    "    y_pred = np.argmax(y_pred, axis=1)     \n",
    "    # Calculate classification report\n",
    "    report = calculate_classification_report(y_test_true, y_pred)\n",
    "    \n",
    "    # Save results\n",
    "    save_results(accuracy, loss, report, model_name)\n",
    "\n",
    "    # Plot accuracy # TODO\n",
    "    #plot(history, model_name)\n",
    "\n",
    "    # Plot confusion matrix # TODO\n",
    "    senti_labels = ['negative', 'neutral', 'positive']\n",
    "    #plot_confusion_matrix(y_test, y_pred, senti_labels, model_name)\n",
    "    \n",
    "    print(\"Params: {}\\n\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a97d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(x_val_encoded, y_val_encoded, model, model_name, params):\n",
    "    score = model.evaluate(x_val_encoded, y_val_encoded, verbose=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss, accuracy = calculate_metrics(score, model_name)\n",
    "    print(\"Params: {}\\n\".format(params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8fdb95a",
   "metadata": {},
   "source": [
    "### Single - Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b78e953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the LSTM model\n",
    "def define_lstm_model(units, dropout_rate): #optimizer, learning_rate\n",
    "    single_lstm_model = Sequential()\n",
    "    single_lstm_model.add(Embedding(embedding_vocab_size, EMBEDDING_DIM, input_length=max_seq_length))\n",
    "    single_lstm_model.add(Dropout(dropout_rate))\n",
    "    single_lstm_model.add(LSTM(units=units))\n",
    "    single_lstm_model.add(Dense(3, activation='softmax'))\n",
    "    single_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return single_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c05e8a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferga\\AppData\\Local\\Temp\\ipykernel_18512\\645917721.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  single_lstm_model = KerasClassifier(build_fn=define_lstm_model, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# TODO: set epochs and batch size\n",
    "single_lstm_model = KerasClassifier(build_fn=define_lstm_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "46363d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "lstm_param_grid = {\n",
    "    'units': [32, 64, 128],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    #'optimizer': [Adam, SGD, RMSprop, Adagrad],\n",
    "    #'learning_rate': [0.001, 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ce49c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'dropout_rate': 0.3, 'units': 32}\n",
      "Best Score:  0.8921869794527689\n",
      "\n",
      "Top 3 parameter combinations ranked by performance (from best to worst):\n",
      "Mean Test Score: 0.8922 (±0.0115) for {'dropout_rate': 0.3, 'units': 32}\n",
      "Mean Test Score: 0.8922 (±0.0115) for {'dropout_rate': 0.3, 'units': 64}\n",
      "Mean Test Score: 0.8922 (±0.0115) for {'dropout_rate': 0.3, 'units': 128}\n",
      "Mean Test Score: 0.8922 (±0.0115) for {'dropout_rate': 0.5, 'units': 32}\n",
      "Mean Test Score: 0.8922 (±0.0115) for {'dropout_rate': 0.5, 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=single_lstm_model, param_grid=lstm_param_grid, cv=3)\n",
    "grid_search.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "# Get the best parameters and best score from grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n",
    "\n",
    "# Get the mean test scores and standard deviations of test scores for all parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "sorted_results = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "top3_models = sorted_results[:5] # TODO: update 10 to 3\n",
    "print_top3_models(top3_models)\n",
    "top3_models = sorted_results[:3] # TODO: and delete this\n",
    "top3_params = top3_models['params'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d084baf",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c5e9303",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a4a0578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "20/20 [==============================] - 8s 145ms/step - loss: 0.8359 - accuracy: 0.8438\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - 3s 152ms/step - loss: 0.4344 - accuracy: 0.8922\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - 3s 139ms/step - loss: 0.4234 - accuracy: 0.8922\n",
      "Epoch 1/3\n",
      "20/20 [==============================] - 9s 203ms/step - loss: 0.7096 - accuracy: 0.8500\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 0.4295 - accuracy: 0.8922\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - 4s 184ms/step - loss: 0.4199 - accuracy: 0.8922\n",
      "Epoch 1/3\n",
      "20/20 [==============================] - 11s 242ms/step - loss: 0.6302 - accuracy: 0.8562\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - 5s 257ms/step - loss: 0.4244 - accuracy: 0.8922\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - 5s 245ms/step - loss: 0.4201 - accuracy: 0.8922\n",
      "LSTM-single-1 Accuracy: 88.7499988079071\n",
      "LSTM-single-1 Loss: 0.42837008833885193\n",
      "Params: {'dropout_rate': 0.3, 'units': 32}\n",
      "\n",
      "LSTM-single-2 Accuracy: 88.7499988079071\n",
      "LSTM-single-2 Loss: 0.43209075927734375\n",
      "Params: {'dropout_rate': 0.3, 'units': 64}\n",
      "\n",
      "LSTM-single-3 Accuracy: 88.7499988079071\n",
      "LSTM-single-3 Loss: 0.4315715730190277\n",
      "Params: {'dropout_rate': 0.3, 'units': 128}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the top 3 models on the validation set\n",
    "# TODO: remove f1_score  \n",
    "# TODO: early stopping & batch_size?\n",
    "single_lstm_cand_1 = define_lstm_model(units=top3_params[0]['units'], dropout_rate=top3_params[0]['dropout_rate'])\n",
    "single_lstm_cand_1.fit(x_train_encoded, y_train_encoded, epochs=3)\n",
    "\n",
    "single_lstm_cand_2 = define_lstm_model(units=top3_params[1]['units'], dropout_rate=top3_params[1]['dropout_rate'])\n",
    "single_lstm_cand_2.fit(x_train_encoded, y_train_encoded, epochs=3)\n",
    "\n",
    "single_lstm_cand_3 = define_lstm_model(units=top3_params[2]['units'], dropout_rate=top3_params[2]['dropout_rate'])\n",
    "single_lstm_cand_3.fit(x_train_encoded, y_train_encoded, epochs=3)\n",
    "\n",
    "\n",
    "# TODO: after fixing plotting set boolean to include or exclude plotting etc.\n",
    "predict_data(x_val_encoded, y_val_encoded, single_lstm_cand_1,  \"LSTM-single-1\", top3_params[0])\n",
    "predict_data(x_val_encoded, y_val_encoded, single_lstm_cand_2, \"LSTM-single-2\", top3_params[1])\n",
    "predict_data(x_val_encoded, y_val_encoded, single_lstm_cand_3, \"LSTM-single-3\", top3_params[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc6833b2",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d6fc7ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM-single-best Testing complete!\n",
      "\n",
      "LSTM-single-best Accuracy: 89.49999809265137\n",
      "LSTM-single-best Loss: 0.41314074397087097\n",
      "7/7 [==============================] - 2s 45ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.06      1.00      0.11        12\n",
      "           3       0.00      0.00      0.00       179\n",
      "\n",
      "    accuracy                           0.06       200\n",
      "   macro avg       0.02      0.33      0.04       200\n",
      "weighted avg       0.00      0.06      0.01       200\n",
      "\n",
      "Params: {'dropout_rate': 0.3, 'units': 32}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferga\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ferga\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ferga\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Set the best model and evaluate the models on the test data #TODO\n",
    "single_lstm_best = single_lstm_cand_1\n",
    "evaluate_model(single_lstm_best,  \"LSTM-single-best\", x_test_encoded, y_test_encoded, y_test, top3_params[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7ab0df6",
   "metadata": {},
   "source": [
    "### Multi - Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "288cfd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: copy-paste above to multi\n",
    "def define_multi_channel_lstm_model(units1, units2, dense_units):\n",
    "    # Vocabulary-based embedding layer\n",
    "    inputs1 = Input(shape=(max_seq_length,))\n",
    "    embedding1 = Embedding(embedding_vocab_size, EMBEDDING_DIM,\n",
    "                           input_length=max_seq_length)(inputs1)\n",
    "    lstm1 = LSTM(units=units1)(embedding1)\n",
    "\n",
    "    # Word2Vec embedding layer\n",
    "    inputs2 = Input(shape=(max_seq_length,))\n",
    "    embedding2 = Embedding(embedding_vocab_size, EMBEDDING_DIM,\n",
    "                           input_length=max_seq_length,\n",
    "                           weights=[w2v_embedding_vectors], trainable=False)(inputs2)\n",
    "    lstm2 = LSTM(units=units2)(embedding2)\n",
    "\n",
    "    # Concatenate the two inputs\n",
    "    merged = concatenate([lstm1, lstm2])\n",
    "\n",
    "    # Dense layer for the merged inputs & Output Layer\n",
    "    merged_dense = Dense(units=dense_units, activation='relu')(merged)\n",
    "    outputs = Dense(3, activation='softmax')(merged_dense)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ef03deb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferga\\AppData\\Local\\Temp\\ipykernel_18512\\774939605.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  multi_lstm_model = KerasClassifier(build_fn=define_multi_channel_lstm_model, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "multi_lstm_model = KerasClassifier(build_fn=define_multi_channel_lstm_model, verbose=0)\n",
    "# multi_lstm_model = define_multi_channel_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d4ce7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "multi_lstm_param_grid = {\n",
    "    'units1': [64, 128],\n",
    "    'units2': [64, 128],\n",
    "    'dense_units': [32, 64],\n",
    "    #'batch_size': [16, 32],\n",
    "    #'epochs': [10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1cf7203d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 32)\n",
      "Model: \"model_57\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_115 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_116 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_145 (Embedding)      (None, 223, 100)     110800      ['input_115[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_146 (Embedding)      (None, 223, 100)     110800      ['input_116[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_145 (LSTM)                (None, 64)           42240       ['embedding_145[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_146 (LSTM)                (None, 64)           42240       ['embedding_146[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_57 (Concatenate)   (None, 128)          0           ['lstm_145[0][0]',               \n",
      "                                                                  'lstm_146[0][0]']               \n",
      "                                                                                                  \n",
      " dense_145 (Dense)              (None, 32)           4128        ['concatenate_57[0][0]']         \n",
      "                                                                                                  \n",
      " dense_146 (Dense)              (None, 3)            99          ['dense_145[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 310,307\n",
      "Trainable params: 199,507\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 9s 159ms/step - loss: 0.7057 - accuracy: 0.8484\n",
      "20/20 [==============================] - 3s 56ms/step - loss: 0.4455 - accuracy: 0.8922\n",
      "(64, 64, 64)\n",
      "Model: \"model_58\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_117 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_118 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_147 (Embedding)      (None, 223, 100)     110800      ['input_117[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_148 (Embedding)      (None, 223, 100)     110800      ['input_118[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_147 (LSTM)                (None, 64)           42240       ['embedding_147[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_148 (LSTM)                (None, 64)           42240       ['embedding_148[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_58 (Concatenate)   (None, 128)          0           ['lstm_147[0][0]',               \n",
      "                                                                  'lstm_148[0][0]']               \n",
      "                                                                                                  \n",
      " dense_147 (Dense)              (None, 64)           8256        ['concatenate_58[0][0]']         \n",
      "                                                                                                  \n",
      " dense_148 (Dense)              (None, 3)            195         ['dense_147[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 314,531\n",
      "Trainable params: 203,731\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 9s 158ms/step - loss: 0.8163 - accuracy: 0.8031\n",
      "20/20 [==============================] - 3s 55ms/step - loss: 0.4975 - accuracy: 0.8922\n",
      "(64, 128, 32)\n",
      "Model: \"model_59\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_119 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_120 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_149 (Embedding)      (None, 223, 100)     110800      ['input_119[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_150 (Embedding)      (None, 223, 100)     110800      ['input_120[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_149 (LSTM)                (None, 64)           42240       ['embedding_149[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_150 (LSTM)                (None, 128)          117248      ['embedding_150[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_59 (Concatenate)   (None, 192)          0           ['lstm_149[0][0]',               \n",
      "                                                                  'lstm_150[0][0]']               \n",
      "                                                                                                  \n",
      " dense_149 (Dense)              (None, 32)           6176        ['concatenate_59[0][0]']         \n",
      "                                                                                                  \n",
      " dense_150 (Dense)              (None, 3)            99          ['dense_149[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 387,363\n",
      "Trainable params: 276,563\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 9s 227ms/step - loss: 0.7168 - accuracy: 0.8484\n",
      "20/20 [==============================] - 3s 84ms/step - loss: 0.4232 - accuracy: 0.8922\n",
      "(64, 128, 64)\n",
      "Model: \"model_60\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_121 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_122 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_151 (Embedding)      (None, 223, 100)     110800      ['input_121[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_152 (Embedding)      (None, 223, 100)     110800      ['input_122[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_151 (LSTM)                (None, 64)           42240       ['embedding_151[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_152 (LSTM)                (None, 128)          117248      ['embedding_152[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_60 (Concatenate)   (None, 192)          0           ['lstm_151[0][0]',               \n",
      "                                                                  'lstm_152[0][0]']               \n",
      "                                                                                                  \n",
      " dense_151 (Dense)              (None, 64)           12352       ['concatenate_60[0][0]']         \n",
      "                                                                                                  \n",
      " dense_152 (Dense)              (None, 3)            195         ['dense_151[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 393,635\n",
      "Trainable params: 282,835\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 9s 191ms/step - loss: 0.6446 - accuracy: 0.8922\n",
      "20/20 [==============================] - 3s 74ms/step - loss: 0.4185 - accuracy: 0.8922\n",
      "(128, 64, 32)\n",
      "Model: \"model_61\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_123 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_124 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_153 (Embedding)      (None, 223, 100)     110800      ['input_123[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_154 (Embedding)      (None, 223, 100)     110800      ['input_124[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_153 (LSTM)                (None, 128)          117248      ['embedding_153[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_154 (LSTM)                (None, 64)           42240       ['embedding_154[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_61 (Concatenate)   (None, 192)          0           ['lstm_153[0][0]',               \n",
      "                                                                  'lstm_154[0][0]']               \n",
      "                                                                                                  \n",
      " dense_153 (Dense)              (None, 32)           6176        ['concatenate_61[0][0]']         \n",
      "                                                                                                  \n",
      " dense_154 (Dense)              (None, 3)            99          ['dense_153[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 387,363\n",
      "Trainable params: 276,563\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 9s 202ms/step - loss: 0.6903 - accuracy: 0.8484\n",
      "20/20 [==============================] - 3s 66ms/step - loss: 0.4252 - accuracy: 0.8922\n",
      "(128, 64, 64)\n",
      "Model: \"model_62\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_125 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_126 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_155 (Embedding)      (None, 223, 100)     110800      ['input_125[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_156 (Embedding)      (None, 223, 100)     110800      ['input_126[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_155 (LSTM)                (None, 128)          117248      ['embedding_155[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_156 (LSTM)                (None, 64)           42240       ['embedding_156[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_62 (Concatenate)   (None, 192)          0           ['lstm_155[0][0]',               \n",
      "                                                                  'lstm_156[0][0]']               \n",
      "                                                                                                  \n",
      " dense_155 (Dense)              (None, 64)           12352       ['concatenate_62[0][0]']         \n",
      "                                                                                                  \n",
      " dense_156 (Dense)              (None, 3)            195         ['dense_155[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 393,635\n",
      "Trainable params: 282,835\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 10s 192ms/step - loss: 0.6154 - accuracy: 0.8562\n",
      "20/20 [==============================] - 3s 67ms/step - loss: 0.4170 - accuracy: 0.8922\n",
      "(128, 128, 32)\n",
      "Model: \"model_63\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_127 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_128 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_157 (Embedding)      (None, 223, 100)     110800      ['input_127[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_158 (Embedding)      (None, 223, 100)     110800      ['input_128[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_157 (LSTM)                (None, 128)          117248      ['embedding_157[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_158 (LSTM)                (None, 128)          117248      ['embedding_158[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_63 (Concatenate)   (None, 256)          0           ['lstm_157[0][0]',               \n",
      "                                                                  'lstm_158[0][0]']               \n",
      "                                                                                                  \n",
      " dense_157 (Dense)              (None, 32)           8224        ['concatenate_63[0][0]']         \n",
      "                                                                                                  \n",
      " dense_158 (Dense)              (None, 3)            99          ['dense_157[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 464,419\n",
      "Trainable params: 353,619\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 9s 211ms/step - loss: 0.6321 - accuracy: 0.8922\n",
      "20/20 [==============================] - 3s 72ms/step - loss: 0.4268 - accuracy: 0.8922\n",
      "(128, 128, 64)\n",
      "Model: \"model_64\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_129 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_130 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_159 (Embedding)      (None, 223, 100)     110800      ['input_129[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_160 (Embedding)      (None, 223, 100)     110800      ['input_130[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_159 (LSTM)                (None, 128)          117248      ['embedding_159[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_160 (LSTM)                (None, 128)          117248      ['embedding_160[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_64 (Concatenate)   (None, 256)          0           ['lstm_159[0][0]',               \n",
      "                                                                  'lstm_160[0][0]']               \n",
      "                                                                                                  \n",
      " dense_159 (Dense)              (None, 64)           16448       ['concatenate_64[0][0]']         \n",
      "                                                                                                  \n",
      " dense_160 (Dense)              (None, 3)            195         ['dense_159[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 472,739\n",
      "Trainable params: 361,939\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 9s 213ms/step - loss: 0.6398 - accuracy: 0.8922\n",
      "20/20 [==============================] - 3s 85ms/step - loss: 0.4395 - accuracy: 0.8922\n"
     ]
    }
   ],
   "source": [
    "param_combinations = product(*multi_lstm_param_grid.values())\n",
    "models = []\n",
    "\n",
    "for params in param_combinations:\n",
    "    print(params)\n",
    "    units1, units2, dense_units = params\n",
    "    \n",
    "    multi_lstm_model = define_multi_channel_lstm_model(\n",
    "        units1=units1,\n",
    "        units2=units2,\n",
    "        dense_units=dense_units\n",
    "    )\n",
    "    \n",
    "    x_train = [x_train_encoded, x_train_encoded]\n",
    "    y_train = asarray(y_train_encoded)\n",
    "    \n",
    "    multi_lstm_model.fit(x_train, y_train)\n",
    "    loss, accuracy = multi_lstm_model.evaluate(x_train, y_train)\n",
    "    \n",
    "    accuracy = accuracy * 100\n",
    "    models.append({\n",
    "            'units1': units1,\n",
    "            'units2': units2,\n",
    "            'dense_units': dense_units,\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ae7a5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top3_models(top3_models):\n",
    "    print(\"\\nTop 3 parameter combinations ranked by performance (from best to worst):\")\n",
    "    for index, row in top3_models.iterrows():\n",
    "        units1 = row['units1']\n",
    "        units2 = row['units2']\n",
    "        dense_units = row['dense_units']\n",
    "        loss = row['loss']\n",
    "        accuracy = row['accuracy']\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}, Loss: {loss:.4f} for units1: {units1}, units2: {units2}, dense_units: {dense_units}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "82f03c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 parameter combinations ranked by performance (from best to worst):\n",
      "Accuracy: 89.2187, Loss: 0.4455 for units1: 64.0, units2: 64.0, dense_units: 32.0\n",
      "Accuracy: 89.2187, Loss: 0.4975 for units1: 64.0, units2: 64.0, dense_units: 64.0\n",
      "Accuracy: 89.2187, Loss: 0.4232 for units1: 64.0, units2: 128.0, dense_units: 32.0\n",
      "Accuracy: 89.2187, Loss: 0.4185 for units1: 64.0, units2: 128.0, dense_units: 64.0\n",
      "Accuracy: 89.2187, Loss: 0.4252 for units1: 128.0, units2: 64.0, dense_units: 32.0\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of models to a pandas DataFrame\n",
    "top3_models = pd.DataFrame(models)\n",
    "\n",
    "# Sort models based on accuracy in descending order and loss in ascending order\n",
    "top3_models = top3_models.sort_values(by=['accuracy', 'loss'], ascending=[False, False])\n",
    "\n",
    "top3_models = models[:5] # TODO: change 5 to 3\n",
    "top3_models = pd.DataFrame(top3_models)\n",
    "\n",
    "print_top3_models(top3_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "14999c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_66\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_136 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_137 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_166 (Embedding)      (None, 223, 100)     110800      ['input_136[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_167 (Embedding)      (None, 223, 100)     110800      ['input_137[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_166 (LSTM)                (None, 64)           42240       ['embedding_166[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_167 (LSTM)                (None, 64)           42240       ['embedding_167[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_66 (Concatenate)   (None, 128)          0           ['lstm_166[0][0]',               \n",
      "                                                                  'lstm_167[0][0]']               \n",
      "                                                                                                  \n",
      " dense_163 (Dense)              (None, 32)           4128        ['concatenate_66[0][0]']         \n",
      "                                                                                                  \n",
      " dense_164 (Dense)              (None, 3)            99          ['dense_163[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 310,307\n",
      "Trainable params: 199,507\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 8s 171ms/step - loss: 0.7585 - accuracy: 0.8531\n",
      "LSTM-multi-0 Accuracy: 88.7499988079071\n",
      "LSTM-multi-0 Loss: 0.4846920073032379\n",
      "Params: {'dropout_rate': 0.3, 'units': 32}\n",
      "\n",
      "Model: \"model_67\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_138 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_139 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_168 (Embedding)      (None, 223, 100)     110800      ['input_138[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_169 (Embedding)      (None, 223, 100)     110800      ['input_139[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_168 (LSTM)                (None, 64)           42240       ['embedding_168[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_169 (LSTM)                (None, 64)           42240       ['embedding_169[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_67 (Concatenate)   (None, 128)          0           ['lstm_168[0][0]',               \n",
      "                                                                  'lstm_169[0][0]']               \n",
      "                                                                                                  \n",
      " dense_165 (Dense)              (None, 64)           8256        ['concatenate_67[0][0]']         \n",
      "                                                                                                  \n",
      " dense_166 (Dense)              (None, 3)            195         ['dense_165[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 314,531\n",
      "Trainable params: 203,731\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 10s 183ms/step - loss: 0.8024 - accuracy: 0.8547\n",
      "LSTM-multi-1 Accuracy: 88.7499988079071\n",
      "LSTM-multi-1 Loss: 0.4979555010795593\n",
      "Params: {'dropout_rate': 0.3, 'units': 32}\n",
      "\n",
      "Model: \"model_68\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_140 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_141 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_170 (Embedding)      (None, 223, 100)     110800      ['input_140[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_171 (Embedding)      (None, 223, 100)     110800      ['input_141[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_170 (LSTM)                (None, 64)           42240       ['embedding_170[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_171 (LSTM)                (None, 128)          117248      ['embedding_171[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_68 (Concatenate)   (None, 192)          0           ['lstm_170[0][0]',               \n",
      "                                                                  'lstm_171[0][0]']               \n",
      "                                                                                                  \n",
      " dense_167 (Dense)              (None, 32)           6176        ['concatenate_68[0][0]']         \n",
      "                                                                                                  \n",
      " dense_168 (Dense)              (None, 3)            99          ['dense_167[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 387,363\n",
      "Trainable params: 276,563\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 9s 198ms/step - loss: 0.7682 - accuracy: 0.8484\n",
      "LSTM-multi-2 Accuracy: 88.7499988079071\n",
      "LSTM-multi-2 Loss: 0.45774808526039124\n",
      "Params: {'dropout_rate': 0.3, 'units': 32}\n",
      "\n",
      "Model: \"model_69\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_142 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_143 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_172 (Embedding)      (None, 223, 100)     110800      ['input_142[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_173 (Embedding)      (None, 223, 100)     110800      ['input_143[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_172 (LSTM)                (None, 64)           42240       ['embedding_172[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_173 (LSTM)                (None, 128)          117248      ['embedding_173[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_69 (Concatenate)   (None, 192)          0           ['lstm_172[0][0]',               \n",
      "                                                                  'lstm_173[0][0]']               \n",
      "                                                                                                  \n",
      " dense_169 (Dense)              (None, 64)           12352       ['concatenate_69[0][0]']         \n",
      "                                                                                                  \n",
      " dense_170 (Dense)              (None, 3)            195         ['dense_169[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 393,635\n",
      "Trainable params: 282,835\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 11s 254ms/step - loss: 0.7658 - accuracy: 0.8422\n",
      "LSTM-multi-3 Accuracy: 88.7499988079071\n",
      "LSTM-multi-3 Loss: 0.44988417625427246\n",
      "Params: {'dropout_rate': 0.3, 'units': 32}\n",
      "\n",
      "Model: \"model_70\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_144 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " input_145 (InputLayer)         [(None, 223)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_174 (Embedding)      (None, 223, 100)     110800      ['input_144[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_175 (Embedding)      (None, 223, 100)     110800      ['input_145[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_174 (LSTM)                (None, 128)          117248      ['embedding_174[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_175 (LSTM)                (None, 64)           42240       ['embedding_175[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_70 (Concatenate)   (None, 192)          0           ['lstm_174[0][0]',               \n",
      "                                                                  'lstm_175[0][0]']               \n",
      "                                                                                                  \n",
      " dense_171 (Dense)              (None, 32)           6176        ['concatenate_70[0][0]']         \n",
      "                                                                                                  \n",
      " dense_172 (Dense)              (None, 3)            99          ['dense_171[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 387,363\n",
      "Trainable params: 276,563\n",
      "Non-trainable params: 110,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "20/20 [==============================] - 11s 217ms/step - loss: 0.6724 - accuracy: 0.8516\n",
      "LSTM-multi-4 Accuracy: 88.7499988079071\n",
      "LSTM-multi-4 Loss: 0.444649875164032\n",
      "Params: {'dropout_rate': 0.3, 'units': 32}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in top3_models.iterrows():\n",
    "    units1 = int(row['units1'])\n",
    "    units2 = int(row['units2'])\n",
    "    dense_units = int(row['dense_units'])\n",
    "\n",
    "    multi_lstm_cand_model = define_multi_channel_lstm_model(\n",
    "        units1=units1,\n",
    "        units2=units2,\n",
    "        dense_units=dense_units\n",
    "    )\n",
    "\n",
    "    x_train = [x_train_encoded, x_train_encoded]\n",
    "    y_train = asarray(y_train_encoded)\n",
    "\n",
    "    multi_lstm_cand_model.fit(x_train, y_train)\n",
    "    # save the model\n",
    "    multi_lstm_model.save(f'multi-lstm-model-{index}.h5')\n",
    "\n",
    "    x_val = [x_val_encoded, x_val_encoded]\n",
    "    predict_data(x_val, y_val_encoded, multi_lstm_cand_model,  f\"LSTM-multi-{index}\", top3_params[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1c4410f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM-multi-best Testing complete!\n",
      "\n",
      "LSTM-multi-best Accuracy: 89.49999809265137\n",
      "LSTM-multi-best Loss: 0.4423085153102875\n",
      "7/7 [==============================] - 2s 73ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.06      1.00      0.11        12\n",
      "           3       0.00      0.00      0.00       179\n",
      "\n",
      "    accuracy                           0.06       200\n",
      "   macro avg       0.02      0.33      0.04       200\n",
      "weighted avg       0.00      0.06      0.01       200\n",
      "\n",
      "Params: {'dropout_rate': 0.3, 'units': 32}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferga\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ferga\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ferga\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# set index and load the model\n",
    "index = 0\n",
    "multi_lstm_best = load_model(f'multi-lstm-model-{index}.h5')\n",
    "x_test = [x_test_encoded, x_test_encoded]\n",
    "evaluate_model(multi_lstm_best,  \"LSTM-multi-best\", x_test, y_test_encoded, y_test, top3_params[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d38e720",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "658bc4eb",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bae840e2",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
